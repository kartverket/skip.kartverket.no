"use strict";(self.webpackChunkskip_docs=self.webpackChunkskip_docs||[]).push([[8130],{77735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"20-teams-on-skip","metadata":{"permalink":"/blog/20-teams-on-skip","source":"@site/blog/2024-07-26-20-teams-on-skip.md","title":"20 teams on SKIP: What we\'ve learned along the way","description":"We recently passed an important milestone, onboarding our 20th team on Kartverket\'s platform. On the occasion of this achievement we\'re going to look back at the decisions we made that led us to building a successful platform. In this tech blog we are showcasing the the secrets to our success - the decisions that have had the biggest impact.\\n","date":"2024-07-26T00:00:00.000Z","tags":[{"inline":true,"label":"learnings","permalink":"/blog/tags/learnings"},{"inline":true,"label":"kubernetes","permalink":"/blog/tags/kubernetes"}],"readingTime":17.75,"hasTruncateMarker":true,"authors":[{"name":"Eline Henriksen","title":"Tidligere Produkteier og Plattformutvikler","url":"https://eliine.dev","imageURL":"https://github.com/eliihen.png","key":"elinehenriksen","page":null}],"frontMatter":{"title":"20 teams on SKIP: What we\'ve learned along the way","description":"We recently passed an important milestone, onboarding our 20th team on Kartverket\'s platform. On the occasion of this achievement we\'re going to look back at the decisions we made that led us to building a successful platform. In this tech blog we are showcasing the the secrets to our success - the decisions that have had the biggest impact.\\n","slug":"20-teams-on-skip","authors":["elinehenriksen"],"tags":["learnings","kubernetes"],"image":"/img/20-teams-on-skip.jpeg","hide_table_of_contents":false},"unlisted":false,"nextItem":{"title":"Scaling with Argo CD: Introducing the Apps Repo Architecture","permalink":"/blog/introducing-apps-repositories"}},"content":"![Cake with text saying 20 teams on SKIP and DASK](/img/20-teams-on-skip.jpeg)\\n\\nWe recently passed an important milestone, onboarding our 20th team on\\nKartverket\'s platform. Since we started a few years ago we\'ve been working hard\\nto build a platform that drives positive change at Kartverket, and we\'re proud\\nof the results we\'ve got. Our research shows that users are happy with the\\ntechnology and support they get, and that they\'re able to deliver faster and\\nmore securely than before.\\n\\nBuilding a platform is not easy, and it requires re-thinking a lot of assumptions\\nheld in your organization. It\'s therefore easy to lose your way and to end up\\nwith something that doesn\'t deliver on the high standards you\'ve set for\\nyour organization. Like everyone that starts with something new we\'ve made\\nmistakes along the way, we\'ve had to change course multiple times and most\\nimportantly we\'ve learned a lot in our journey.\\n\\nOn the occasion of this achievement we\'re going to look back at the decisions we\\nmade that led us to building a successful platform. In this tech blog we are\\nshowcasing the secrets to our success - the decisions that have had the\\nbiggest impact.\\n\\n\x3c!--truncate--\x3e\\n\\n## Principles matter\\n\\nWhen you set out to create something new, you have the privilege of setting\\nsome standards that encourage best practices. While this is possible to do for\\nan existing system, in practice it will mean a lot of work to get to the\\npoint where you\'re able to enforce these standards. It\'s much easier to start\\nwith a clean slate.\\n\\nFor our platform, we decided on a set of principles that we wanted to follow.\\nSome of these are:\\n\\n- **Stateless**: Our clusters are stateless, which means that we can easily\\n  replace them if something goes wrong. All configuration is held in a GitOps\\n  repository and all state is held in external systems like managed databases,\\n  object storage, etc. This significantly reduces operational complexity and\\n  recovery time. If a cluster fails we can easily replace or revert it by\\n  applying the configuration from the GitOps repository without worrying about\\n  losing state, or doing time-consuming data recovery operations.\\n- **Ownership**: For each application, there is a clear owner. This owner is\\n  responsible for the application and maintains and supports it.  This way we\'re\\n  able to avoid the \\"tragedy of the commons\\", where no one is responsible for\\n  an application. If an app has unclear or short-term ownership, you simply\\n  don\'t get to use the platform. We\'re not an orphanage.\\n- **Financing**: You use the platform? You also need to pay for its continued\\n  support and development. We\'re working towards a chargeback model where your\\n  department is billed for the resources they use as a way to ensure that the\\n  platform is sustainable. Until this is ready, we expose the costs of the\\n  resources used by each team and then negotiate with the departments on how to\\n  cover these costs, but this is time-consuming work.\\n- **Secure by default**: We enforce security best practices by default. Examples\\n  of this are zero trust networking with Network Policies, where no app can talk\\n  to another without explicitly allowing this. Some applications will need to opt\\n  out of some of these defaults, and they can do so by altering their\\n  configuration. But the defaults are secure, which is especially useful for\\n  teams that are new to Kubernetes.\\n  \\nAll teams that are onboarded on SKIP are given an introduction to these\\nprinciples and are expected to follow them. This means that being able to use\\nthe modern platform is contingent on the teams being able to prioritize\\nmodernizing their applications and working in sustainable ways, which helps push\\nfor positive change.\\n\\n## Encourage collaboration\\n\\nIt\'s easy for a product team to ask the platform team for help when they\'re\\nstuck. We\'re always happy to help, but we also have a heavy workload of exciting\\nthings we\'re working on. Therefore it\'s much better when platform users can help\\neach other, as this facilitates collaboration and learning. This is why we\\nhighly encourage teams to help each other out - to build a community around the\\nplatform.\\n\\nIn practice this is done through a single Slack channel where all teams that are\\nusing the platform are invited. This is a great place to ask questions, share\\nexperiences, and learn from each other - and it\'s a place where all new features\\nand changes are announced. We used to have many different channels for different\\nteams, but we found that this was not as effective for building a community as\\na channel where everyone can help each other out.\\n\\nAnd a final tip: As a platform developer, sometimes it\'s better to wait a little\\nwhile before responding to questions in these channels to allow the community to\\nhelp each other out before you jump in and help.\\n\\n## Make time for innovation\\n\\nIt\'s easy to get bogged down in the day-to-day work of keeping the platform\\nrunning. This is why it\'s important to set aside time for innovation, this\\nis something we take very seriously. \\n\\nOn SKIP we have dedicated innovation days where we work on new features,\\nimprovements, and other things that we think will make the platform better. This\\nis an extraordinarily successful initiative, and we\'ve seen many great features\\ncome out of these days. It\'s also a great way to build team morale and to build\\na culture of learning and innovation.\\n\\nIn practice we have two days in a row of dedicated innovation work every other\\nmonth. We used to have one day every month, but we found that this was not\\nenough time to really get into the flow of things so we started doing two\\ndays every 2 months, which worked better. We also have a rule that you can\'t\\nwork on anything that\'s on the roadmap, as this is work that we\'re already going\\nto do. This is a great way to get new ideas and to work on things that might not\\notherwise get done.\\n\\n![Innovation work](img/innovation-day-results.png)\\n\\nThere\'s a little bit of structure around these days, but not too much. \\n\\nFirst, it is understood by everyone that these days are for things that are\\n\\"useful for Kartverket\\". This means that you can\'t work on your own pet project,\\nbut it\'s vague enough that you can work on pretty much anything that you think\\nwill be useful for the organization. \\n\\nThen, a week before the innovation day we will have a \\"pitching session\\", where\\neveryone who has an idea can pitch it to the rest of the team. This is a great\\nway to get feedback on your idea and to get others to join you in working on it.\\n\\nFinally, we have a \\"show and tell\\" session at the end of the last day where\\neveryone shows what they\'ve been working on. This way we can share our\\nexperiences and discuss if this work can be improved and put into production.\\nWe encourage everyone to show something, even if it\'s not finished or you did\\nvideo lessons, as this creates discussion and further ideas.\\n\\n![Demo time!](img/show-and-tell.jpeg)\\n\\nThere\'s plenty of examples of features that are results of work done on these\\ndays. On-premise Web Application Firewall with Wasm, Grafana features, open\\nsource tools like [Skiperator](https://github.com/kartverket/skiperator) and\\n[Skyline](https://github.com/kartverket/skyline) as well as this very website!\\n\\nNo one has time to prioritize innovation, and we\'re no different. But we\\nprioritize it anyway, because we know that it\'s important to keep improving and\\nto keep learning.\\n\\n## Communication is key\\n\\n![SKIPs kommunikasjonsstrategi](img/communication.png)\\n\\nUnfortunately a lot of infrastructure teams don\'t prioritize communication very\\nwell. This is a mistake. Communication is key to building a successful platform.\\n\\nYour users exist in the context of all the platform features that you have\\nshipped and the changes you will ship in the future. Not informing them and\\nkeeping them up to date with what\'s going on is a surefire way to lose their\\ntrust and to make them unhappy.\\n\\nIt starts with simply informing users of the new features that ship. This can be\\ndone through a Slack channel, a newsletter, a blog or a town hall meeting. We\\nuse a combination of all of these, but the most important thing is that you\\ninform your users of what\'s coming. An added benefit of this is helps push\\nadoption of new features and excitement around the platform by showcasing\\ninnovation.\\n\\nThe next step is informing users on what will ship and when. This will help\\nusers plan their work and to know what to expect, but it also helps users feel\\ninvolved when they see their requests being planned. This can be done through a\\nroadmap, a technical forum, or a blog. We use a combination of all of these, but\\nthe easiest way to do this is to have a roadmap that you keep up to date on a\\nregular basis.\\n\\nNow for the hard part: When things go wrong, you need to communicate this as\\nwell. Product teams will want to know when their applications are affected by\\noutages or other issues, and they will want to know what you\'re doing to fix it.\\nThis can be done through a status page, a Slack channel, or postmortems. Again,\\nwe use a mix of these so that we can reach as many users as possible at the\\nright time.\\n\\nDo these things and you will have happy users that feel informed.\\n\\n## Branding is important\\n\\n![SKIP logo](img/skip-brand.png)\\n\\nDo you think Spotify would be as successful if it was called \\"Music Player\\"? \\nDo you think Apple would be as successful if it was called \\"Computer Company\\"?\\nOf course not. Branding is important. It builds a sense of identity and\\ncommunity around your platform.\\n\\nThis is especially important for a platform team, as you\'re not just building a\\nproduct, you\'re building a community. You want your users to feel like they\'re\\npart of something bigger, and you want them to feel excited to use the platform.\\n\\nWhen you\'re starting out, you want to drive adoption. Here a brand really helps\\nas it\'s easier to talk about a good brand in a positive way. It\'s also easier to\\nget leadership buy-in when you have a strong brand.\\n\\nThis holds true when you\'re more established as well. When you grow larger than\\nyour ability to talk to everyone, a brand helps you communicate your values and\\nintent to your users, which will drive organic growth from teams that want to\\nwork with you.\\n\\nA minimum viable brand is a logo, a name, and a color scheme. This is something\\nyou should think deeply about, as it\'s something that will stick with you for a\\nlong time. After this you can think about a website, merchandise like stickers\\nand t-shirts, and a mascot. These things are not necessary, but they can help\\nbuild a sense of identity and community around your platform.\\n\\n## Using the cloud is a long journey\\n\\nAs a platform team, it\'s our responsibility to push for modern, user-friendly\\nand secure solutions. This generally means using public cloud solutions like\\nGoogle Cloud Platform. But for most organizations, pushing this narrative incurs\\nsignificant friction and to some extent fear due to legal and cost concerns.\\nThis is understandable, as the known is always more comfortable than the\\nunknown, and it\'s a view that\'s hard to change.\\n\\nThis is why it\'s important to take a long-term view on this. You\'re not going to\\nmove everything to the cloud overnight, and you\'re not going to convince\\neveryone to get on board with this idea overnight. It\'s a long journey, and you\\nneed to be patient and persistent. \\n\\nWe\'ve spent years pushing for the cloud, and we\'re still not there. You\'re going\\nto have to participate in many (*many!*) meetings, and you\'re going to have to\\nfight for every little thing over and over again. But it\'s necessary. Once \\neveryone has a clear understanding of the risks and how to mitigate them, you\\nwill be able to formulate a document guiding the organization\'s teams on how to\\nget to the cloud from a compliance point of view.\\n\\nIf you asked me for any recommendations on how to get to the cloud as easily as\\npossible, it would be to first get leadership buy-in across the organization.\\nThis is important, as it will make any large initiative like cloud migration\\neasier. After this and a competent platform team is in place, you can start\\npushing for the cloud technologies and eventually cloud migration. Here you need\\nto talk directly with the legal team, not via other people.  Have\\nrepresentatives of the platform team sit down with the lawyers and talk through\\nthe risks and how to mitigate them.  This is the only way you can combine the\\ntechnical and legal aspects of this work. Working in silos and not talking to\\neach other is a surefire way to fail.\\n\\n## Autonomy and platform as a product\\n\\nYour platform is a product, and so you need to work as a product team. This\\nmeans continuously improving your product, listening to your users, and building\\nthe features that they need.\\n\\nResearch-based literature like \\"Team Topologies\\" establishes the importance of\\nautonomous teams in modern organizations. Traditional top-down organizations are\\njust not going to be able to have as close of a relationship with their\\nstakeholders as a team that is able to proactively understand the needs of their\\nusers and make their own decisions that push continuous improvement of their\\nproducts. This is why it\'s important, even for infrastructure teams, to be able\\nto own their roadmap and make decisions on what to build when.\\n\\nAs a team you\'re obviously limited to the amount of resources you have and not\\nable to do everything, so understanding the needs of your stakeholders and\\nprioritizing them is essential. You need to do research to know the needs of\\nyour users; sometimes requests don\'t align well with the actual needs. Just\\nbecause someone asks loudly for something, doesn\'t mean it\'s the right fit for\\nyour platform. Saying yes to everything does not result in a good product. Dare\\nto challenge assumptions and ask why.\\n\\n## Abstractions save time\\n\\nIt should go without saying that a platform team\'s job is to make tools that\\nmake product teams\' jobs easier. But it really can\'t be said enough. The better\\nthe tooling you provide, the less you have to do support. This is a win-win for\\neveryone.\\n\\nWhen building tools, think about how you can abstract away complexity. This can\\nbe done in many ways, but we\'ve had great success building an operator that\\nabstracts away the complexity of managing applications on Kubernetes. The\\noperator is called [Skiperator](https://github.com/kartverket/skiperator) and\\nmakes deploying applications on Kubernetes as easy as writing a configuration\\nmanifest.\\n\\n```yaml\\napiVersion: skiperator.kartverket.no/v1alpha1\\nkind: Application\\nmetadata:\\n  namespace: sample\\n  name: sample-two\\nspec:\\n  image: nginxinc/nginx-unprivileged\\n  port: 80\\n  replicas: 2\\n  ingresses:\\n    - foo.com\\n    - bar.com\\n```\\n\\nThe key takeaway here is that abstractions like Skiperator are designed to speak\\nthe language of the user. There is no mention of NetworkPolicies or Istio\\nVirtualServices in the configuration, as these are things that the user\\ngenerally doesn\'t have any knowledge of. Instead, the user can specify things\\nlike \\"I want to expose this service to the internet\\" or \\"I want to run this job\\nevery day at midnight\\". This simplifies the user experience of Kubernetes, which\\nis a complex system, and makes it easier for users to get started.\\n\\nWork smarter not harder.\\n\\n## Build forward- and backwards compatibility\\n\\nWe\'ve had multiple experiences where we\'ve weighed our options and decided to\\nmake a breaking change. Just recently we asked our users to migrate their apps\\nfrom one cluster to another in order to improve the architecture of the\\nplatform. Multiple options were considered, but in the end the scale of the\\nchanges meant that upgrading the clusters in-place would not be practical, so\\nwe commissioned new clusters with the new architecture and asked users to\\nmigrate their apps.\\n\\nIn our case, we had a simple way to migrate, only requiring moving a config file\\nfrom one directory to another to make the change. But even so, this was a time\\nconsuming process for our users, and a laborious process for us to support.\\nThis is because even though the change was simple, it was still a change that\\nrequired testing and validation, and it was a change that was not necessarily\\nthe highest priority for the teams that were asked to make it. So even though\\nthe change was simple, it took months.\\n\\nIf you ask your users to make changes to their applications, you\'re asking a\\nteam that is already busy to do more work. Any changes you ask them to make\\nwill take time, as it would not necessarily be the highest priority for them.\\nTherefore avoiding breaking changes should be a primary goal, so wherever\\npossible building in forward and backwards compatibility by inferring as much as\\npossible from the existing configuration is a good thing.\\n\\nWhen building operators, don\'t change or remove fields that are in use. Use \\ndefault values for new fields, and use lists of objects instead of raw values\\nlike lists of strings as they are easier to extend.\\n\\n## Documentation is key\\n\\nOne thing we keep hearing from our users is the need for more and better\\ndocumentation. This is understandable. When you\'re using a platform, you don\'t\\nwant to have to ask for help all the time - you want to be able to discover\\nplatform features and implement them yourself with the support of good\\ndocumentation.\\n\\nThe point here is that as a platform team you need to prioritize documentation.\\nA task is not done until it has been documented. This way announcing new\\nfeatures will always include a link to the documentation where users can dive\\ndeeper into the feature and how to use it, like the example below.\\n\\n![A Slack thread announcing a new SKIP feature](img/feature-announcement.png)\\n\\nThe bigger challenge here is preventing documentation from going stale. It\'s\\ntoo easy to forget about updating documentation to reflect changes in the\\ncode. Here we can share a few tips from our experience:\\n\\nFirst, the obvious way to keep docs up to date is to allocate time to update\\nthem. One way we do this is that a few times a year we will do a documentation\\ngrooming session where we huddle together and review documentation, rewriting it\\nwhen we find out of date information.\\n\\nA more interesting way to keep docs up to date is changing how you respond to\\nquestions. Instead of answering questions immediately, we should be asking\\nourselves: \\"How can we make sure that this question never gets asked again?\\". In\\nour case we spend some time to write documentation or improve existing\\ndocumentation and reply with a link to the documentation page. This is a triple\\nwin, as you will now have more updated documentation, save time in the future by\\nbeing able to refer to the improved docs instead of writing a lengthy response\\nand the user will now know where to look for answers.\\n\\n## Learn from others\\n\\nWhen building a platform you\'ll quickly learn that you don\'t have all the\\nanswers. You might discuss how to implement a feature with your team, but you\\nmight not have the experience to know what works well in this context. When you\\nget into this situation, an outside perspective can be crucial to avoid making\\ncostly mistakes.\\n\\nOne great advantage of working in the public sector is that we can ask other\\npublic sector platform teams for advice and learn from their experiences. We can\\nalso share our experiences with others, which is usually interesting. Invest\\nsome time in building these relationships of mutual benefit.\\n\\n![Public PaaS presentation](img/public-paas.png)\\n\\nI also want to give special credit to Hans Kristian Flaatten and the [Public\\nPaaS](https://offentlig-paas.no/) network here. Having a shared forum to discuss\\nplatform issues is a strong asset and helps the Norwegian public sector get\\nahead and stay competitive.\\n\\nEven if you work in the private sector, you can still learn from other\\norganizations. Honestly, if you want to learn from someone\'s experiences it\\nnever hurts to ask. Teams generally want to help each other out, and it\'s\\nusually possible to make a trade of some sort. I suggest to offer to give a talk\\non your experiences and ask if they can do the same. It\'s a win-win for both\\nparties.\\n\\n## Conclusion\\n\\nYou may think building a platform is mostly technology, and we\'ve written a lot\\nabout technology in [previous blog\\nposts](/blog/hybrid-kubernetes-in-production-part-1). But it\'s important to\\nremember that building a platform is also about building a community, and\\ncommunities have expectations and needs that go beyond technology. This is a\\nstrength, and not a weakness, as if you\'re able to inspire and motivate your\\nusers you will be able to build a platform that is sustainable and that drives\\npositive change in your organization.\\n\\nBest of luck in your endeavors!"},{"id":"introducing-apps-repositories","metadata":{"permalink":"/blog/introducing-apps-repositories","source":"@site/blog/2024-04-22-apps-repositories.md","title":"Scaling with Argo CD: Introducing the Apps Repo Architecture","description":"What\'s the best way to scale when adding more teams to Argo CD? How can we make sure that we\'re building our GitOps in a way that facilitates self service and security? Kartverket shares our experiences and introduces the apps repo architecture\\n","date":"2024-04-22T00:00:00.000Z","tags":[{"inline":true,"label":"kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":true,"label":"argo-cd","permalink":"/blog/tags/argo-cd"},{"inline":true,"label":"gitops","permalink":"/blog/tags/gitops"}],"readingTime":19.61,"hasTruncateMarker":true,"authors":[{"name":"Eline Henriksen","title":"Tidligere Produkteier og Plattformutvikler","url":"https://eliine.dev","imageURL":"https://github.com/eliihen.png","key":"elinehenriksen","page":null}],"frontMatter":{"title":"Scaling with Argo CD: Introducing the Apps Repo Architecture","description":"What\'s the best way to scale when adding more teams to Argo CD? How can we make sure that we\'re building our GitOps in a way that facilitates self service and security? Kartverket shares our experiences and introduces the apps repo architecture\\n","slug":"introducing-apps-repositories","authors":["elinehenriksen"],"tags":["kubernetes","argo-cd","gitops"],"image":"/img/apps-repo-announcment.jpeg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"20 teams on SKIP: What we\'ve learned along the way","permalink":"/blog/20-teams-on-skip"},"nextItem":{"title":"Crisis Management Exercises","permalink":"/blog/crisis-management-exercises"}},"content":"![A screenshot of Argo CD](./img/argo-3.png)\\n\\nArgo CD is an awesome tool. It helps teams de-mystify the deployment process on\\nKubernetes by providing a visual representation of the deployments in the\\ncluster, and GitOps methodologies gives a consistent and understandable\\nstructure to your configuration files.\\n\\nBut what\'s the best way to scale when adding more teams? How can we make sure\\nthat we\'re building our GitOps in a way that facilitates for self service and\\nsecurity? That\'s what we\'ll discuss in this blog post.\\n\\nKartverket has been using Argo CD and GitOps for several years, and we\'ve built\\nan architecture that solves our needs for scale and self-service. Here we\'ll\\nshare our learnings and discuss why our teams are so happy with our Argo setup.\\n\\n\x3c!--truncate--\x3e\\n\\n## Multi-tenancy in Argo CD\\n\\nSo you\'ve deployed Argo CD on your multi-tenant cluster and given your teams\\naccess to the user interface. Let\'s imagine we now have tens of teams and\\nhundreds of applications in the Argo UI. When we start scaling out to more than\\na handful of users we get into some issues with scale. Examples of these issues\\ncan be:\\n\\n- How do you organize your apps and projects?\\n- How do you make sure no two teams accidentally (or maliciously) use the same\\n  namespace?\\n- How can we make sure teams clean up unused deployment resources?\\n- How do you seamlessly deploy to multiple clusters?\\n\\nAs a platform team we often find ourselves thinking that everyone loves \\ninfrastructure and Kubernetes as much as we do. This is not the case! Most\\npeople have not had the joy of having their childhood ruined by installing\\nLinux on their school laptops and configuring WLAN drivers using ndiswrapper.\\nBelieve it or not, most people just want tools to get out of their way and let\\nthem do their job, be that programming, testing or anything else. Not every team\\nis going to be experts in Kubernetes and Argo. So should we expect all teams to\\nknow what a deletion finalizer is? What about the intricacies of serverside\\napply vs. clientside apply?\\n\\nIt\'s our responsibility as a platform team to make the user experience of\\ndeploying to Kubernetes as user friendly as possible. After implementing an\\narchitecture built with UX in mind we\'ve had the joy of seeing people who are\\nextremely skeptical of Kubernetes and the cloud be won over by how easy it is\\nto get your workloads running on Kubernetes. This is thanks to the consistent\\nuser experience and built-in best practices of the apps-repo architecture.\\nBut we\'re getting ahead of ourselves, first we need to talk about a few\\nabstractions that make this possible.\\n\\n## What are ApplicationSets?\\n\\nIn Argo CD there\'s an advanced feature that allows for automating \\ncreation of Argo CD Applications called\\n[ApplicationSets](https://argo-cd.readthedocs.io/en/stable/user-guide/application-set/).\\nUsing an ApplicationSet we can essentially make a template that generates\\nArgo CD applications based on files or folders in a Git repository, sort of like\\na `ReplicaSet` for `Pods`. Using ApplicationSets we can build in features and\\nassumptions and provide the teams with a user experience that essentially boils\\ndown to \\"add a file to a repo and it gets deployed to the cluster\\". The purest\\nform of GitOps. No messing around with Argo CD applications and projects.\\n\\nA core Argo CD component called the ApplicationSet controller will detect any\\n`ApplicationSet` resources deployed to the cluster and read them. After this, it\\nwill periodically scan the a repo configured in the `ApplicationSet` resource\\nand generate `Application` resources, which in turn scan a repo for manifest\\nfiles and sync them to the cluster. So in other words: `ApplicationSet` ->\\n`Application` -> `Deployments`\\n\\nFor this to work you need a Git repo containing manifest files. You could have\\nthe teams put these manifest files into their source code repositories, but this\\nis [not considered best\\npractice](https://argo-cd.readthedocs.io/en/stable/user-guide/best_practices/#separating-config-vs-source-code-repositories).\\nUsually you would put your manifests into a separate repo so that changes to the\\nmanifests don\'t conflict with changes in the source code. At Kartverket we call\\nthis manifest repo an apps repo.\\n\\n## Introducing apps repositories\\n\\n![A cartoon character standing on a stage pointing to large letters saying \\"Apps repo\\"](./img/apps-repo-announcment.jpeg)\\n\\nThe apps repo is where the product teams put their manifests. It has a consistent\\nstructure and is designed to be read by an Argo CD ApplicationSet. It also has a\\nlot of nifty features that enable self-service which we\'ll get back to.\\n\\nFirst, let\'s have a look at the structure of an apps repo.\\n\\n```\\nteamname-apps/\\n  env/\\n    clustername/\\n      namespace/\\n        example.yaml\\n```\\n\\nIn the simplest of terms, this tree describes where to deploy a given manifest. By\\nusing a directory tree it makes setting up an ApplicationSet for this repo trivial.\\n\\nConsider this example ApplicationSet:\\n\\n```yaml\\napiVersion: argoproj.io/v1alpha1\\nkind: ApplicationSet\\nmetadata:\\n  name: exampleteam-apps\\n  namespace: argocd\\nspec:\\n  generators:\\n    - git:\\n        directories:\\n          - path: env/*/*\\n        repoURL: \'https://github.com/kartverket/exampleteam-apps.git\'\\n        revision: HEAD\\n  goTemplate: true\\n  goTemplateOptions:\\n    - missingkey=error\\n  template:\\n    metadata:\\n      name: \'{{.path.basename}}\'\\n    spec:\\n      destination:\\n        namespace: \'{{ index .path.segments 2 }}\'\\n        name: \'{{ index .path.segments 1 }}\'\\n      project: exampleteam\\n      source:\\n        path: \'{{.path.path}}\'\\n        repoURL: \'https://github.com/kartverket/exampleteam-apps.git\'\\n        targetRevision: HEAD\\n      syncPolicy:\\n        syncOptions:\\n          - CreateNamespace=true\\n        automated:\\n          prune: true\\n          allowEmpty: true\\n          selfHeal: true\\n```\\n\\nWith this ApplicationSet any directory within `env/*/*` will be picked up by\\nthe ApplicationSet controller and a new Argo CD Application will be created\\nbased on the template in the `template` object. This enables a product team\\nto create any number of applications for their products.\\n\\n![A chart showing apps repos synced into Kubernetes](./img/argo-apps-repos.png)\\n\\nAn example use for this is a product team wanting a namespace for each of\\ntheir products. Instead of having to order a new namespace from the platform\\nteam when they create a new product, they can simply create it themselves by\\nadding a new directory with the same name as the namespace they want. A new\\nKubernetes namespace will be automatically created thanks to the\\n`CreateNamespace=true` sync option.\\n\\nEphemeral namespaces, aka. preview namespaces, is another usecase. Say a team\\nwants to review a change before merging it to `main`. They could review the\\nchange in the Pull Request, but this removes us from the end user\'s perspective\\nand is not suitable for non-technical people. With a preview environment the\\nteam will automatically create a new directory in the apps repo when a PR is\\ncreated, and thus get a complete deployment with the change in question. This\\nenables end-to-end testing in a browser, and also allows non-technical people\\nto do QA before a change is merged. When it is merged another workflow can\\nautomatically delete the directory, which cleans up and deletes the preview\\nenvironment.\\n\\nOur convention is that namespaces are formatted with `productname-branch`. This\\nallows teams to have multiple deploys per product, and also multiple products \\nper team. So when a new PR is created all a team needs to do to automate the\\ncreation of a new directory using CI tools like GitHub actions to create a new \\ncommit in the apps-repo. This also enables the flexibility to create it as a PR\\nin the apps-repo, but for ephemeral namespaces, this is usually not necessary.\\n\\nFor example:\\n\\n```\\nfooteam-apps/\\n  env/\\n    foo-cluster/\\n      foo-main/\\n        app.yaml\\n      foo-feature-123/\\n        app.yaml\\n```\\n\\n## Automating and avoiding duplication\\n\\nDepending on the complexity of the apps repo, the amount of products and branches\\nand a subjective \\"ickyness\\" with duplicating files (can you spell DRY?), you have\\nseveral options on how to automate creating new namespaces.\\n\\nSimple repos will probably be fine with directories containing simple yaml-files\\nthat are synced to the cluster. Newer product teams especially appreciate the\\nsimplicity of this approach. To optimize for this you may consider using a \\n`template` directory at the base containing some example files that are copied\\ninto the sub-directories. A pseudo-coded GitHub action that uses a\\n`frontend.yaml` template from the templates directory could look like the\\nfollowing:\\n\\n```yaml\\njobs:\\n  build:\\n    # Build a container image and push it\\n\\n  deploy:\\n    strategy:\\n      matrix:\\n        env: [\'dev\', \'test\', \'prod\']\\n    steps:\\n      # .. Checkout repo & other setup ..\\n\\n      - name: Deploy to ${{ matrix.version }}\\n        run: |\\n          namespace=\\"myapp-${{ github.ref_name }}\\"\\n          path=\\"./env/atkv3-${{ matrix.env }}/$namespace\\"\\n          mkdir -p $path\\n          cp -r templates/frontend.yaml $path/frontend.yaml\\n          kubectl patch --local \\\\\\n            -f $path/frontend.yaml \\\\\\n            -p \'{\\"spec\\":{\\"image\\":\\"${{needs.build.outputs.container_image_tag}}\\"}}\' \\\\\\n            -o yaml\\n          git config --global user.email \\"github-actions@github.com\\"\\n          git config --global user.name \\"GitHub Actions\\"\\n          git commit -am \\"Deploy ${{ matrix.env }} version ${{ github.ref_name }}\\"\\n          git push\\n```\\n\\nThis works for most simple apps. Our experience, however, is that as a team\\nmatures and gets more experienced with Kubernetes and Argo CD, they add more\\ncomplexity and want more control. At this point most teams will migrate to using\\n[jsonnet](https://jsonnet.org) to enable referencing and extending a reusable\\nlibrary shared between multiple components. SKIP also provides some common\\nmanifests via [ArgoKit](https://github.com/kartverket/argokit), a jsonnet\\nlibrary.\\n\\n[Kustomize](https://kustomize.io) is also a common choice, widely used by SKIP\\nfor our own infrastructure, but not really widespread with other teams.\\n\\nDespite Argo supporting [Helm](https://helm.sh/) we mostly avoid using it to\\ncreate reusable templates due to the complexity of templating YAML. Jsonnet is\\nsuperior in this regard.\\n\\n<blockquote className=\\"twitter-tweet\\"><p lang=\\"en\\" dir=\\"ltr\\">Fixing indentation errors in YAML templates in a Helm chart <a href=\\"https://t.co/Dv2JUkCdiM\\">pic.twitter.com/Dv2JUkCdiM</a></p>&mdash; memenetes (@memenetes) <a href=\\"https://twitter.com/memenetes/status/1600898397279502336?ref_src=twsrc%5Etfw\\">December 8, 2022</a></blockquote>\\n\\n## Security considerations\\n\\nYou may be wondering: \\"This seems great and all, but what about the security\\nimplications of allowing teams to create and edit namespaces in a multi-tenant\\ncluster? That seems really dangerous!\\". \\n\\nFirst of all, I love you for thinking about security. We need more people like\\nyou. Second, Argo CD has some great features we can leverage to make this work\\nwithout removing the self-service nature of the apps repo architecture.\\n\\n### Prefixes\\n\\nIn order to make this work we need to give each team a set of prefixes. A\\nprefix will usually be the name of a product that a product team has\\nresponsibility for maintaining. The only important part is that it is unique\\nand that no other teams have been allocated the same prefix. At Kartverket\\nthis is done by the platform team as part of the team onboarding process.\\n\\nThe prefix is used as part of all namespaces that are created by the teams. In\\nthe example namespace `product-feature-123`, `product` is the prefix. By giving\\neach team a set of prefixes it helps them separate products into easily\\nidentifiable namespaces and it ensures that a product team does not accidentally\\nuse another team\'s namespace.\\n\\nSince each product team has an apps repo with the ability to name their\\ndirectories as they wish, how can we enforce this? This is where Argo CD\'s\\nProjects come into play.\\n\\n[Argo CD Projects](https://argo-cd.readthedocs.io/en/stable/user-guide/projects/)\\nprovide a logical grouping of applications, which is useful when Argo CD is used\\nby multiple teams. It also contains a field that allows allowlisting which\\nclusters and namespaces are usable by a project.\\n\\nAdd the following to a Project to only allow this project to create and sync to\\nnamespaces prefixed with `myprefix-`.\\n\\n```\\nmetadata:\\n  name: exampleteam\\nspec:\\n  destinations:\\n  - namespace: \'myprefix-*\'\\n    server: \'*\'\\n```\\n\\nIf you scroll back up to the ApplicationSet example above, you will see that it\\nonly creates applications with the project `exampleteam`. This will automatically\\nwire any applications created to the destination rules we\'ve defined in this\\nproject and therefore deny any attempts by a team to use prefixes that they have\\nnot been allocated.\\n\\nThe crucial part here is that ApplicationSets and Projects are provisioned by the\\nplatform team, and therefore build in these security features. These resources\\nmust not be accessible to the teams, or an attacker can simply add exclusions.\\n\\n### Namespace resources\\n\\nAnother way this could be abused is if a team is able to create Namespace\\nresources in their apps repository. This should be denied using Argo and/or\\ncluster policies.\\n\\nIf a team is able to create namespace resources (or other cluster scoped\\nresources) in their namespace an attacker can use this to break their namespace\\n\\"encapsulation\\". Imagine for example if one could use their apps repo to sync\\na namespace resource named `kube-system` into their `env/foo-cluster/foo-main`\\ndirectory. Argo CD would allow this, as the manifests are read into an Argo CD\\napplication. Then the attacker could delete the namespace and take down the\\ncluster.\\n\\n[![A slide from KubeCon Europe 2024 showing why it\'s a bad idea to let product teams create namespaces](./img/no-edit-namespaces.png)](https://www.youtube.com/watch?v=8Zwftqf8g8w)\\n\\nIt\'s useful in this multi-tenancy scenario to think of namespaces as resources\\nowned by the platform team and namespace-scoped resources as owned by the\\nproduct teams. This is considered a best practice, and was reiterated at [KubeCon\\nEurope 2024 by Marco De Benedictis](https://www.youtube.com/watch?v=8Zwftqf8g8w). \\nAllowing product teams to edit namespaces can open up a ton of attack vectors,\\nlike disabling [Pod Security\\nAdmission](https://kubernetes.io/docs/concepts/security/pod-security-admission)\\ncontrollers, allowing an attacker to create privileged containers which can\\ncompromise the host node. \\n\\nFriends don\'t let friends edit namespaces!\\n\\n## Self service customization\\n\\nSo we set up an ApplicationSet that configures best practices and secure\\ndefaults for product teams! Great! But now that team with experienced cloud\\nengineers really wants to customize their Argo configuration. Maybe they want to\\nconfigure that one app has [auto\\nsync](https://argo-cd.readthedocs.io/en/stable/user-guide/auto_sync/) on, but\\nanother app has it turned off. Maybe they want to disable self-healing for a\\nshort period to manually edit in the cluster. In any case, how can we let teams\\nchange this configuration self-service when applications are provisioned by the\\n`ApplicationSet` resource?\\n\\nWe could let the teams edit the ApplicationSet. In our case this would mean\\nthe teams need to learn about the ApplicationSet abstraction, gotemplate and\\nSKIP\'s internal GitOps repo structure. This is overkill when a team usually just\\nwants to flip a flag between true or false for a directory. There could also be\\nsecurity implications with allowing teams to edit `ApplicationSet` resources\\nthat could break encapsulation, which we want to avoid.\\n\\nAnother option would be to contact the platform team and tell us to change some\\nconfig for them. This is not in line with our thinking, as we want the teams to\\nbe able to work autonomously for most operations like this. It would also mean\\nwe were given a lot of menial tasks which would mean we have less time to do\\nother more meaningful things or become a bottleneck for the teams.\\n\\nA third option is setting the `ApplicationSet` sync policy to `create-only`.\\nThis would confifure the ApplicationSet controller to create Application\\nresources, but prevent any further modification, such as deletion, or\\nmodification of Application fields. This would allow a team to edit the\\napplication in the UI after creation, for example disabling auto sync. This last\\noption is user friendly, but in violation of GitOps principles where config\\nlives in git and not in a database. If you run Argo stateless like we do this\\nwould also mean the changes disappear when the pod restarts.\\n\\nBecause none of these options seemed to be the best, we created a better\\nsolution. By using a combination of generators and the new [template patch](https://argo-cd.readthedocs.io/en/stable/operator-manual/applicationset/Template/#template-patch)\\nfeature in Argo CD 2.8 we can look through every directory in the apps repo\\nfor a configuration file called `config.json`.\\n\\nLet\'s look at an example `config.json` file. This example file is commited in\\nthe apps repo to the `env/foo-cluster/foo-main` directory.\\n\\n```json\\n{\\n  \\"tool\\": \\"kustomize\\",\\n  \\"autoSync\\": false\\n}\\n```\\n\\nThis file is not required, but if this file is found the values configured there\\noverrides a set of default values in the `ApplicationSet` template. These flags\\nare then used to determine how the resulting `Application` will behave. This \\nmeans the team is able to change the values they care about per directory of\\ntheir apps repo\\n\\n```\\nfooteam-apps/\\n  env/\\n    foo-cluster/\\n      foo-main/\\n        config.json\\n        app.yaml\\n      foo-feature-123/\\n        config.json\\n        app.yaml\\n      foo-feature-with-default-config/\\n        app.yaml\\n```\\n\\nAdditionaly, since the platform team is in control of the template we can\\neliminate the ability to maliciously change the template by parsing the inputs\\nin a secure way.\\n\\n### Example ApplicationSet\\n\\nLet\'s look at how we can write an `ApplicationSet` that allows us to use\\n`config.json` files.\\n\\nFirst, we need to configure the `ApplicationSet` to look through all directories,\\nand at the same time use a `config.json` file if it is found. This is perhaps\\nthe least intuitive part of this new `ApplicationSet`, so let\'s walk through it\\nstep by step.\\n\\nFirst we create a merge generator, which will merge two generators. The key\\nthing here is that it only merges if the `key` matches in both generators, so\\nthis allows us to first find all directories (the default), then directories\\nthat contain `config.json` files (the override).\\n\\n```yaml\\n generators:\\n  - merge:\\n      generators:\\n      - # default\\n      - # override\\n      mergeKeys:\\n      - key\\n```\\n\\nNow we\'re going to add the generator from before into the default. The only\\ndifference is we\'re doing this using a matrix generator. Doing this combines the\\nparameters generated by the two child generators, which gives us the values from\\nthe git generator like before, but also a set of default values we can use in\\nour template later if the `config.json` file is not provided.\\n\\nWe\'re also using a value from the git generator to assign a `key` that will\\nuniquely identify this directory for the merge generator later.\\n\\n```yaml\\n generators:\\n  - merge:\\n      generators:\\n      - matrix:\\n          generators:\\n          - git:\\n              directories:\\n                - path: env/*/*\\n              repoURL: \'https://github.com/kartverket/exampleteam-apps.git\'\\n              revision: HEAD\\n          - list:\\n              elements:\\n              - allowEmpty: false\\n                autoSync: true\\n                key: \'{{ .path.basenameNormalized}}\'\\n                prune: true\\n                selfHeal: true\\n                tool: directory\\n      - # override\\n      mergeKeys:\\n      - key\\n```\\n\\nNow we use a variant of the git generator to find all `config.json` files in\\nthe same repo and extract the values from it. Again we\'re using the key field\\nto uniquely identify this directory so that it will be merged with the correct\\ndirectory in the merge generator.\\n\\nWe\'re repeating the default values here as well, since not all fields are\\nrequired and we don\'t want them to be overwritten as null in the resulting\\nmerge.\\n\\n```yaml\\n generators:\\n  - merge:\\n      generators:\\n      - matrix:\\n          generators:\\n          - git:\\n              directories:\\n                - path: env/*/*\\n              repoURL: \'https://github.com/kartverket/exampleteam-apps.git\'\\n              revision: HEAD\\n          - list:\\n              elements:\\n              - allowEmpty: false\\n                autoSync: true\\n                key: \'{{ .path.basenameNormalized}}\'\\n                prune: true\\n                selfHeal: true\\n                tool: directory\\n      - matrix:\\n          generators:\\n          - git:\\n              files:\\n              - path: env/*/*/config.json\\n              repoURL: \'https://github.com/kartverket/exampleteam-apps.git\'\\n              revision: HEAD\\n          - list:\\n              elements:\\n              - allowEmpty: false\\n                autoSync: true\\n                key: \'{{ .path.basenameNormalized}}\'\\n                prune: true\\n                selfHeal: true\\n                tool: directory\\n      mergeKeys:\\n      - key\\n```\\n\\nThat\'s it for the generator! Now we can use these variables in the \\n`templatePatch` field (and other fields). In this case we want to set syncPolicy\\noptions, so we need to use the `templatePatch`, as gotemplates don\'t work for\\nobjects.\\n\\nWe\'re also adding a special case where for `directory` sources (the default) we\\nexclude `config.json` files, as we don\'t want to sync the config file with Argo.\\nThis allows us to extend it later to add options for other tools like Kustomize\\nor Helm.\\n\\nKeep in mind that we don\'t want users to inject maliciously formed patches, so\\nwe cast booleans to booleans. \\n\\n```yaml\\n  templatePatch: |\\n    spec:\\n      source:\\n        directory:\\n      {{- if eq .tool \\"directory\\" }}\\n          exclude: config.json\\n      {{- end }}\\n    {{- if .autoSync }}\\n      syncPolicy:\\n        automated:\\n          allowEmpty: {{ .allowEmpty | toJson }}\\n          prune: {{ .prune | toJson }}\\n          selfHeal: {{ .selfHeal | toJson }}\\n    {{- end }}\\n```\\n\\n## Complete ApplicationSet\\n\\nHere is a complete `ApplicationSet` containing all the features we\'ve discussed\\nso far.\\n\\n```yaml\\napiVersion: argoproj.io/v1alpha1\\nkind: ApplicationSet\\nmetadata:\\n  name: exampleteam-apps\\n  namespace: argocd\\nspec:\\n  generators:\\n  - merge:\\n      generators:\\n      - matrix:\\n          generators:\\n          - git:\\n              directories:\\n                - path: env/*/*\\n              repoURL: \'https://github.com/kartverket/exampleteam-apps.git\'\\n              revision: HEAD\\n          - list:\\n              elements:\\n              - allowEmpty: false\\n                autoSync: true\\n                key: \'{{ .path.basenameNormalized}}\'\\n                prune: true\\n                selfHeal: true\\n                tool: directory\\n      - matrix:\\n          generators:\\n          - git:\\n              files:\\n              - path: env/*/*/config.json\\n              repoURL: https://github.com/kartverket/exampleteam-apps.git\\n              revision: HEAD\\n          - list:\\n              elements:\\n              - allowEmpty: false\\n                autoSync: true\\n                key: \'{{ .path.basenameNormalized}}\'\\n                prune: true\\n                selfHeal: true\\n                tool: directory\\n      mergeKeys:\\n      - key\\n  goTemplate: true\\n  goTemplateOptions:\\n  - missingkey=error\\n  template:\\n    metadata:\\n      name: \'{{.path.basenameNormalized}}\'\\n    spec:\\n      destination:\\n        namespace: \'{{ index .path.segments 2 }}\'\\n        name: \'{{ index .path.segments 1 }}\'\\n      project: exampleteam\\n      source:\\n        path: \'{{.path.path}}\'\\n        repoURL: \'https://github.com/kartverket/exampleteam-apps.git\'\\n        targetRevision: HEAD\\n      syncPolicy:\\n        managedNamespaceMetadata:\\n          labels:\\n            app.kubernetes.io/managed-by: argocd\\n            pod-security.kubernetes.io/audit: restricted\\n            team: exampleteam\\n        syncOptions:\\n        - CreateNamespace=true\\n        - ServerSideApply=true\\n        - PrunePropagationPolicy=background\\n  templatePatch: |\\n    spec:\\n      source:\\n        directory:\\n      {{- if eq .tool \\"directory\\" }}\\n          exclude: config.json\\n      {{- end }}\\n    {{- if .autoSync }}\\n      syncPolicy:\\n        automated:\\n          allowEmpty: {{ .allowEmpty | toJson }}\\n          prune: {{ .prune | toJson }}\\n          selfHeal: {{ .selfHeal | toJson }}\\n    {{- end }}\\n```\\n\\n\\n## Results\\n\\nWith Argo CD and the apps repo architecture, we\'ve seen some real improvements\\nin our deploy system. Teams find it to be incredibly intuitive to just update a\\nfile in Git and have it be instantly reflected in Argo CD and Kubernetes,\\nespecially when combined with Argo CD auto-sync.\\n\\nOnboarding new teams is quick and easy, since just putting files into a Git repo\\nis something most developers are already familiar with. We just show them the\\nstructure of the apps repo and they\'re good to go. A team can go from not having\\nany experience with Kubernetes to deploying their first application in a matter\\nof minutes.\\n\\nMigrating from one cluster to another is also a breeze. Just move manifests from\\none directory under `env` to another, and the ApplicationSet will take care of\\nthe rest. This is especially useful for teams that want to start developing with\\nnew cloud native principles on-premises, modernizing the application and\\neventually moving to the cloud.\\n\\nI feel the key part of this architecture is the `config.json` file. It allows\\na degree of customization that is not possible with the default `ApplicationSet`\\ntemplate and was to us the last missing piece. It allows teams to change\\nconfiguration without needing to know about the `ApplicationSet` abstraction,\\nand it allows the platform team to enforce security and best practices.\\n\\n### Tradeoffs\\n\\nBut of course, there are some drawbacks. Like always, it\'s tradeoffs all the\\nway down. \\n\\nSince a product team uses an apps repo to organize their apps, moving apps\\nfrom one team to another will require migrating files from one repo to another.\\nThis will require some manual work to prevent Argo deleting the entire namespace\\nwhen the directory is removed from the old repo. Usually this is not a big\\nissue, and moving projects between teams happens very rarely, but it\'s something\\nto keep in mind.\\n\\nThere is also a risk that a team could accidentally delete a namespace by\\nremoving a directory in the apps repo. We have mitigated this by disabling\\nauto-sync for most mission critical applications in production.\\n\\nAnd finally, projects that don\'t have clear ownership or shared ownership can\\nbe tricky to place into a repo. You could make an apps repo for a \\"pseudo-team\\"\\nconsisting of the teams that need access, but generally we find that it\'s better\\nthat all products have a clear singular main owner. This also prevents \\n[diffusion of responsibility](https://en.wikipedia.org/wiki/Diffusion_of_responsibility).\\n\\n## Thank you for reading!\\n\\nWe hope you found this article helpful and informative. Getting into\\n`ApplicationSets` can be a bit tricky, so we hope we managed to convey the most\\nimportant parts in a clear and understandable way. Thanks for reading!\\n\\nWe recently created a Mastodon account [@kv_plattform](https://mastodon.social/@kv_plattform)!\\nIf you want to contact us or discuss this article, feel free to reach out to us\\nthere."},{"id":"crisis-management-exercises","metadata":{"permalink":"/blog/crisis-management-exercises","source":"@site/blog/2024-02-19-crisis-mgmt.md","title":"Crisis Management Exercises","description":"Crisis management and disaster recovery exercises are a great way to learn and to refine your processes and documentation! You should do it too!\\n","date":"2024-02-19T00:00:00.000Z","tags":[{"inline":true,"label":"crisis-management","permalink":"/blog/tags/crisis-management"},{"inline":true,"label":"disaster-recovery","permalink":"/blog/tags/disaster-recovery"},{"inline":true,"label":"exercises","permalink":"/blog/tags/exercises"}],"readingTime":9.49,"hasTruncateMarker":true,"authors":[{"name":"Thomas Berg","title":"Plattformutvikler","url":"https://github.com/berg-thom","imageURL":"https://github.com/berg-thom.png","key":"thomasberg","page":null}],"frontMatter":{"title":"Crisis Management Exercises","description":"Crisis management and disaster recovery exercises are a great way to learn and to refine your processes and documentation! You should do it too!\\n","slug":"crisis-management-exercises","authors":["thomasberg"],"tags":["crisis-management","disaster-recovery","exercises"],"image":"/img/crisis_3.jpeg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Scaling with Argo CD: Introducing the Apps Repo Architecture","permalink":"/blog/introducing-apps-repositories"},"nextItem":{"title":"Hybrid Kubernetes in production pt. 3","permalink":"/blog/hybrid-kubernetes-in-production-part-3"}},"content":"![Crisis management exercise](img/crisismanagamentexercise.jpeg)\\n\\nEvery IT organization with some semblance of sanity has at least one crisis management or disaster recovery plan, and probably several, depending on the scope and severity of the scenario.\\nRanging from \\"one of our more important applications is experiencing issues in production\\" through \\"everything is on fire\\" on to total loss of data,\\na good crisis management or disaster recovery plan should help you retain business continuity or at the very least ensure a return to normal operations within the shortest possible timeframe.\\nBut when was the last time you actually found the time to test your plans and ability to respond to a crisis?\\n\\n\x3c!--truncate--\x3e\\n\\nAnd we\'re not just talking a theoretical abstract exercise - we\'re talking an actual, realistic scenario involving downtime, troubleshooting, configuration and restoration of services in an actual live environment,\\npreferably as close to production in terms of architecture as possible. \\nBefore you have had the chance to test every single point of your plans, can you actually be sure that they will work?\\nAnd as times change, so do applications, configurations, hardware, architecture - even personnel. New people are added to your team, and people leave - you both need to onboard the new ones and make sure that the knowledge and skillset of people leaving are retained in some fashion.\\nHave your plans been updated to take all these factors into account?\\n\\nLet us take you on a journey through a couple of SKIPs most recent crisis management exercises and explore what happened,\\nhow we handled it and what we learned from it.\\n\\n##  Exercise 1: Malicious actor\\n![Illustration of malicious actor](img/hacker3.jpeg)\\n\\nThe first exercise scenario revolved around a malicious actor gaining privileged access to our production Kubernetes cluster, simulated in this case by our internal sandbox cluster.\\nAdmittedly, it was somewhat difficult to set up a realistic scenario without outright disabling some of our security tools,\\nso in the end we simulated a hostile takeover of the user account belonging to the person responsible for planning and running the exercise.\\n\\nThe first sign that something was amiss was an alert from our [Sysdig Secure](https://sysdig.com/products/platform/) toolset, a [Falco](https://falco.org)-based agent software which continually monitors our cluster \\nfor signs of abnormal activity according to a predefined ruleset and provides a SaaS portal for further analysis and management of threats. \\n(We will cover more of our security features and mechanisms and how we try to build a modern kubernetes based application platform with built-in security and zero trust in a future blog post.)\\nAfter initial examination, we found that the incident was of such a nature that we engaged our crisis management plan in order to investigate, contain and mitigate the incident.\\nWe simulated communication with the organization-level crisis management team, having regular meetings in order to keep them informed of progress.\\nSystematic examination of logs and audit logs soon turned up suspicious activity confined to one specific platform developer account, and the\\ndecision was made to immediately suspend (simulated in this case) the account, removing all access to organizational systems and in effect locking it out.\\nSimultaneously, the malicious software was removed once enough evidence was secured in order to further analyze the actions and impact of it.\\nThe exercise was announced as ended once we suspended the compromised user account and removed the malicious application while retaining and analyzing enough logs, forensic captures and other traces of activity.\\n\\n\\n\\n## Exercise 2: \\"Everything is on fire\\"\\n![Illustration of malicious actor](img/serverroomonfire.jpeg)\\n\\nThe second exercise scenario was somewhat more involved, taking place over two days. The incident itself was as follows:\\nA software update or rogue script caused catastrophic hardware failure in production infrastructure, necessitating creation of a \\nnew Kubernetes cluster from scratch. Once the cluster itself and all underlying infrastructure had been created and configured, it would then be up to our platform team to \\ndeploy all necessary IAM configuration, service accounts, RBAC and supporting systems (Istio, ArgoCD ++) needed to deploy workloads and restore normal operations.\\nThe exercise itself focused on this second phase of restoration, as the infrastructure configuration and cluster creation itself is done by another team, with little involvement by our platform team members. \\n\\nThe failure itself was simulated by having our infrastructure team wipe our sandbox environment and present us with a clean-slate Kubernetes cluster. \\nWe called an all-hands meeting and set to work restoring services right away. Right at the onset, we recognized that this was a golden opportunity\\nboth to ensure that our documentation was up-to-date, consistent and easy to follow, as well as give our three newest team members some much-needed\\nexperience and insight into setting up our services from scratch.\\nWe therefore decided that the newest team members would be the ones to actually execute all the \\nactions outlined in our documentation, while the rest of us followed along and made notes, updated documentation and otherwise provided guidance throughout the process.\\n\\nThe first run-through of the recovery process took around 2-3 hours before everything was in working order. Keep in mind that we took the time to update our documentation and explain everything we did while we were working, so in a real-life scenario this would have been even quicker. Once the IAM, RBAC, Istio and ArgoCD was up and running, it was merely a matter of using ArgoCD to synchronize and deploy all relevant workloads.\\nAfterwards, we had a meeting to discuss the process and what experiences we gained from it. Based on the feedback from this meeting, we made further adjustments and updates to our documentation\\nin order to make it even easier to follow on a step-by-step basis, focusing on removing any ambiguity and put any \\"tribal\\" knowledge among our platform developers into writing.\\nThis ensured that we are way less dependent on the knowledge and skillset of specific people, enabling any team member to contribute to recovery efforts by simply following the documentation.\\n\\nThe newest team members greatly enjoyed being responsible for the recovery effort itself, and expressed a wish to run through the scenario again in order to refine their skills and further improve the documentation.\\nTherefore, we decided to set aside most of day 2 to do just that. We had the infrastructure team tear down and setup the cluster again, and let the newest team members loose on it - this time on their own without guidance - an additional two times.\\nThe last run-through of the exercise took between 30 and 60 minutes, a significant improvement from the initial attempt.\\n\\nAll in all, we considered the exercise to be a great success, with many important lessons learned and a substantial improvement in the quality of our documentation and crisis management plans.\\n\\n\\n## What did we learn?\\n![Illustration of malicious actor](img/whatdidwelearn.jpeg)\\n### Lesson 1: You are only as good as your documentation\\nDocumentation is vitally important during a crisis, and should be detailed enough that any team member may follow it on a step-by-step basis and be able to restore normal service, even with minimal knowledge and during a stressful situation.\\nThis ensures that you avoid being dependent upon key personnel that might or might not be available during a crisis scenario, and also ensures that you retain vital institutional knowledge even when team members move on to different tasks or even new jobs.\\n\\n### Lesson 2: Logging, logging, logging! Oh, and monitoring too!\\nHaving the ability to search through logs of all parts of our system greatly simplifies any incident management, whether the incident revolved around malicious actors or other factors.\\nBut logs by themselves are not sufficient - you need some sort of monitoring and alerting system in order to alert on and react to abnormal situations/behaviour in your systems.\\nIdeally, you should be able to react on these alerts instead of messages from users - or worse, customers - that something is wrong.\\n\\n### Lesson 3: Test your plans!\\nMerely having plans, routines and documentation is insufficient. Unless they have been thoroughly tested and their quality assured through crisis exercises in realistic scenarios and conditions, they should be treated as flawed and unreliable until the opposite is proven. \\nRunning crisis management exercises is a great way to expose flaws, insufficiencies and outdated documentation, and careful note-taking and postmortems should be the norm throughout the exercise in order to easily identify and update weak spots in your plans and documentation. As systems and circumstances change, so should plans and documentation too in order to reflect the new order of the day.\\n\\n### Lesson 4: Communicate!\\nOpenness and communication is critical during both exercises and real-world crisis scenarios. Plans should always involve key points of communication - who needs to be informed, whose responsibility it is to keep said people informed, and the frequency, scope and format of information to disseminate.\\nThis also applies to communication afterwards. Anyone in your organization should be able to understand what happened, how it was solved and what lessons were learned from it. \\nIn Kartverket, we solve this by writing postmortems about incidents, summing up the incident itself and what we learned from it. We favour [Blameless Postmortems](https://www.atlassian.com/incident-management/postmortem/blameless), enabling us to quickly and thoroughly analayze and document all aspects of an incident without focusing on individual mistakes   and avoid passing blame.\\nThis contributes to a culture of openness, learning and improvement. Hoarding information and disseminating it only on a \\"need-to-know\\" basis only breeds distrust and contempt, as does a culture that focuses on blaming and punishing people for mistakes instead of learning from them.\\nA further bonus when communicating the happenings and results of your crisis management exercises is the potential to inspire others - when people see the great results and lessons you yourselves have gained from such exercises, they might want to try it with their own systems and teams. \\n\\n### Lesson 5: Let the \\"newbies\\" handle it\\nPutting our newest team members in charge of the recovery operations was a great learning experience for them, as well as enabling us to quickly find flaws and shortcomings in our documentation and crisis management plans.\\nIt is also a great confidence booster, because if they succeed, they\'ll gain valuable insight and positive experiences with setting up all those scary critical systems from scratch - and if they don\'t succeed, well, that\'s not their fault, it was because the documentation and training was insufficent to enable them to handle the situation!\\n\\n### Lesson 6: Crisis exercises as team building\\nCrisis exercises are fun and contribute to better teamwork! They bring everyone together in order to achieve a common goal - get things up and running again as quickly as possible. Combine it with \\"pair programming\\" - that is, if possible make sure at least two people are working on any given task together - this helps facilitate cooperation and communication, and provides an extra set of eyes to help catch any manual errors or deviations from the plan.\\n\\n## Thank you for reading!\\nWe appreciate you taking the time to read through this blog post. We have learned quite a lot (and had lots of fun) through our approach to crisis management exercises. We hope our experiences and thoughts regarding this subject has been interesting, and that they may inspire others to start doing crisis management exercises as well."},{"id":"hybrid-kubernetes-in-production-part-3","metadata":{"permalink":"/blog/hybrid-kubernetes-in-production-part-3","source":"@site/blog/2024-01-25-anthos-3.md","title":"Hybrid Kubernetes in production pt. 3","description":"In this final installment of the Anthos series, we\'ll talk about what we learned on the way building hybrid infrastructure\\n","date":"2024-01-25T00:00:00.000Z","tags":[{"inline":true,"label":"anthos","permalink":"/blog/tags/anthos"},{"inline":true,"label":"kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":true,"label":"hybrid","permalink":"/blog/tags/hybrid"}],"readingTime":10.13,"hasTruncateMarker":true,"authors":[{"name":"Eline Henriksen","title":"Tidligere Produkteier og Plattformutvikler","url":"https://eliine.dev","imageURL":"https://github.com/eliihen.png","key":"elinehenriksen","page":null}],"frontMatter":{"title":"Hybrid Kubernetes in production pt. 3","description":"In this final installment of the Anthos series, we\'ll talk about what we learned on the way building hybrid infrastructure\\n","slug":"hybrid-kubernetes-in-production-part-3","authors":["elinehenriksen"],"tags":["anthos","kubernetes","hybrid"],"image":"/img/anthos-6.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Crisis Management Exercises","permalink":"/blog/crisis-management-exercises"},"nextItem":{"title":"SKIP on Plattformpodden!","permalink":"/blog/skip-on-plattformpodden"}},"content":"![Anthos in Google Cloud](img/anthos-6.jpg)\\n\\nIn this final installment of the Anthos series, we will talk about what we\\nlearned on the way to building hybrid infrastructure at [Kartverket](https://kartverket.no/en). \\n\\nIt\'s been a long journey, and there\'s plenty of things we\'ve learned along the\\nway in building a hybrid Kubernetes platform. We\'ll try to share some of those\\nhard earned lessons in this post.\\n\\n\x3c!--truncate--\x3e\\n\\nThis newsletter is the final entry of a three part series about Anthos in\\nKartverket.\\n\\n1. [Why we chose Anthos](/blog/hybrid-kubernetes-in-production-part-1)\\n2. [How we run Anthos](/blog/hybrid-kubernetes-in-production-part-2)\\n3. Benefits and what we would have done differently (You are here!)\\n\\n## Do you really need hybrid?\\n\\nWhen we started out, there was an assumption that it was simply impossible to\\nuse the cloud. This came from all sides of the organization, so this was \\ntaken as a given. SKIP was therefore started as a project to build an on-premise\\nKubernetes platform to service our needs as a transition to cloud native\\ndevelopment principles.\\n\\nAs we moved along, a lot of these assumptions got challenged. We found that\\nmost of these assumptions were based on misunderstandings or a lack of a deeper\\nunderstanding of cloud technologies and the surrounding legal aspects. This led\\nto a fear of the unknown, and subsequent inaction. In the end it turned out\\nthat quite a lot of our workloads could indeed run in the public cloud, given some\\nminor adjustments.\\n\\nHad we started out with the knowledge we have now, we would probably have\\nstarted with a public cloud provider, and then moved to hybrid when and if\\nwe saw a need for it. Using a cloud provider\'s managed Kubernetes offering\\nis significantly easier than running your own, and you can get started much\\nfaster, with less risk.\\n\\nGiven our organization, we would probably have ended up with hybrid anyway, but\\nthat complexity could potentially have been moved down the timeline to a point\\nwhere the platform was more mature.\\n\\nStarting with hybrid is a massive undertaking, and you should have a good reason\\nfor doing so. Do you need hybrid, or do you just need to mature your\\norganization? If you do, reduce the scope of the initial work to get to a\\nworkable platform, and preferably start in the cloud, adding hybrid features\\nlater. If you\'re not sure, you probably don\'t need hybrid.\\n\\n## Hybrid gives your organization flexibility\\n\\n![Illustration of cloud components](img/cloud-1.jpg)\\n\\nNow that we\'ve built a platform that seamlessly runs workloads in both public\\ncloud and on-premise, we have a lot of flexibility in where we run our workloads\\nand how we manage them. Our experience is that this makes it easier for the\\norganization to mature legacy workloads.\\n\\nAll our greenfield projects are written with cloud native principles in mind,\\nwhich makes it trivial to run them in the cloud. Legacy workloads, however, are\\nnot so lucky. They are often written with a lot of assumptions about the\\nunderlying infrastructure and are not cognizant of the resources they use. This\\nmeans they are a poor fit to lift and shift to the cloud, as they will often be\\nexpensive and inefficient.\\n\\nWith a hybrid platform, we can use our on-premise offering as a spring board for\\nmodernization. Product teams will start by shifting their app to our on-premise\\nKubernetes platform, and then gradually modernize it to be cloud native. \\nThis method gives a few immediate benefits from the lift and shift like better\\nobservability, developer experience and security features but also gives fewer of the\\ndrawbacks, as the on-premise cloud is closer to the existing dependencies than a\\npublic cloud. Once this is done, smaller chunks kan be rewritten as\\nmicroservices and moved to the cloud, communicating with the monolith seamlessly\\nover the hybrid network. This is sometimes referred to as the [strangler\\napplication](https://microservices.io/patterns/refactoring/strangler-application.html).\\n\\nThis method significantly reduces the scope of refactoring, as one can focus on\\ngradually rewriting smaller modules instead of rewriting the entire application.\\n\\n## Service mesh is hard, but maybe a necessary evil to make hybrid less painful\\n\\n![Illustration of service mesh with Istio logo](img/mesh-1.png)\\n\\nOh my word how we have struggled with service mesh.\\n\\nStarting from nothing with a goal of providing a secure-by-default zero-trust\\nnetwork layer with observability and traffic control is quite an undertaking,\\nespecially when you pair that with setting up a new kubernetes-based\\ninfrastructure from scratch. Istio is famously complex, and we\'ve had our fair\\nshare of that.\\n\\nSo how do we feel about Istio? There are various opinions in the team, but if we\\naverage them all out, we\'re content. It\'s quite complex and can be hard to\\ndebug, but it does the job. As we\'ve matured and gotten more experience with\\nIstio, we\'ve also started to see more benefits, like extensions for [handling\\nOAuth2](https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/oauth2_filter)\\nand the traffic control features for gradual rollouts which we used for\\ncanary-testing the migration of some of our larger applications to SKIP. Not all\\nof these features, like EnvoyFilters, are supported by Anthos Service Mesh (ASM),\\nwhich is why we\'re exploring using upstream Istio instead of ASM.\\n\\nOne thing we quickly learned is to not let the product teams configure the\\nservice mesh directly using service mesh resources. This is a recipe for\\ndisaster. We tried this in the beginning, and first of all it\'s a huge\\ncomplexity burden for the product teams. We also started getting a lot of\\nweird issues when product teams would configure the mesh in ways that broke\\ntheir encapsulation. Since the service mesh is a cluster-wide feature, if one\\nteam makes an invalid configuration, it can break other teams\' workloads.\\nKubernetes namespaces be damned. We\'ve therefore moved to a model where the\\nplatform team provides an abstraction through\\n[Skiperator](https://github.dev/kartverket/skiperator) which configures the\\nservice mesh on their behalf.\\n\\nFinally, I think it\'s prudent to ask yourself wether or not you actually need a\\nservice mesh. If you\'re running a small cluster with a few services, you\'ll\\nprobably be fine with using the built-in Kubernetes features like Ingress and\\nNetwork Policies. The observability features are nice, but you can get most of\\nthem with a combination of [instrumentation and\\nGrafana](https://grafana.com/docs/tempo/latest/metrics-generator/service_graphs/).\\n\\nIf you need service mesh then limit the scope until you get comfortable with the\\nmesh, for example start with just mTLS and observability, and then add zero\\ntrust networking features later.\\n\\nAlso keep in mind there is a lot of competition in the service mesh space, and\\nthere are some interesting alternatives to Istio, like\\n[Linkerd](https://linkerd.io/) and the up-and-coming [Cilium Service\\nMesh](https://cilium.io/use-cases/cluster-mesh/).\\n\\n## Anthos helps you as a platform team getting started with best practices.. Even if you plan to move to open source components later\\n\\n![Google Anthos logo](img/anthos-2.png)\\n\\nWhen our platform team started out a few years ago, we picked some of the\\nbrightest cloud engineers from within the organization and combined them with\\nsome consultants to work on the platform. Most of these engineers had some\\nexperience working with Kubernetes and cloud, but not building something of this\\nscale from scratch. The first months would therefore be a learning experience for\\nmost of the team.\\n\\nI think a lot of teams will be in a similar situation, and this is where a\\nmanaged service like Anthos can be a huge help. Anthos is built with best\\npractices in mind, so a lot of the architecture decisions were built-in to the\\ninstaller. Choosing a managed offering, even when running on-prem has therefore\\nhelped us deliver value to the product teams much quicker than if we had to\\nbuild everything from scratch. \\n\\nWhat\'s important to point out is that choosing something that is managed does\\nnot rule out using open source components later. We started out using all the\\nparts that Anthos gave us, including service mesh, logging, monitoring and\\nconfiguration management. Managed services do come with some tradeoffs, however,\\nas you lose some of the finer control of the platform. As the team has matured\\nand gained experience, we\'ve started to replace some of these components with\\nopen source alternatives, which has helped us save money and gain more control\\nover our platform. This has the downside of having to maintain these\\ncomponents ourselves, but with more experience in the team, this is a tradeoff\\nwe feel is worth it.\\n\\nEven though we\'re increasingly using more open source components, we don\'t\\nregret using a paid managed offering in the beginning. It helped us get started\\nand make the right decisions early on, and we\'re now in a position where we can\\ncapitalize on that great start.\\n\\n## Keep in mind autoscaling when choosing licensing models\\n\\n![Autoscaling](img/anthos-7.png)\\n\\nThis may be an obvious point to some of the more experienced platform engineers\\nout there, but it was still something that we had to learn. When we started out,\\nwe appreciated the simplicity of SaaS products that billed per node, as it made\\nit easy to predict costs. We could simply look at the number of nodes we had\\nrunning and multiply that with the price per node to get a relatively accurate\\nestimate of what this offering would cost. This would turn out to be a double\\nedged sword, however.\\n\\nIt is safe to assume that one of the reasons people choose Kubernetes is the ability\\nto scale workloads easily. This could be scaling up to handle more traffic, or\\nscaling down to save money. This is a great feature, but as the number of workloads\\ngrow, the provisioned nodes will start to become insufficient and new nodes will\\nbe provisioned. With Kubernetes and Anthos on VMware this can be done\\nautomatically, which is a fantastic feature. \\n\\nThe problem arises when you scale out more nodes and have a static license that\\nbills per node. We\'ve made the mistake of getting contracts with two (now just\\none) SaaS providers where we order a set of nodes, let\'s say 10, and when\\nworkloads scale up, we end up with more than 10 nodes. This means we\'re not\\nrunning that SaaS-service\'s agents on the new nodes, which can be anything from\\ninconvenient to critical, depending on the service. In the end we\'ve had to\\nrestrict our node scaling to avoid this issue, which goes against the whole\\nethos of Kubernetes. We\'re also provisioning bigger nodes than we need to avoid\\nscaling out, which can be suboptimal.\\n\\nWe\'re now working with the vendors to get a more flexible license that bills per\\nnode on demand, but this is something to keep in mind when choosing a SaaS\\noffering. Try to factor in the future scaling needs of your platform when\\npurchasing SaaS services.\\n\\n\\n## Summary\\n\\nTo summarize: We\'ve learned a lot on our journey to building a hybrid Kubernetes\\nplatform. Over the last few years we\'ve iterated on our platform and learned\\nlots of great lessons. It\'s been a huge help and privilege to have the support\\nof our organization, especially in terms of us being allowed to fail and learn\\nfrom our mistakes. The Norwegian saying \\"it\'s never too late to turn around\\"\\ncomes to mind, as we\'ve changed course several times on our journey, sometimes\\nto the annoyance of our product teams who depend on a stable platform - but in\\nthe end we\'ve ended up with a better product - a platform we can be proud of and\\nthat our product teams love using.\\n\\nThanks for reading this series on Anthos and hybrid Kubernetes. We hope you\'ve\\nlearned something from our experiences, and that our hard earned lessons can\\nhelp you on your journey to building a hybrid Kubernetes platform.\\n\\n_Disclaimer - Google, GKE and Anthos are trademarks of Google LLC and this website is not\\nendorsed by or affiliated with Google in any way._"},{"id":"skip-on-plattformpodden","metadata":{"permalink":"/blog/skip-on-plattformpodden","source":"@site/blog/2023-12-26-podcast.md","title":"SKIP on Plattformpodden!","description":"SKIP has been featured on the Plattformpodden podcast! Give it a listen!\\n","date":"2023-12-26T00:00:00.000Z","tags":[{"inline":true,"label":"podcast","permalink":"/blog/tags/podcast"},{"inline":true,"label":"plattformpodden","permalink":"/blog/tags/plattformpodden"}],"readingTime":0.25,"hasTruncateMarker":true,"authors":[{"name":"Eline Henriksen","title":"Tidligere Produkteier og Plattformutvikler","url":"https://eliine.dev","imageURL":"https://github.com/eliihen.png","key":"elinehenriksen","page":null},{"name":"Vegar Andersen","title":"Teknologileder Plattform","url":"https://github.com/Veg4r","imageURL":"https://github.com/veg4r.png","key":"vegarandersen","page":null}],"frontMatter":{"title":"SKIP on Plattformpodden!","description":"SKIP has been featured on the Plattformpodden podcast! Give it a listen!\\n","slug":"skip-on-plattformpodden","authors":["elinehenriksen","vegarandersen"],"tags":["podcast","plattformpodden"],"image":"/img/plattformpodden.jpg","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Hybrid Kubernetes in production pt. 3","permalink":"/blog/hybrid-kubernetes-in-production-part-3"},"nextItem":{"title":"Hybrid Kubernetes in production pt. 2","permalink":"/blog/hybrid-kubernetes-in-production-part-2"}},"content":"![Vegar and Eline in the studio](/img/plattformpodden.jpg)\\n\\nVery recently, SKIP was featured on the\\n[Plattformpodden](https://plattformpodden.no) podcast! Vegar and Eline were\\ninvited to talk about SKIP, how it came to be and what it\'s like to work on it.\\n\\n\x3c!--truncate--\x3e\\n\\nGive it a listen!\\n\\nhttps://plattformpodden.no/episode/6"},{"id":"hybrid-kubernetes-in-production-part-2","metadata":{"permalink":"/blog/hybrid-kubernetes-in-production-part-2","source":"@site/blog/2023-12-14-anthos-2.md","title":"Hybrid Kubernetes in production pt. 2","description":"In this second installment of the Anthos series, we\'ll talk about how we run Anthos and hybrid cloud in Kartverket. \\n","date":"2023-12-14T00:00:00.000Z","tags":[{"inline":true,"label":"anthos","permalink":"/blog/tags/anthos"},{"inline":true,"label":"kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":true,"label":"hybrid","permalink":"/blog/tags/hybrid"}],"readingTime":18.52,"hasTruncateMarker":true,"authors":[{"name":"Eline Henriksen","title":"Tidligere Produkteier og Plattformutvikler","url":"https://eliine.dev","imageURL":"https://github.com/eliihen.png","key":"elinehenriksen","page":null},{"name":"B\xe5rd Ove Hoel","title":"Tech Lead og Plattformutvikler","url":"https://github.com/bardove","imageURL":"https://github.com/bardove.png","key":"bardovehoel","page":null}],"frontMatter":{"title":"Hybrid Kubernetes in production pt. 2","description":"In this second installment of the Anthos series, we\'ll talk about how we run Anthos and hybrid cloud in Kartverket. \\n","slug":"hybrid-kubernetes-in-production-part-2","authors":["elinehenriksen","bardovehoel"],"tags":["anthos","kubernetes","hybrid"],"image":"/img/anthos-2.png","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"SKIP on Plattformpodden!","permalink":"/blog/skip-on-plattformpodden"},"nextItem":{"title":"Hybrid Kubernetes in production pt. 1","permalink":"/blog/hybrid-kubernetes-in-production-part-1"}},"content":"![Anthos in Google Cloud](img/anthos-4.jpg)\\n\\nIn this second installment of the Anthos series, we will talk about how we run\\nAnthos and hybrid cloud at [Kartverket](https://kartverket.no/en). We\'ll touch\\non the hardware, the software, and the processes we use to keep it running.\\n\\nBy the end we hope that we\'ll have de-mystified Anthos a bit, and maybe given\\nyou an idea of what it takes to run Anthos in production.\\n\\nIf you haven\'t read the first part, you can find it\\n[here](/blog/hybrid-kubernetes-in-production-part-1).\\n\\n\x3c!--truncate--\x3e\\n\\nThis newsletter is the second of the three part series about Anthos in\\nKartverket.\\n\\n1. [Why we chose Anthos](/blog/hybrid-kubernetes-in-production-part-1)\\n2. How we run Anthos (You are here!)\\n3. [Benefits and what we would have done differently](/blog/hybrid-kubernetes-in-production-part-3)\\n\\n## Installation and upgrades\\n\\n![Illustration of the cluster architecture](img/anthos-5.png)\\n\\nWe have been early adopters of Anthos, so when doing the install we did not have\\noptions for controlplane architecture. We wanted to use existing underlying\\nVMware infrastructure, so the nodes in our clusters are VMs, provisioned by\\nscripts provided by Google. Our cluster is installed with\\n[kubeception](https://kubernetes.io/blog/2017/01/how-we-run-kubernetes-in-kubernetes-kubeception/)\\ncontrolplane architechture, this no longer the only, or recommended way. The\\nrecommended model is [Controlplane\\nV2](https://cloud.google.com/anthos/clusters/docs/on-prem/latest/how-to/create-user-cluster-controlplane-v2),\\nwhere the controlplane nodes for the user cluster are in the user cluster\\nitself. \\n\\nIn the kubeception model, Kubernetes clusters are nested inside other Kubernetes\\nclusters. Specifically, the control plane of the user clusters runs in an\\nadmin-cluster. For each on-premise cluster created, a new set of nodes and a\\nnamespace are created in the admin cluster.\\n\\nTo install and make changes to the admin cluster, an admin workstation is\\nrequired, which must be located in the same network as the admin cluster. All\\nconfigurations are done using a CLI tool called `gkectl`. This tool handles most\\ncluster administration tasks, and the cluster specific configuration is provided\\nin YAML files.\\n\\nOur cluster setup is more or less static, and most cluster administration tasks\\ninvolve upgrading or scaling existing clusters. The SKIP team has a cluster\\nreferred to as \u201csandbox\u201d, which is always the first recipient of potentially\\nbreaking changes. After testing in sandbox, we\'ll deploy changes to both\\ndevelopment and test environments, and if nothing breaks, we roll out the\\nchanges to our production environment. This is mostly done outside work-hours,\\nalthough we have not experienced downtime during cluster upgrades. Here is the\\ngeneral workflow for upgrading:\\n\\n1. Upgrade your admin workstation to the target version of your upgrade.\\n2. From your admin workstation, upgrade your user clusters.\\n3. After all of the user clusters have been upgraded, you can upgrade your admin\\n   cluster from the admin workstation.\\n\\nWe have tried using [Terraform](https://www.terraform.io/) where possible to\\nsimplify the setup. This can not be done in the same way for clusters using the\\nkubeception model. When we migrate to Controlplane V2 however, clusters can be\\nmanaged via GCP, and we can finally start using terraform for our on-premise\\ncluster config in the same way as for our GKE clusters, and GCP configuration in\\ngeneral.\\n\\n\\n## GCP integration\\nWhen working with an on-premise Anthos cluster, some of the nice-to-have\\nfeatures of a standard GKE cluster have been lost. However, recently Anthos on\\nVMware clusters have gradually received more and more features compared to GKE\\nclusters. \\n\\n### IAM and Groups\\nSince we were early adaptors of Anthos, we had to endure not being able to\\ndelegate clusterroles to IAM groups, and had to add single users to\\nclusterrole/rolebindings in Kubernetes. This was not a huge problem for us,\\nsince we were working with a very limited number of teams and devs, but it was\\napparent that this was not going to scale well. Luckily we got support for\\ngroups before it was a problem, and our config files went from containing way\\ntoo many names and email addresses, to only containing groups.\\n\\nOur Google Workspace receives groups and users from our Microsoft Active\\nDirectory. Groups are initially created either in Entra ID, or on our local\\nDomain Controllers, and at set intervals changes are pushed to Google Workspace.\\n[Role-based access control\\n(RBAC)](https://en.wikipedia.org/wiki/Role-based_access_control) based on\\nmembership in these groups was needed. We wanted to manage this through\\nTerraform, and created a repo with where we store and configure our entire IAM\\nconfiguration. Since we have had growing adoption of Kubernetes and public cloud\\nin our organization, more teams, projects and apps have been onboarded to SKIP,\\nand this IAM repo has grown. We\'ve tried to simplify the structure more than\\nonce, but since this is a problem not affecting dev teams, we have chosen to\\nprioritize other tasks.\\n\\n### Workloads\\nAll clusters created in in Anthos can be viewed from the GCP console, and the\\n[Connect\\ngateway](https://cloud.google.com/anthos/multicluster-management/gateway/using)\\nmakes it possible to do management from the console (or via kubectl) as well.\\nThe GCP console can be used to get information about, or manage the state of the\\ncluster, workloads and resources present. This is a web GUI, part of the GCP\\nconsole, and not as snappy as cli-tools, but still usable, and intuitive to use.\\n\\n![Anthos in Google Cloud](img/workload.png)\\nThis view shows workloads running in the argocd namespace. All workloads\\ndisplayed here can be clicked, and explored further.\\n\\nWhen accessing the cluster via the Connect gateway there are some limits. The\\nConnect gateway does not handle persistent connections, and this makes it\\nimpossible to do [exec, port-forward, proxy or\\nattach](https://cloud.google.com/anthos/multicluster-management/gateway/using#run_commands_against_the_cluster).\\nThis is not a problem for a production environment, where containers should\\nnever be used in this way. But for a dev, or sandbox environment, this is a bit\\nof a pain-point.\\n\\nThis issue should be partially fixed in Kubernetes 1.29 and should be completely\\nresolved in Kubernetes 1.30.\\n\\n### Service Mesh\\nA [Service Mesh](https://istio.io/latest/about/service-mesh/) in Kubernetes is\\nan infrastructure layer that manages communication between services. We are\\nusing Anthos Service Mesh (ASM), which is based on [Istio](https://istio.io) and\\nnicely integrated with the GCP console. It\'s easy to get an overview of\\nservices, the connection between them, and what services are connected to either\\nour internal or external gateways. This can be displayed in a Topology view, or\\nif you click on a service, you\'ll get a more detailed drilldown.\\n\\n![Anthos Service Mesh](img/services.png)\\n_A snippet of services running in our sandbox cluster._\\n\\nWhen we deploy services to our cluster we create almost all Kubernetes and\\nservice-mesh resources with our custom operator;\\n[Skiperator](https://github.com/kartverket/skiperator). This operator configures\\nthe resources to fit our setup, and applies \\"best practices\\" the easy way. This\\nhas been one of the great success stories in SKIP, and Skiperator is in\\ncontinuous development.\\n\\n\\n## Deployment\\n\\nDeployment is a very interesting subject when it comes to Anthos. As a platform\\nteam, it is our job to make sure that deployment is as quick and convenient as\\npossible for the product teams. This ambition has led us to iterate on our\\nprocesses, which has finally led us to a solution that both we and the\\ndevelopers enjoy using.\\n\\n### Iteration 1 - Terraform\\n\\nWhen we first started out with Anthos, we had a very manual process for\\ndeploying applications. A service account was provisioned in GCP, which allowed\\nthe developers to impersonate a service account in Kubernetes, which in turn\\nallowed them to deploy apps using Terraform. This approach worked, but had a\\ndecent amount of rough edges, and also would fail in ways that was hard to\\ndebug.\\n\\nWith this approach the developers would have to manage their own Terraform\\nfiles, which most of the time was not within their area of expertise. And while\\nSKIP was able to build modules and tools to make this easier, it was still a\\ncomplex system that was hard to understand. Observability and discoverability\\nwas also an issue.\\n\\nBecause of this we would consistently get feedback that this way of deploying\\nwas too complicated and slow, in addition handling Terraform state was a pain.\\nAs a platform team we\'re committed to our teams\' well being, so we took this\\nseriously and looked at alternatives. This was around the time we adopted Anthos,\\nso thus Anthos Config Managment was a natural choice.\\n\\n### Iteration 2 - Anthos Config Managment (ACM)\\n\\n![Anthos Config Management architecture showing multiple Git repos deployed to a cluster](img/acm-1.png)\\n\\nACM is a set of tools that allows you to declaratively manage your Kubernetes\\nresources. Here we\'re mostly going to talk about Config Sync, which is a\\n[GitOps](https://about.gitlab.com/topics/gitops/) system for Kubernetes.\\n\\nIn a GitOps system, a team will have a Git repository that contains all the\\nKubernetes resources that they want to deploy. This repository is then synced\\nto the Kubernetes cluster, and the resources are applied.\\n\\nThis can be likened to a pull-based system, where the GitOps tool (Config sync)\\nwatches the repo for changes and pulls them into the cluster. This is in\\ncontrast to a push-based system, where a script pushes the changes to a\\ncluster. It is therefore a dedicated system for deployment to Kubernetes, and\\nfollowing the [UNIX philosophy](https://en.wikipedia.org/wiki/Unix_philosophy)\\nwhich focuses on doing that one thing well.\\n\\nUsing this type of a workflow solves a lot of the issues around the Terraform\\nbased deployment that we had in the previous iteration. No longer do developers\\nneed to set up a complicated integration with GCP service accounts and\\nimpersonation, committing a file to a Git repo will trigger a deployment. The\\nGit repo and the manifests in them also works as a state of truth for the\\ncluster, instead of having to reverse engineer what was deployed based on\\nterraform diffs and state.\\n\\n![ACM UI showing a sync in progress](img/acm-2.gif)\\n\\nIt started well, however we soon ran into issues. The system would often take\\na long time to reconcile the sync, and during the sync we would not have any\\nvisibility into what was happening. This was not a deal breaker, but at the\\nsame time this was not a particularly good developer experience.\\n\\nWe also ran into issues with implementing a level of self-service that we were\\nsatisfied with. We wanted to give the developers the ability to provision their\\nown namespaces, but due to the multi-tenant nature of our clusters we also had\\nto make sure that teams were not able to write to each others\' namespaces.\\nThis was not a feature we were able to implement, but luckily our next iteration\\nhad this built in, and we\'ll get back to that.\\n\\nThe final nail was the user interface. We simply expected more from a deployment\\nsystem than what ACM was able to provide. The only view into the deployment was\\na long list of resources, which to a developer that is not an expert in\\nKubernetes, was not intuitive enough.\\n\\n### Final iteration - Argo CD\\n\\n![](img/argo-1.png)\\n\\nThis finally brought us to our current iteration. We had heard about Argo CD\\nbefore, but initially we were hesitant to add another system to our stack.\\nAfter ACM had introduced us to GitOps and we looked deeper into Argo CD, it was\\nobvious to us that Argo was more mature and would give our developers a better\\nuser experience.\\n\\nThe killer feature here is the UI. Argo CD has an intuitive and user-friendly\\nUI that gives the developers a good overview of what is deployed. Whenever\\nanything fails, it\'s immediately obvious which resource is failing, and Argo\\nallows you to drill down into the resource to see the details of the failure,\\nlogs for deployments, Kubernetes events, etc.\\n\\n![](img/argo-2.png)\\n\\nThe above photo illustrates this well. Here you can see a project with a number\\nof [Skiperator](https://github.com/kartverket/skiperator) applications. The\\ngreen checkmarks indicate that the application is synced and the green heart\\nindicates that the application is healthy. A developer can see the underlying\\n\\"owned\\" resources that Skiperator creates (such as a deployment, service, etc),\\nand get a look \\"behind the curtain\\" to see what is actually deployed. This helps\\ndebugging and gives the developers a better insight into what is happening\\nduring a deployment.\\n\\nIn terms of multi tenancy, Argo CD has a concept of projects. A project is a\\nset of namespaces that a team has access to, and a team can only use Argo to\\nsync to namespaces that are part of their project. The namespace allowlist can\\nalso include wildcards, which sounds small but this solved our self-service\\nissue! With our apps-repo architecture, we would give a team a \\"prefix\\" (for\\nexample `seeiendom-`), and that team would then be able to deploy to and create\\nany namespace that started with that prefix. If they tried to deploy to another\\nteam\'s namespace they would be stopped, as they would not have access to that\\nprefix.\\n\\nThe prefix feature allows product teams to create a new directory in their apps\\nrepo, which will then be synced to the cluster and deployed as a new namespace.\\nThis is a very simple and intuitive workflow for creating short-lived\\ndeployments, for example for pull requests, and it has been very well received\\nby the developers.\\n\\nThe apps-repo architecture will be a blog post itself at some point, so I won\'t\\ngo too much into it.\\n\\nAnd finally, if you\'re wondering what disaster recovery of an entire cluster\\nlooks like with Argo CD, I leave you with the following video at the end.\\n\\n<video controls width=\\"100%\\" muted={true}>\\n  <source src=\\"/img/argo-3.mov\\" type=\\"video/mp4\\" />\\n</video>\\n\\n## Hybrid Mesh\\nA hybrid mesh service mesh configuration is a setup that allows for service\\nnetworking across different environments. For Kartverket this includes a hybrid\\ncloud environment. The setup involves several steps, including setting up\\ncross-cluster credentials, installing the east-west gateway, enabling endpoint\\ndiscovery, and configuring certificate authorities. All clusters in a hybrid\\nmesh are registered to the same fleet host project, and istiod in each cluster\\nmust be able to communicate with the Kube-API on the opposing clusters.\\n\\nASM is as previously mentioned based on Istio, and after some internal\\ndiscussion we decided to experiment with running vanilla upstream Istio in our\\nGKE clusters running in GCP. Pairing it with ASM in our on-premise clusters\\nworked as expected (after a bit of config), and we are now running upstream\\nIstio in GKE, with ASM on-prem in a multi-cluster setup. We also looked into\\nusing managed ASM in our GKE cluster, this was hard for us however, due to it\\nrequiring firewall openings on-prem for sources we could not predict.\\n\\n![Multi-Primary on different networks](img/multi-cluster.png)\\n\\nWe have chosen the [Multi-Primary on different\\nnetworks](https://istio.io/latest/docs/setup/install/multicluster/multi-primary_multi-network/)\\nafter reviewing our network topology and configuration. We connect our\\non-premise network, with the GCP VPC through a VPN connection (using host and\\nservice projects). To have a production ready environment, the VPN connection\\nmust be configured with redundancy.\\n\\nWe\'re working towards getting this architecture into production, as this will\\nenable us to seamlessly use GKE clusters in GCP together with our on-premise\\nclusters. The elasticity of cloud infrastructure can be utilized where needed,\\nand we can handle communication between services on different clusters much more\\nsmoothly. This has been a bit of a journey to configure, but as a learning\\nexperience it has been valuable. Being able to address services seamlessly and\\ncommunicate with mTLS enabled by default across sites, zones and clusters\\nwithout developers having to think about it feels a bit like magic.\\n\\n## Monitoring\\n\\n### Google Cloud Monitoring\\n\\n![Google Cloud Monitoring dashboard](img/gcp-monitoring-1.png)\\n\\nGKE Enterprise includes an agent that collects metrics from the cluster and sends\\nthem to Google Cloud. This is a great feature which makes it relatively easy\\nto get started with metrics and monitoring. However, we have decided not to use\\nthe agent, and instead use Grafana and LGTM for metrics and monitoring.\\n\\nThis is mainly due to a couple of challenges:\\n\\nThe amount of metrics that are collected out of the box and sent to GCP\\ncontributes a significant part of our total spend. It\'s not that we have a lot\\nof clusters, but the amount of metrics that are collected out of the box is very\\nhigh, and Anthos\' default setup didn\'t give us the control we needed to be able\\nto manage it in a good way. \\n\\nNote that this was before [Managed Service for\\nPrometheus](https://cloud.google.com/managed-prometheus?hl=en) was released with\\nmore fine grained control over what metrics are collected. It is now the\\nrecommended default, which should make metrics collection easier to manage.\\n\\nSecond, while Google Cloud Monitoring has a few nice dashboards ready for\\nAnthos, it feels inconsistent which dashboards work on-premise and which only\\nwork in cloud as they are not labeled as such. This is not a big issue, but it\'s\\na bit annoying. The bigger issue is that all the dashboards feel sluggish and\\nslow to load. Several of us have used Grafana before, so we\'re used to a\\nsnappy and responsive UI. In our opinion, Google Cloud Monitoring feels clunky\\nin comparison.\\n\\nSo the cost and the user experience were the main reasons we decided to look at\\nalternatives to Google Cloud Monitoring. We ended up using Grafana and LGTM,\\nwhich we\'ll talk about next.\\n\\n### Grafana with the LGTM stack\\n\\n![Grafana dashboard with a Kubernetes cluster overview](img/grafana-1.png)\\n\\nWhen we realized that our needs were not entirely met by Google Cloud Monitoring,\\nwe started a project to develop a monitoring stack that would meet our needs.\\nSince Grafana is open source and has a large community, we decided to use that\\nas our frontend. Our backend is the LGTM stack, which is a set of open source\\ntools that are designed to work well together for ingesting, storing and querying\\nlogs, traces and metrics.\\n\\nWhat we noticed immediately was that the product teams were much more engaged\\nwith this stack than they were with [Google Cloud\\nMonitoring](https://cloud.google.com/monitoring/?hl=en). Previously they would\\nnot really look at the dashboards, but now they are using them and even creating\\ntheir own. This is a huge win for us, as we want the teams to be engaged with\\nthe monitoring and observability of their services.\\n\\nIt definitely helps that most developers on the product teams are familiar with\\nGrafana, which makes it easier for them to get started as the learning curve is\\nnot as steep.\\n\\nThere was a discussion about what the backend should be, if we should use\\n[Grafana Cloud](https://grafana.com/products/cloud/) or host it ourselves. There\\nwould be a lot of benefits of using the cloud, as we would not have to maintain\\nthe stack or worry about performance or storage. There was, however, a concern\\nabout cost and whether or not log files could be shipped to a cloud provider. In\\nthe end we decided to host it ourselves, mostly because we didn\'t have control\\nover what quantities of data we\'re processing. Now that we have a better\\nunderstanding of our usage we can use that to calculate our spend, so we\'re not\\nruling out migrating to Grafana Cloud in the future.\\n\\nThe collection (scraping) of data is done by [Grafana\\nAgent](https://grafana.com/oss/agent/), which is an \\"all-in-one\\" agent that\\ncollects metrics, logs and traces. This means a few less moving parts for the\\nstack, as we don\'t have to run both [Prometheus](https://prometheus.io/),\\n[Fluent Bit](https://fluentbit.io/) and some\\n[OpenTelemetry](https://opentelemetry.io/) compatible agent for traces. It\'s a\\nrelatively new project, but it\'s already relative stable and has a lot of\\nfeatures. It uses a funky format for configuration called river, which is based\\non Hashicorp\'s HCL. The config enables forming pipelines to process data before\\nit\'s forwarded to Loki, Tempo or Mimir.  It\'s a bit different, but it works well\\nand is easy to understand and configure to our needs.\\n\\n![Alerting with Grafana](img/grafana-2.png)\\n\\nUsing a system like Grafana also enables us to build an integrated experience\\nthat also includes alerting. Using Grafana alerting and OnCall, we configure\\nalerts that are sent to the correct team based on the service that is failing.\\nThis helps the teams get a better overview of what is happening in their\\nservices, and also helps us as a platform team to not have to be involved in\\nevery alert that is triggered.\\n\\nOverall we\'re very happy with the LGTM stack, even though it\'s a fair bit of\\nwork to maintain the stack (especially with Istio and other security measures).\\nWe\'re also happy with Grafana, and we\'re looking forward to seeing what the\\nfuture holds for monitoring and observability in Kubernetes.\\n\\n## Summary\\n\\nTo summarize: We like Anthos, and we think it\'s a great platform for running\\nhybrid Kubernetes. As a platform team we look at each feature on a case-by-case\\nbasis, with the goal of giving our developers the best possible experience\\ninstead of naively trying to use as much as possible of the platform. Because of\\nthis we\'ve decided to use Anthos for Kubernetes and service mesh, but not for\\nconfig sync and monitoring. This has given us a great platform that we\'re\\nconfident will serve us well for years to come.\\n\\nStay tuned for the third and final part of this series, where we\'ll talk about\\nthe benefits we\'ve seen from Anthos, and what we would have done differently if\\nwe were to start over.\\n\\n_Disclaimer - Google, GKE and Anthos are trademarks of Google LLC and this website is not\\nendorsed by or affiliated with Google in any way._"},{"id":"hybrid-kubernetes-in-production-part-1","metadata":{"permalink":"/blog/hybrid-kubernetes-in-production-part-1","source":"@site/blog/2023-11-7-anthos-1.md","title":"Hybrid Kubernetes in production pt. 1","description":"One of the biggest challenges that we hear is the challenge of running hybridized K8s workloads. Here we share our experience using Anthos for hybrid cloud\\n","date":"2023-11-07T00:00:00.000Z","tags":[{"inline":true,"label":"anthos","permalink":"/blog/tags/anthos"},{"inline":true,"label":"kubernetes","permalink":"/blog/tags/kubernetes"},{"inline":true,"label":"hybrid","permalink":"/blog/tags/hybrid"}],"readingTime":7.49,"hasTruncateMarker":true,"authors":[{"name":"Eline Henriksen","title":"Tidligere Produkteier og Plattformutvikler","url":"https://eliine.dev","imageURL":"https://github.com/eliihen.png","key":"elinehenriksen","page":null}],"frontMatter":{"title":"Hybrid Kubernetes in production pt. 1","description":"One of the biggest challenges that we hear is the challenge of running hybridized K8s workloads. Here we share our experience using Anthos for hybrid cloud\\n","slug":"hybrid-kubernetes-in-production-part-1","authors":["elinehenriksen"],"tags":["anthos","kubernetes","hybrid"],"image":"/img/skip.png","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Hybrid Kubernetes in production pt. 2","permalink":"/blog/hybrid-kubernetes-in-production-part-2"},"nextItem":{"title":"SKIP has a tech blog!","permalink":"/blog/welcome"}},"content":"![Anthos in Google Cloud](img/anthos-1.png)\\n\\nOver the years we talked with many other public sector companies about their\\nexperiences in running containers in production. One of the biggest challenges\\nthat we hear again and again is the challenge of running hybridized workloads,\\nor how to have some workloads running on-premise and some in the the cloud in a\\ngood way. \\n\\nIn this newsletter-series we will share some of our experiences solving this\\nissue by running Anthos on VMWare (or GKE on-prem, if you prefer) tied together\\nto the cloud in Kartverket using hybrid mesh. We will also discuss the reasons\\nwe went with Anthos and pros and cons we have experienced so far.\\n\\n\x3c!--truncate--\x3e\\n\\nAt [Kartverket](https://www.kartverket.no/en) we have an ambition to adopt cloud\\nnative technologies. There\'s thousands of ways to do this, and after trialing a\\ncouple of alternative solutions, including running plain Kubernetes and VMWare\\nTanzu, we decided to go with Anthos. Anthos is a platform that allows us to run\\nKubernetes clusters on-premise and in the cloud, and manage them from a single\\npane of glass. We have been running Anthos in production for a while now, at\\nleast long enough to be able to share our thoughts.\\n\\nThis newsletter is the first of three part series about Anthos in\\nKartverket.\\n\\n1. Why we chose Anthos (You are here!)\\n2. [How we run Anthos](/blog/hybrid-kubernetes-in-production-part-2)\\n3. [Benefits and what we would have done differently](/blog/hybrid-kubernetes-in-production-part-3)\\n\\n## So why a hybrid cloud?\\n\\n![Illustration: Anthos runs on GCP, on-premise, other clouds and Edge](img/anthos-3.png)\\n\\nWere you to take the time machine back a few years, you would see Kartverket as a\\ntraditional enterprise with a lot of knowledge and experience in running\\non-premise workloads. This knowledge served us well, but also slightly held us\\nback in terms of our imagination. We knew that there had to be a better way,\\nbut our enterprise was simply not mature enough to adopt a pure cloud strategy.\\nThe fear of the unknown cloud weighed heavily on many people, and therefore few\\npeople wanted to take the risk of moving to the cloud.\\n\\nThis is something we\'ve worked on for a long time, and still are. After a\\nlong time of working with the stakeholders in the organization, we eventually\\nbuilt a cloud strategy, which in simple terms stated that we would prefer \\nSaaS-products over hosting things ourselves, and that we would gradually move\\nour workloads to the cloud. \\n\\nThis cloud strategy however, which cleared up a lot of blockers, came too late\\nfor us on <abbr title=\\"Statens Kartverk Infrastructure Platform\\">SKIP</abbr>. At\\nthat point we had already done most of the work on our on-premise platform,\\nbuilding on the assumptions the organization held at the time, which was that we\\nmet our needs through existing infrastructure and that using public cloud had\\ndisqualifying cost and compliance implications. For SKIP it was therefore full\\nsteam ahead, building the on-prem part first, then adding the hybrid and cloud\\npart later.\\n\\nIt\'s not like we would have ended up with a pure cloud setup in any case,\\nthough. If you\'re at all familiar with large enterprises, you will know that\\nthey are often very complex. This is also true for Kartverket, where we have a\\nlot of existing systems that are not easy to move to the cloud. We also have a\\nlot of systems that are not suitable for the cloud, mostly because they are\\ndesigned to run in a way that would not be cost effective in the cloud. In\\naddition we have absolutely massive datasets (petabyte-scale) that would be very\\nexpensive to move to the cloud.\\n\\nBecause of these limitations, a pure cloud strategy is not considered to be a\\ngood fit for us.\\n\\nA hybrid cloud, however, can give us the scalability and flexibility of the\\ncloud, while still allowing us to run some of our systems on-prem, with the\\nexperience being more or less seamless for the developers.\\n\\n## Why we chose Anthos\\n\\nAfter some disastrous issues with our previous hybrid cloud PoC (that\'s a whole\\nstory in itself) we decided to to look at what alternatives existed on the\\nmarket. We considered various options, but eventually decided to run a PoC on\\nAnthos. This was based on a series of conditions at the time, to name a few:\\n\\n- We had a decent pool of knowledge in GCP compared to AWS and Azure at the time\\n- Some very well established platform teams in the public sector were also using\\n  GCP, which meant it would be easier to share work and learnings\\n- Anthos and GCP seemed to offer a good developer experience, which for us as a\\n  platform team is of paramount importance\\n- A provider like Google is well established in the cloud space (especially\\n  Kubernetes), and would have a fully featured, stable and user friendly product\\n\\nSKIP ran the Anthos PoC over a few months, initially as an on-prem offering only.\\nDrawing on the knowledge of internal network and infrastructure engineers, this\\ntook us all the way from provisioning clusters and networking, to iterating on\\ntools and docs and finally onboarding an internal product team on the platform.\\nOnce we felt we had learned what we could from the PoC, we gathered thoughts\\nfrom the product team, infrastructure team and of course the SKIP platform team.\\n\\nThe results were unanimous. All the participants lauded the GCP user interfaces that\\nallowed visibility into running workloads, as well as the new self-service\\nfeatures that came with it. Infrastructure engineers complimented the\\ninstallation scripts and documentation, which would make it easier to keep the\\nclusters up to date.\\n\\nBased on the total package we therefore decided to move ahead with Anthos. To\\ninfinity and beyond! \ud83d\ude80\\n\\n## What is Anthos anyway?\\n\\n![Anthos logo](img/anthos-2.png)\\n\\nAnthos is Google\'s solution to multicloud. It\'s a product portfolio where the\\nmain product is GKE (Google Kubernetes Engine) on-premise. Using GKE on-prem\\nyou can run Kubernetes clusters on-premise and manage them from the same\\ncontrol plane in Google Cloud, as if they were proper cloud clusters.\\n\\nIn fact, Anthos is truly multi-cloud. That means you can deploy Anthos\\nclusters to GKE and on-prem, but also AWS and Azure. On other cloud platforms\\nit uses the provider\'s Kubernetes distribution like\\n[AKS](https://learn.microsoft.com/en-us/azure/aks/), but you can still manage it\\nfrom GKE alongside your other clusters.\\n\\nIn addition to GKE, the toolbox includes:\\n\\n### Anthos Service Mesh (ASM)\\n\\nA networking solution based on [Istio](http://istio.io). This is sort of the\\nbackbone of the hybrid features of Anthos, as provided you\'ve configured a\\nhybrid mesh it allows applications deployed to the cloud to communicate with\\non-premise workloads automatically and without manual steps like opening\\nfirewalls. \\n\\nAll traffic that flows between microservices on the mesh is also automatically \\nencrypted with mTLS.\\n\\n### Anthos Config Managment (ACM)\\n\\nA way to sync git repos into a running cluster. Think GitOps here. Build a repo\\ncontaining all your Kubernetes manifests and sync them into your cluster, making\\ncluster maintenance easier.\\n\\nACM also includes a policy controller based on [Open Policy Agent Gatekeeper\\n(OPA)](https://open-policy-agent.github.io/gatekeeper/website/) which allows\\nplatform developers to build guardrails into developers\' workflows using\\npolicies like _\\"don\'t allow containers to run as root\\"_.\\n\\n### Anthos Connect Gateway\\n\\nThe connect gateway allows developers to log on to the cluster using `gcloud`\\nand `kubectl` commands, despite the cluster potentially being behind a\\nfirewall. From a user experience standpoint this is quite useful, as devs\\nwill be logged in to GCP using two factor authentication, and the same strong\\nauthentication allows you to access kubernetes on-premise.\\n\\nConnect Gateway also integrates with GCP groups, enabling RBAC in Kubernetes\\nto be assigned to groups instead of manually administered lists of users.\\n\\nCurrently the connect gateway only supports stateless requests, for example\\n`kubectl get pods` or `kubectl logs` (including `-f`). It does not support\\n`port-forward`, `exec` or `run`, which can be a bit annoying.\\n\\n## Summary\\n\\nAs you can see, the above tools gives us a lot of benefits.\\n\\n- Combined with the power of Google Cloud and\\nTerraform, they give us a good combination of flexibility through cloud services\\n- Ease the maintenance by using the tools that Anthos and Terraform supply us\\n- Eases the compliance and modernization burden by allowing a gradual or\\npartial migration to cloud, allowing parts to remain on-premise while still\\nretaining most of the modern tooling of the cloud\\n\\nThat\'s it for now! \ud83d\ude42 We\'ll be back with more details on how we run Anthos as\\nwell as the pros and cons we\'ve seen so far in the coming weeks. Stay tuned!\\n\\n_Disclaimer - Google, GKE and Anthos are trademarks of Google LLC and this website is not\\nendorsed by or affiliated with Google in any way._"},{"id":"welcome","metadata":{"permalink":"/blog/welcome","source":"@site/blog/2023-11-6-welcome.md","title":"SKIP has a tech blog!","description":"SKIP is starting a tech blog! \ud83d\ude80\\n","date":"2023-11-06T00:00:00.000Z","tags":[],"readingTime":0.87,"hasTruncateMarker":true,"authors":[{"name":"Eline Henriksen","title":"Tidligere Produkteier og Plattformutvikler","url":"https://eliine.dev","imageURL":"https://github.com/eliihen.png","key":"elinehenriksen","page":null}],"frontMatter":{"title":"SKIP has a tech blog!","description":"SKIP is starting a tech blog! \ud83d\ude80\\n","slug":"welcome","authors":["elinehenriksen"],"tags":[],"image":"/img/skip.png","hide_table_of_contents":false},"unlisted":false,"prevItem":{"title":"Hybrid Kubernetes in production pt. 1","permalink":"/blog/hybrid-kubernetes-in-production-part-1"}},"content":"![Anthos in Google Cloud](../static/img/skip.png)\\n\\nSKIP is starting a tech blog! \ud83d\ude80\\n\\nOr call it a newsletter if you\'re tired of blogs \ud83e\udd2a\\n\\nOur first entry is already out, and it\'s about [why we chose\\nAnthos](/blog/hybrid-kubernetes-in-production-part-1) for hybrid cloud. We\'re\\nworking on more entries into that series and other exciting topics, so stay\\ntuned!\\n\\n\x3c!--truncate--\x3e\\n\\n# Huh? SKIP?\\n\\nSKIP is Statens Kartverks Infrastruktur Plattform, or in English, the\\nInfrastructure Platform of the Norwegian Mapping Authority.\\n\\nWe\'re the platform team at [Kartverket](https://kartverket.no). We tame\\nKubernetes and the Cloud. With SKIP, developers in Kartverket are empowered to\\nrun, not walk, using a comprehensive toolbox of modern cloud technology. Using\\nSKIP, developers can deploy applications to Kubernetes in a matter of minutes,\\nwhile still being able to use the tools they know and love.\\n\\n## Like what you see?\\n\\nWe\'re a small team, but we\'re growing fast. We\'re also hiring, so if you\'re\\ninterested in working with us, check out our [open\\npositions](https://www.kartverket.no/en/about-kartverket/careers)."}]}}')}}]);