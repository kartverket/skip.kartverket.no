"use strict";(self.webpackChunkskip_docs=self.webpackChunkskip_docs||[]).push([[938],{8921:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>c,frontMatter:()=>a,metadata:()=>r,toc:()=>h});var o=n(5893),s=n(1151);const a={title:"Hybrid Kubernetes in production pt. 2",description:"In this second installment of the Anthos series, we'll talk about how we run Anthos and hybrid cloud in Kartverket. \n",slug:"hybrid-kubernetes-in-production-part-2",authors:[{name:"Espen Henriksen",title:"Product Owner and Platform Developer",url:"https://espen.dev",image_url:"https://github.com/esphen.png"},{name:"B\xe5rd Ove Hoel",title:"Tech Lead and Platform Developer",url:"https://github.com/bardove",image_url:"https://github.com/bardove.png"}],tags:["anthos","kubernetes","hybrid"],image:"/img/anthos-2.png",hide_table_of_contents:!1},i=void 0,r={permalink:"/blog/hybrid-kubernetes-in-production-part-2",source:"@site/blog/2023-12-14-anthos-2.md",title:"Hybrid Kubernetes in production pt. 2",description:"In this second installment of the Anthos series, we'll talk about how we run Anthos and hybrid cloud in Kartverket. \n",date:"2023-12-14T00:00:00.000Z",formattedDate:"December 14, 2023",tags:[{label:"anthos",permalink:"/blog/tags/anthos"},{label:"kubernetes",permalink:"/blog/tags/kubernetes"},{label:"hybrid",permalink:"/blog/tags/hybrid"}],readingTime:17.525,hasTruncateMarker:!0,authors:[{name:"Espen Henriksen",title:"Product Owner and Platform Developer",url:"https://espen.dev",image_url:"https://github.com/esphen.png",imageURL:"https://github.com/esphen.png"},{name:"B\xe5rd Ove Hoel",title:"Tech Lead and Platform Developer",url:"https://github.com/bardove",image_url:"https://github.com/bardove.png",imageURL:"https://github.com/bardove.png"}],frontMatter:{title:"Hybrid Kubernetes in production pt. 2",description:"In this second installment of the Anthos series, we'll talk about how we run Anthos and hybrid cloud in Kartverket. \n",slug:"hybrid-kubernetes-in-production-part-2",authors:[{name:"Espen Henriksen",title:"Product Owner and Platform Developer",url:"https://espen.dev",image_url:"https://github.com/esphen.png",imageURL:"https://github.com/esphen.png"},{name:"B\xe5rd Ove Hoel",title:"Tech Lead and Platform Developer",url:"https://github.com/bardove",image_url:"https://github.com/bardove.png",imageURL:"https://github.com/bardove.png"}],tags:["anthos","kubernetes","hybrid"],image:"/img/anthos-2.png",hide_table_of_contents:!1},unlisted:!1,prevItem:{title:"SKIP on Plattformpodden!",permalink:"/blog/skip-on-plattformpodden"},nextItem:{title:"Hybrid Kubernetes in production pt. 1",permalink:"/blog/hybrid-kubernetes-in-production-part-1"}},l={authorsImageUrls:[void 0,void 0]},h=[{value:"Installation and upgrades",id:"installation-and-upgrades",level:2},{value:"GCP integration",id:"gcp-integration",level:2},{value:"IAM and Groups",id:"iam-and-groups",level:3},{value:"Workloads",id:"workloads",level:3},{value:"Service Mesh",id:"service-mesh",level:3},{value:"Deployment",id:"deployment",level:2},{value:"Iteration 1 - Terraform",id:"iteration-1---terraform",level:3},{value:"Iteration 2 - Anthos Config Managment (ACM)",id:"iteration-2---anthos-config-managment-acm",level:3},{value:"Final iteration - Argo CD",id:"final-iteration---argo-cd",level:3},{value:"Hybrid Mesh",id:"hybrid-mesh",level:2},{value:"Monitoring",id:"monitoring",level:2},{value:"Google Cloud Monitoring",id:"google-cloud-monitoring",level:3},{value:"Grafana with the LGTM stack",id:"grafana-with-the-lgtm-stack",level:3},{value:"Summary",id:"summary",level:2}];function d(e){const t={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",...(0,s.a)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"Anthos in Google Cloud",src:n(3582).Z+"",width:"2880",height:"1200"})}),"\n",(0,o.jsxs)(t.p,{children:["In this second installment of the Anthos series, we will talk about how we run\nAnthos and hybrid cloud at ",(0,o.jsx)(t.a,{href:"https://kartverket.no/en",children:"Kartverket"}),". We'll touch\non the hardware, the software, and the processes we use to keep it running."]}),"\n",(0,o.jsx)(t.p,{children:"By the end we hope that we'll have de-mystified Anthos a bit, and maybe given\nyou an idea of what it takes to run Anthos in production."}),"\n",(0,o.jsxs)(t.p,{children:["If you haven't read the first part, you can find it\n",(0,o.jsx)(t.a,{href:"/blog/hybrid-kubernetes-in-production-part-1",children:"here"}),"."]}),"\n",(0,o.jsx)(t.p,{children:"This newsletter is the second of the three part series about Anthos in\nKartverket."}),"\n",(0,o.jsxs)(t.ol,{children:["\n",(0,o.jsx)(t.li,{children:(0,o.jsx)(t.a,{href:"/blog/hybrid-kubernetes-in-production-part-1",children:"Why we chose Anthos"})}),"\n",(0,o.jsx)(t.li,{children:"How we run Anthos (You are here!)"}),"\n",(0,o.jsx)(t.li,{children:"Benefits and what we would have done differently (Coming soon)"}),"\n"]}),"\n",(0,o.jsx)(t.h2,{id:"installation-and-upgrades",children:"Installation and upgrades"}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"Illustration of the cluster architecture",src:n(6818).Z+"",width:"1456",height:"1082"})}),"\n",(0,o.jsxs)(t.p,{children:["We have been early adopters of Anthos, so when doing the install we did not have\noptions for controlplane architecture. We wanted to use existing underlying\nVMware infrastructure, so the nodes in our clusters are VMs, provisioned by\nscripts provided by Google. Our cluster is installed with\n",(0,o.jsx)(t.a,{href:"https://kubernetes.io/blog/2017/01/how-we-run-kubernetes-in-kubernetes-kubeception/",children:"kubeception"}),"\ncontrolplane architechture, this no longer the only, or recommended way. The\nrecommended model is ",(0,o.jsx)(t.a,{href:"https://cloud.google.com/anthos/clusters/docs/on-prem/latest/how-to/create-user-cluster-controlplane-v2",children:"Controlplane\nV2"}),",\nwhere the controlplane nodes for the user cluster are in the user cluster\nitself."]}),"\n",(0,o.jsx)(t.p,{children:"In the kubeception model, Kubernetes clusters are nested inside other Kubernetes\nclusters. Specifically, the control plane of the user clusters runs in an\nadmin-cluster. For each on-premise cluster created, a new set of nodes and a\nnamespace are created in the admin cluster."}),"\n",(0,o.jsxs)(t.p,{children:["To install and make changes to the admin cluster, an admin workstation is\nrequired, which must be located in the same network as the admin cluster. All\nconfigurations are done using a CLI tool called ",(0,o.jsx)(t.code,{children:"gkectl"}),". This tool handles most\ncluster administration tasks, and the cluster specific configuration is provided\nin YAML files."]}),"\n",(0,o.jsx)(t.p,{children:"Our cluster setup is more or less static, and most cluster administration tasks\ninvolve upgrading or scaling existing clusters. The SKIP team has a cluster\nreferred to as \u201csandbox\u201d, which is always the first recipient of potentially\nbreaking changes. After testing in sandbox, we'll deploy changes to both\ndevelopment and test environments, and if nothing breaks, we roll out the\nchanges to our production environment. This is mostly done outside work-hours,\nalthough we have not experienced downtime during cluster upgrades. Here is the\ngeneral workflow for upgrading:"}),"\n",(0,o.jsxs)(t.ol,{children:["\n",(0,o.jsx)(t.li,{children:"Upgrade your admin workstation to the target version of your upgrade."}),"\n",(0,o.jsx)(t.li,{children:"From your admin workstation, upgrade your user clusters."}),"\n",(0,o.jsx)(t.li,{children:"After all of the user clusters have been upgraded, you can upgrade your admin\ncluster from the admin workstation."}),"\n"]}),"\n",(0,o.jsxs)(t.p,{children:["We have tried using ",(0,o.jsx)(t.a,{href:"https://www.terraform.io/",children:"Terraform"})," where possible to\nsimplify the setup. This can not be done in the same way for clusters using the\nkubeception model. When we migrate to Controlplane V2 however, clusters can be\nmanaged via GCP, and we can finally start using terraform for our on-premise\ncluster config in the same way as for our GKE clusters, and GCP configuration in\ngeneral."]}),"\n",(0,o.jsx)(t.h2,{id:"gcp-integration",children:"GCP integration"}),"\n",(0,o.jsx)(t.p,{children:"When working with an on-premise Anthos cluster, some of the nice-to-have\nfeatures of a standard GKE cluster have been lost. However, recently Anthos on\nVMware clusters have gradually received more and more features compared to GKE\nclusters."}),"\n",(0,o.jsx)(t.h3,{id:"iam-and-groups",children:"IAM and Groups"}),"\n",(0,o.jsx)(t.p,{children:"Since we were early adaptors of Anthos, we had to endure not being able to\ndelegate clusterroles to IAM groups, and had to add single users to\nclusterrole/rolebindings in Kubernetes. This was not a huge problem for us,\nsince we were working with a very limited number of teams and devs, but it was\napparent that this was not going to scale well. Luckily we got support for\ngroups before it was a problem, and our config files went from containing way\ntoo many names and email addresses, to only containing groups."}),"\n",(0,o.jsxs)(t.p,{children:["Our Google Workspace receives groups and users from our Microsoft Active\nDirectory. Groups are initially created either in Entra ID, or on our local\nDomain Controllers, and at set intervals changes are pushed to Google Workspace.\n",(0,o.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Role-based_access_control",children:"Role-based access control\n(RBAC)"})," based on\nmembership in these groups was needed. We wanted to manage this through\nTerraform, and created a repo with where we store and configure our entire IAM\nconfiguration. Since we have had growing adoption of Kubernetes and public cloud\nin our organization, more teams, projects and apps have been onboarded to SKIP,\nand this IAM repo has grown. We've tried to simplify the structure more than\nonce, but since this is a problem not affecting dev teams, we have chosen to\nprioritize other tasks."]}),"\n",(0,o.jsx)(t.h3,{id:"workloads",children:"Workloads"}),"\n",(0,o.jsxs)(t.p,{children:["All clusters created in in Anthos can be viewed from the GCP console, and the\n",(0,o.jsx)(t.a,{href:"https://cloud.google.com/anthos/multicluster-management/gateway/using",children:"Connect\ngateway"}),"\nmakes it possible to do management from the console (or via kubectl) as well.\nThe GCP console can be used to get information about, or manage the state of the\ncluster, workloads and resources present. This is a web GUI, part of the GCP\nconsole, and not as snappy as cli-tools, but still usable, and intuitive to use."]}),"\n",(0,o.jsxs)(t.p,{children:[(0,o.jsx)(t.img,{alt:"Anthos in Google Cloud",src:n(4505).Z+"",width:"1223",height:"618"}),"\nThis view shows workloads running in the argocd namespace. All workloads\ndisplayed here can be clicked, and explored further."]}),"\n",(0,o.jsxs)(t.p,{children:["When accessing the cluster via the Connect gateway there are some limits. The\nConnect gateway does not handle persistent connections, and this makes it\nimpossible to do ",(0,o.jsx)(t.a,{href:"https://cloud.google.com/anthos/multicluster-management/gateway/using#run_commands_against_the_cluster",children:"exec, port-forward, proxy or\nattach"}),".\nThis is not a problem for a production environment, where containers should\nnever be used in this way. But for a dev, or sandbox environment, this is a bit\nof a pain-point."]}),"\n",(0,o.jsx)(t.p,{children:"This issue should be partially fixed in Kubernetes 1.29 and should be completely\nresolved in Kubernetes 1.30."}),"\n",(0,o.jsx)(t.h3,{id:"service-mesh",children:"Service Mesh"}),"\n",(0,o.jsxs)(t.p,{children:["A ",(0,o.jsx)(t.a,{href:"https://istio.io/latest/about/service-mesh/",children:"Service Mesh"})," in Kubernetes is\nan infrastructure layer that manages communication between services. We are\nusing Anthos Service Mesh (ASM), which is based on ",(0,o.jsx)(t.a,{href:"https://istio.io",children:"Istio"})," and\nnicely integrated with the GCP console. It's easy to get an overview of\nservices, the connection between them, and what services are connected to either\nour internal or external gateways. This can be displayed in a Topology view, or\nif you click on a service, you'll get a more detailed drilldown."]}),"\n",(0,o.jsxs)(t.p,{children:[(0,o.jsx)(t.img,{alt:"Anthos Service Mesh",src:n(9412).Z+"",width:"1539",height:"935"}),"\n",(0,o.jsx)(t.em,{children:"A snippet of services running in our sandbox cluster."})]}),"\n",(0,o.jsxs)(t.p,{children:["When we deploy services to our cluster we create almost all Kubernetes and\nservice-mesh resources with our custom operator;\n",(0,o.jsx)(t.a,{href:"https://github.com/kartverket/skiperator",children:"Skiperator"}),'. This operator configures\nthe resources to fit our setup, and applies "best practices" the easy way. This\nhas been one of the great success stories in SKIP, and Skiperator is in\ncontinuous development.']}),"\n",(0,o.jsx)(t.h2,{id:"deployment",children:"Deployment"}),"\n",(0,o.jsx)(t.p,{children:"Deployment is a very interesting subject when it comes to Anthos. As a platform\nteam, it is our job to make sure that deployment is as quick and convenient as\npossible for the product teams. This ambition has led us to iterate on our\nprocesses, which has finally led us to a solution that both we and the\ndevelopers enjoy using."}),"\n",(0,o.jsx)(t.h3,{id:"iteration-1---terraform",children:"Iteration 1 - Terraform"}),"\n",(0,o.jsx)(t.p,{children:"When we first started out with Anthos, we had a very manual process for\ndeploying applications. A service account was provisioned in GCP, which allowed\nthe developers to impersonate a service account in Kubernetes, which in turn\nallowed them to deploy apps using Terraform. This approach worked, but had a\ndecent amount of rough edges, and also would fail in ways that was hard to\ndebug."}),"\n",(0,o.jsx)(t.p,{children:"With this approach the developers would have to manage their own Terraform\nfiles, which most of the time was not within their area of expertise. And while\nSKIP was able to build modules and tools to make this easier, it was still a\ncomplex system that was hard to understand. Observability and discoverability\nwas also an issue."}),"\n",(0,o.jsx)(t.p,{children:"Because of this we would consistently get feedback that this way of deploying\nwas too complicated and slow, in addition handling Terraform state was a pain.\nAs a platform team we're committed to our teams' well being, so we took this\nseriously and looked at alternatives. This was around the time we adopted Anthos,\nso thus Anthos Config Managment was a natural choice."}),"\n",(0,o.jsx)(t.h3,{id:"iteration-2---anthos-config-managment-acm",children:"Iteration 2 - Anthos Config Managment (ACM)"}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"Anthos Config Management architecture showing multiple Git repos deployed to a cluster",src:n(8094).Z+"",width:"732",height:"726"})}),"\n",(0,o.jsxs)(t.p,{children:["ACM is a set of tools that allows you to declaratively manage your Kubernetes\nresources. Here we're mostly going to talk about Config Sync, which is a\n",(0,o.jsx)(t.a,{href:"https://about.gitlab.com/topics/gitops/",children:"GitOps"})," system for Kubernetes."]}),"\n",(0,o.jsx)(t.p,{children:"In a GitOps system, a team will have a Git repository that contains all the\nKubernetes resources that they want to deploy. This repository is then synced\nto the Kubernetes cluster, and the resources are applied."}),"\n",(0,o.jsxs)(t.p,{children:["This can be likened to a pull-based system, where the GitOps tool (Config sync)\nwatches the repo for changes and pulls them into the cluster. This is in\ncontrast to a push-based system, where a script pushes the changes to a\ncluster. It is therefore a dedicated system for deployment to Kubernetes, and\nfollowing the ",(0,o.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Unix_philosophy",children:"UNIX philosophy"}),"\nwhich focuses on doing that one thing well."]}),"\n",(0,o.jsx)(t.p,{children:"Using this type of a workflow solves a lot of the issues around the Terraform\nbased deployment that we had in the previous iteration. No longer do developers\nneed to set up a complicated integration with GCP service accounts and\nimpersonation, committing a file to a Git repo will trigger a deployment. The\nGit repo and the manifests in them also works as a state of truth for the\ncluster, instead of having to reverse engineer what was deployed based on\nterraform diffs and state."}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"ACM UI showing a sync in progress",src:n(7707).Z+"",width:"928",height:"568"})}),"\n",(0,o.jsx)(t.p,{children:"It started well, however we soon ran into issues. The system would often take\na long time to reconcile the sync, and during the sync we would not have any\nvisibility into what was happening. This was not a deal breaker, but at the\nsame time this was not a particularly good developer experience."}),"\n",(0,o.jsx)(t.p,{children:"We also ran into issues with implementing a level of self-service that we were\nsatisfied with. We wanted to give the developers the ability to provision their\nown namespaces, but due to the multi-tenant nature of our clusters we also had\nto make sure that teams were not able to write to each others' namespaces.\nThis was not a feature we were able to implement, but luckily our next iteration\nhad this built in, and we'll get back to that."}),"\n",(0,o.jsx)(t.p,{children:"The final nail was the user interface. We simply expected more from a deployment\nsystem than what ACM was able to provide. The only view into the deployment was\na long list of resources, which to a developer that is not an expert in\nKubernetes, was not intuitive enough."}),"\n",(0,o.jsx)(t.h3,{id:"final-iteration---argo-cd",children:"Final iteration - Argo CD"}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{src:n(3125).Z+"",width:"2498",height:"1229"})}),"\n",(0,o.jsx)(t.p,{children:"This finally brought us to our current iteration. We had heard about Argo CD\nbefore, but initially we were hesitant to add another system to our stack.\nAfter ACM had introduced us to GitOps and we looked deeper into Argo CD, it was\nobvious to us that Argo was more mature and would give our developers a better\nuser experience."}),"\n",(0,o.jsx)(t.p,{children:"The killer feature here is the UI. Argo CD has an intuitive and user-friendly\nUI that gives the developers a good overview of what is deployed. Whenever\nanything fails, it's immediately obvious which resource is failing, and Argo\nallows you to drill down into the resource to see the details of the failure,\nlogs for deployments, Kubernetes events, etc."}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{src:n(5249).Z+"",width:"2499",height:"1209"})}),"\n",(0,o.jsxs)(t.p,{children:["The above photo illustrates this well. Here you can see a project with a number\nof ",(0,o.jsx)(t.a,{href:"https://github.com/kartverket/skiperator",children:"Skiperator"}),' applications. The\ngreen checkmarks indicate that the application is synced and the green heart\nindicates that the application is healthy. A developer can see the underlying\n"owned" resources that Skiperator creates (such as a deployment, service, etc),\nand get a look "behind the curtain" to see what is actually deployed. This helps\ndebugging and gives the developers a better insight into what is happening\nduring a deployment.']}),"\n",(0,o.jsxs)(t.p,{children:['In terms of multi tenancy, Argo CD has a concept of projects. A project is a\nset of namespaces that a team has access to, and a team can only use Argo to\nsync to namespaces that are part of their project. The namespace allowlist can\nalso include wildcards, which sounds small but this solved our self-service\nissue! With our apps-repo architecture, we would give a team a "prefix" (for\nexample ',(0,o.jsx)(t.code,{children:"seeiendom-"}),"), and that team would then be able to deploy to and create\nany namespace that started with that prefix. If they tried to deploy to another\nteam's namespace they would be stopped, as they would not have access to that\nprefix."]}),"\n",(0,o.jsx)(t.p,{children:"The prefix feature allows product teams to create a new directory in their apps\nrepo, which will then be synced to the cluster and deployed as a new namespace.\nThis is a very simple and intuitive workflow for creating short-lived\ndeployments, for example for pull requests, and it has been very well received\nby the developers."}),"\n",(0,o.jsx)(t.p,{children:"The apps-repo architecture will be a blog post itself at some point, so I won't\ngo too much into it."}),"\n",(0,o.jsx)(t.p,{children:"And finally, if you're wondering what disaster recovery of an entire cluster\nlooks like with Argo CD, I leave you with the following video at the end."}),"\n",(0,o.jsx)("video",{controls:!0,width:"100%",muted:!0,children:(0,o.jsx)("source",{src:"/img/argo-3.mov",type:"video/mp4"})}),"\n",(0,o.jsx)(t.h2,{id:"hybrid-mesh",children:"Hybrid Mesh"}),"\n",(0,o.jsx)(t.p,{children:"A hybrid mesh service mesh configuration is a setup that allows for service\nnetworking across different environments. For Kartverket this includes a hybrid\ncloud environment. The setup involves several steps, including setting up\ncross-cluster credentials, installing the east-west gateway, enabling endpoint\ndiscovery, and configuring certificate authorities. All clusters in a hybrid\nmesh are registered to the same fleet host project, and istiod in each cluster\nmust be able to communicate with the Kube-API on the opposing clusters."}),"\n",(0,o.jsx)(t.p,{children:"ASM is as previously mentioned based on Istio, and after some internal\ndiscussion we decided to experiment with running vanilla upstream Istio in our\nGKE clusters running in GCP. Pairing it with ASM in our on-premise clusters\nworked as expected (after a bit of config), and we are now running upstream\nIstio in GKE, with ASM on-prem in a multi-cluster setup. We also looked into\nusing managed ASM in our GKE cluster, this was hard for us however, due to it\nrequiring firewall openings on-prem for sources we could not predict."}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"Multi-Primary on different networks",src:n(4312).Z+"",width:"748",height:"555"})}),"\n",(0,o.jsxs)(t.p,{children:["We have chosen the ",(0,o.jsx)(t.a,{href:"https://istio.io/latest/docs/setup/install/multicluster/multi-primary_multi-network/",children:"Multi-Primary on different\nnetworks"}),"\nafter reviewing our network topology and configuration. We connect our\non-premise network, with the GCP VPC through a VPN connection (using host and\nservice projects). To have a production ready environment, the VPN connection\nmust be configured with redundancy."]}),"\n",(0,o.jsx)(t.p,{children:"We're working towards getting this architecture into production, as this will\nenable us to seamlessly use GKE clusters in GCP together with our on-premise\nclusters. The elasticity of cloud infrastructure can be utilized where needed,\nand we can handle communication between services on different clusters much more\nsmoothly. This has been a bit of a journey to configure, but as a learning\nexperience it has been valuable. Being able to address services seamlessly and\ncommunicate with mTLS enabled by default across sites, zones and clusters\nwithout developers having to think about it feels a bit like magic."}),"\n",(0,o.jsx)(t.h2,{id:"monitoring",children:"Monitoring"}),"\n",(0,o.jsx)(t.h3,{id:"google-cloud-monitoring",children:"Google Cloud Monitoring"}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"Google Cloud Monitoring dashboard",src:n(8252).Z+"",width:"2256",height:"1108"})}),"\n",(0,o.jsx)(t.p,{children:"GKE Enterprise includes an agent that collects metrics from the cluster and sends\nthem to Google Cloud. This is a great feature which makes it relatively easy\nto get started with metrics and monitoring. However, we have decided not to use\nthe agent, and instead use Grafana and LGTM for metrics and monitoring."}),"\n",(0,o.jsx)(t.p,{children:"This is mainly due to a couple of challenges:"}),"\n",(0,o.jsx)(t.p,{children:"The amount of metrics that are collected out of the box and sent to GCP\ncontributes a significant part of our total spend. It's not that we have a lot\nof clusters, but the amount of metrics that are collected out of the box is very\nhigh, and Anthos' default setup didn't give us the control we needed to be able\nto manage it in a good way."}),"\n",(0,o.jsxs)(t.p,{children:["Note that this was before ",(0,o.jsx)(t.a,{href:"https://cloud.google.com/managed-prometheus?hl=en",children:"Managed Service for\nPrometheus"})," was released with\nmore fine grained control over what metrics are collected. It is now the\nrecommended default, which should make metrics collection easier to manage."]}),"\n",(0,o.jsx)(t.p,{children:"Second, while Google Cloud Monitoring has a few nice dashboards ready for\nAnthos, it feels inconsistent which dashboards work on-premise and which only\nwork in cloud as they are not labeled as such. This is not a big issue, but it's\na bit annoying. The bigger issue is that all the dashboards feel sluggish and\nslow to load. Several of us have used Grafana before, so we're used to a\nsnappy and responsive UI. In our opinion, Google Cloud Monitoring feels clunky\nin comparison."}),"\n",(0,o.jsx)(t.p,{children:"So the cost and the user experience were the main reasons we decided to look at\nalternatives to Google Cloud Monitoring. We ended up using Grafana and LGTM,\nwhich we'll talk about next."}),"\n",(0,o.jsx)(t.h3,{id:"grafana-with-the-lgtm-stack",children:"Grafana with the LGTM stack"}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"Grafana dashboard with a Kubernetes cluster overview",src:n(2138).Z+"",width:"5088",height:"3266"})}),"\n",(0,o.jsx)(t.p,{children:"When we realized that our needs were not entirely met by Google Cloud Monitoring,\nwe started a project to develop a monitoring stack that would meet our needs.\nSince Grafana is open source and has a large community, we decided to use that\nas our frontend. Our backend is the LGTM stack, which is a set of open source\ntools that are designed to work well together for ingesting, storing and querying\nlogs, traces and metrics."}),"\n",(0,o.jsxs)(t.p,{children:["What we noticed immediately was that the product teams were much more engaged\nwith this stack than they were with ",(0,o.jsx)(t.a,{href:"https://cloud.google.com/monitoring/?hl=en",children:"Google Cloud\nMonitoring"}),". Previously they would\nnot really look at the dashboards, but now they are using them and even creating\ntheir own. This is a huge win for us, as we want the teams to be engaged with\nthe monitoring and observability of their services."]}),"\n",(0,o.jsx)(t.p,{children:"It definitely helps that most developers on the product teams are familiar with\nGrafana, which makes it easier for them to get started as the learning curve is\nnot as steep."}),"\n",(0,o.jsxs)(t.p,{children:["There was a discussion about what the backend should be, if we should use\n",(0,o.jsx)(t.a,{href:"https://grafana.com/products/cloud/",children:"Grafana Cloud"})," or host it ourselves. There\nwould be a lot of benefits of using the cloud, as we would not have to maintain\nthe stack or worry about performance or storage. There was, however, a concern\nabout cost and whether or not log files could be shipped to a cloud provider. In\nthe end we decided to host it ourselves, mostly because we didn't have control\nover what quantities of data we're processing. Now that we have a better\nunderstanding of our usage we can use that to calculate our spend, so we're not\nruling out migrating to Grafana Cloud in the future."]}),"\n",(0,o.jsxs)(t.p,{children:["The collection (scraping) of data is done by ",(0,o.jsx)(t.a,{href:"https://grafana.com/oss/agent/",children:"Grafana\nAgent"}),', which is an "all-in-one" agent that\ncollects metrics, logs and traces. This means a few less moving parts for the\nstack, as we don\'t have to run both ',(0,o.jsx)(t.a,{href:"https://prometheus.io/",children:"Prometheus"}),",\n",(0,o.jsx)(t.a,{href:"https://fluentbit.io/",children:"Fluent Bit"})," and some\n",(0,o.jsx)(t.a,{href:"https://opentelemetry.io/",children:"OpenTelemetry"})," compatible agent for traces. It's a\nrelatively new project, but it's already relative stable and has a lot of\nfeatures. It uses a funky format for configuration called river, which is based\non Hashicorp's HCL. The config enables forming pipelines to process data before\nit's forwarded to Loki, Tempo or Mimir.  It's a bit different, but it works well\nand is easy to understand and configure to our needs."]}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.img,{alt:"Alerting with Grafana",src:n(3319).Z+"",width:"5088",height:"3342"})}),"\n",(0,o.jsx)(t.p,{children:"Using a system like Grafana also enables us to build an integrated experience\nthat also includes alerting. Using Grafana alerting and OnCall, we configure\nalerts that are sent to the correct team based on the service that is failing.\nThis helps the teams get a better overview of what is happening in their\nservices, and also helps us as a platform team to not have to be involved in\nevery alert that is triggered."}),"\n",(0,o.jsx)(t.p,{children:"Overall we're very happy with the LGTM stack, even though it's a fair bit of\nwork to maintain the stack (especially with Istio and other security measures).\nWe're also happy with Grafana, and we're looking forward to seeing what the\nfuture holds for monitoring and observability in Kubernetes."}),"\n",(0,o.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(t.p,{children:"To summarize: We like Anthos, and we think it's a great platform for running\nhybrid Kubernetes. As a platform team we look at each feature on a case-by-case\nbasis, with the goal of giving our developers the best possible experience\ninstead of naively trying to use as much as possible of the platform. Because of\nthis we've decided to use Anthos for Kubernetes and service mesh, but not for\nconfig sync and monitoring. This has given us a great platform that we're\nconfident will serve us well for years to come."}),"\n",(0,o.jsx)(t.p,{children:"Stay tuned for the third and final part of this series, where we'll talk about\nthe benefits we've seen from Anthos, and what we would have done differently if\nwe were to start over."}),"\n",(0,o.jsx)(t.p,{children:(0,o.jsx)(t.em,{children:"Disclaimer - Google, GKE and Anthos are trademarks of Google LLC and this website is not\nendorsed by or affiliated with Google in any way."})})]})}function c(e={}){const{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,o.jsx)(t,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8094:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/acm-1-46a04fbc7dd63ebcae3c08935a5aa51c.png"},7707:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/acm-2-1a9a222556da2f846d4f64bb1978db27.gif"},3582:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/anthos-4-7d2f18bbcb2f378e0657da61ac28fa2f.jpg"},6818:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/anthos-5-41f824768cab6b841ef2fbdc0f5a375b.png"},3125:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/argo-1-10ff6f4861d91a7f670f671e2f0ba43d.png"},5249:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/argo-2-61a9fc0299932ae38d232796c8f4f677.png"},8252:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/gcp-monitoring-1-eb5c94f71f6e0d8c45c6211002c702e5.png"},2138:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/grafana-1-aa91144f9105339f1edf0c1ad1aab0b0.png"},3319:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/grafana-2-849aa5c18bba686f0b10f13a446ff672.png"},4312:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/multi-cluster-37ee6eb4218f5c79582ad4738a942ac6.png"},9412:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/services-67dda8848ee239d94d4edfed09900478.png"},4505:(e,t,n)=>{n.d(t,{Z:()=>o});const o=n.p+"assets/images/workload-5cb93c6dd1fc37775e7194682731de53.png"},1151:(e,t,n)=>{n.d(t,{Z:()=>r,a:()=>i});var o=n(7294);const s={},a=o.createContext(s);function i(e){const t=o.useContext(a);return o.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),o.createElement(a.Provider,{value:t},e.children)}}}]);