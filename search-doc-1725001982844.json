{"searchDocs":[{"title":"Crisis Management Exercises","type":0,"sectionRef":"#","url":"/blog/crisis-management-exercises","content":"","keywords":"","version":null},{"title":"Exercise 1: Malicious actorâ€‹","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#exercise-1-malicious-actor","content":"   The first exercise scenario revolved around a malicious actor gaining privileged access to our production Kubernetes cluster, simulated in this case by our internal sandbox cluster. Admittedly, it was somewhat difficult to set up a realistic scenario without outright disabling some of our security tools, so in the end we simulated a hostile takeover of the user account belonging to the person responsible for planning and running the exercise.  The first sign that something was amiss was an alert from our Sysdig Secure toolset, a Falco-based agent software which continually monitors our cluster for signs of abnormal activity according to a predefined ruleset and provides a SaaS portal for further analysis and management of threats. (We will cover more of our security features and mechanisms and how we try to build a modern kubernetes based application platform with built-in security and zero trust in a future blog post.) After initial examination, we found that the incident was of such a nature that we engaged our crisis management plan in order to investigate, contain and mitigate the incident. We simulated communication with the organization-level crisis management team, having regular meetings in order to keep them informed of progress. Systematic examination of logs and audit logs soon turned up suspicious activity confined to one specific platform developer account, and the decision was made to immediately suspend (simulated in this case) the account, removing all access to organizational systems and in effect locking it out. Simultaneously, the malicious software was removed once enough evidence was secured in order to further analyze the actions and impact of it. The exercise was announced as ended once we suspended the compromised user account and removed the malicious application while retaining and analyzing enough logs, forensic captures and other traces of activity.  ","version":null,"tagName":"h2"},{"title":"Exercise 2: \"Everything is on fire\"â€‹","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#exercise-2-everything-is-on-fire","content":"   The second exercise scenario was somewhat more involved, taking place over two days. The incident itself was as follows: A software update or rogue script caused catastrophic hardware failure in production infrastructure, necessitating creation of a new Kubernetes cluster from scratch. Once the cluster itself and all underlying infrastructure had been created and configured, it would then be up to our platform team to deploy all necessary IAM configuration, service accounts, RBAC and supporting systems (Istio, ArgoCD ++) needed to deploy workloads and restore normal operations. The exercise itself focused on this second phase of restoration, as the infrastructure configuration and cluster creation itself is done by another team, with little involvement by our platform team members.  The failure itself was simulated by having our infrastructure team wipe our sandbox environment and present us with a clean-slate Kubernetes cluster. We called an all-hands meeting and set to work restoring services right away. Right at the onset, we recognized that this was a golden opportunity both to ensure that our documentation was up-to-date, consistent and easy to follow, as well as give our three newest team members some much-needed experience and insight into setting up our services from scratch. We therefore decided that the newest team members would be the ones to actually execute all the actions outlined in our documentation, while the rest of us followed along and made notes, updated documentation and otherwise provided guidance throughout the process.  The first run-through of the recovery process took around 2-3 hours before everything was in working order. Keep in mind that we took the time to update our documentation and explain everything we did while we were working, so in a real-life scenario this would have been even quicker. Once the IAM, RBAC, Istio and ArgoCD was up and running, it was merely a matter of using ArgoCD to synchronize and deploy all relevant workloads. Afterwards, we had a meeting to discuss the process and what experiences we gained from it. Based on the feedback from this meeting, we made further adjustments and updates to our documentation in order to make it even easier to follow on a step-by-step basis, focusing on removing any ambiguity and put any &quot;tribal&quot; knowledge among our platform developers into writing. This ensured that we are way less dependent on the knowledge and skillset of specific people, enabling any team member to contribute to recovery efforts by simply following the documentation.  The newest team members greatly enjoyed being responsible for the recovery effort itself, and expressed a wish to run through the scenario again in order to refine their skills and further improve the documentation. Therefore, we decided to set aside most of day 2 to do just that. We had the infrastructure team tear down and setup the cluster again, and let the newest team members loose on it - this time on their own without guidance - an additional two times. The last run-through of the exercise took between 30 and 60 minutes, a significant improvement from the initial attempt.  All in all, we considered the exercise to be a great success, with many important lessons learned and a substantial improvement in the quality of our documentation and crisis management plans.  ","version":null,"tagName":"h2"},{"title":"What did we learn?â€‹","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#what-did-we-learn","content":"   ","version":null,"tagName":"h2"},{"title":"Lesson 1: You are only as good as your documentationâ€‹","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-1-you-are-only-as-good-as-your-documentation","content":" Documentation is vitally important during a crisis, and should be detailed enough that any team member may follow it on a step-by-step basis and be able to restore normal service, even with minimal knowledge and during a stressful situation. This ensures that you avoid being dependent upon key personnel that might or might not be available during a crisis scenario, and also ensures that you retain vital institutional knowledge even when team members move on to different tasks or even new jobs.  ","version":null,"tagName":"h3"},{"title":"Lesson 2: Logging, logging, logging! Oh, and monitoring too!â€‹","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-2-logging-logging-logging-oh-and-monitoring-too","content":" Having the ability to search through logs of all parts of our system greatly simplifies any incident management, whether the incident revolved around malicious actors or other factors. But logs by themselves are not sufficient - you need some sort of monitoring and alerting system in order to alert on and react to abnormal situations/behaviour in your systems. Ideally, you should be able to react on these alerts instead of messages from users - or worse, customers - that something is wrong.  ","version":null,"tagName":"h3"},{"title":"Lesson 3: Test your plans!â€‹","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-3-test-your-plans","content":" Merely having plans, routines and documentation is insufficient. Unless they have been thoroughly tested and their quality assured through crisis exercises in realistic scenarios and conditions, they should be treated as flawed and unreliable until the opposite is proven. Running crisis management exercises is a great way to expose flaws, insufficiencies and outdated documentation, and careful note-taking and postmortems should be the norm throughout the exercise in order to easily identify and update weak spots in your plans and documentation. As systems and circumstances change, so should plans and documentation too in order to reflect the new order of the day.  ","version":null,"tagName":"h3"},{"title":"Lesson 4: Communicate!â€‹","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-4-communicate","content":" Openness and communication is critical during both exercises and real-world crisis scenarios. Plans should always involve key points of communication - who needs to be informed, whose responsibility it is to keep said people informed, and the frequency, scope and format of information to disseminate. This also applies to communication afterwards. Anyone in your organization should be able to understand what happened, how it was solved and what lessons were learned from it. In Kartverket, we solve this by writing postmortems about incidents, summing up the incident itself and what we learned from it. We favour Blameless Postmortems, enabling us to quickly and thoroughly analayze and document all aspects of an incident without focusing on individual mistakes and avoid passing blame. This contributes to a culture of openness, learning and improvement. Hoarding information and disseminating it only on a &quot;need-to-know&quot; basis only breeds distrust and contempt, as does a culture that focuses on blaming and punishing people for mistakes instead of learning from them. A further bonus when communicating the happenings and results of your crisis management exercises is the potential to inspire others - when people see the great results and lessons you yourselves have gained from such exercises, they might want to try it with their own systems and teams.  ","version":null,"tagName":"h3"},{"title":"Lesson 5: Let the \"newbies\" handle itâ€‹","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-5-let-the-newbies-handle-it","content":" Putting our newest team members in charge of the recovery operations was a great learning experience for them, as well as enabling us to quickly find flaws and shortcomings in our documentation and crisis management plans. It is also a great confidence booster, because if they succeed, they'll gain valuable insight and positive experiences with setting up all those scary critical systems from scratch - and if they don't succeed, well, that's not their fault, it was because the documentation and training was insufficent to enable them to handle the situation!  ","version":null,"tagName":"h3"},{"title":"Lesson 6: Crisis exercises as team buildingâ€‹","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-6-crisis-exercises-as-team-building","content":" Crisis exercises are fun and contribute to better teamwork! They bring everyone together in order to achieve a common goal - get things up and running again as quickly as possible. Combine it with &quot;pair programming&quot; - that is, if possible make sure at least two people are working on any given task together - this helps facilitate cooperation and communication, and provides an extra set of eyes to help catch any manual errors or deviations from the plan.  ","version":null,"tagName":"h3"},{"title":"Thank you for reading!â€‹","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#thank-you-for-reading","content":" We appreciate you taking the time to read through this blog post. We have learned quite a lot (and had lots of fun) through our approach to crisis management exercises. We hope our experiences and thoughts regarding this subject has been interesting, and that they may inspire others to start doing crisis management exercises as well. ","version":null,"tagName":"h2"},{"title":"20 teams on SKIP: What we've learned along the way","type":0,"sectionRef":"#","url":"/blog/20-teams-on-skip","content":"","keywords":"","version":null},{"title":"Principles matterâ€‹","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#principles-matter","content":" When you set out to create something new, you have the privilege of setting some standards that encourage best practices. While this is possible to do for an existing system, in practice it will mean a lot of work to get to the point where you're able to enforce these standards. It's much easier to start with a clean slate.  For our platform, we decided on a set of principles that we wanted to follow. Some of these are:  Stateless: Our clusters are stateless, which means that we can easily replace them if something goes wrong. All configuration is held in a GitOps repository and all state is held in external systems like managed databases, object storage, etc. This significantly reduces operational complexity and recovery time. If a cluster fails we can easily replace or revert it by applying the configuration from the GitOps repository without worrying about losing state, or doing time-consuming data recovery operations.Ownership: For each application, there is a clear owner. This owner is responsible for the application and maintains and supports it. This way we're able to avoid the &quot;tragedy of the commons&quot;, where no one is responsible for an application. If an app has unclear or short-term ownership, you simply don't get to use the platform. We're not an orphanage.Financing: You use the platform? You also need to pay for its continued support and development. We're working towards a chargeback model where your department is billed for the resources they use as a way to ensure that the platform is sustainable. Until this is ready, we expose the costs of the resources used by each team and then negotiate with the departments on how to cover these costs, but this is time-consuming work.Secure by default: We enforce security best practices by default. Examples of this are zero trust networking with Network Policies, where no app can talk to another without explicitly allowing this. Some applications will need to opt out of some of these defaults, and they can do so by altering their configuration. But the defaults are secure, which is especially useful for teams that are new to Kubernetes.  All teams that are onboarded on SKIP are given an introduction to these principles and are expected to follow them. This means that being able to use the modern platform is contingent on the teams being able to prioritize modernizing their applications and working in sustainable ways, which helps push for positive change.  ","version":null,"tagName":"h2"},{"title":"Encourage collaborationâ€‹","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#encourage-collaboration","content":" It's easy for a product team to ask the platform team for help when they're stuck. We're always happy to help, but we also have a heavy workload of exciting things we're working on. Therefore it's much better when platform users can help each other, as this facilitates collaboration and learning. This is why we highly encourage teams to help each other out - to build a community around the platform.  In practice this is done through a single Slack channel where all teams that are using the platform are invited. This is a great place to ask questions, share experiences, and learn from each other - and it's a place where all new features and changes are announced. We used to have many different channels for different teams, but we found that this was not as effective for building a community as a channel where everyone can help each other out.  And a final tip: As a platform developer, sometimes it's better to wait a little while before responding to questions in these channels to allow the community to help each other out before you jump in and help.  ","version":null,"tagName":"h2"},{"title":"Make time for innovationâ€‹","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#make-time-for-innovation","content":" It's easy to get bogged down in the day-to-day work of keeping the platform running. This is why it's important to set aside time for innovation, this is something we take very seriously.  On SKIP we have dedicated innovation days where we work on new features, improvements, and other things that we think will make the platform better. This is an extraordinarily successful initiative, and we've seen many great features come out of these days. It's also a great way to build team morale and to build a culture of learning and innovation.  In practice we have two days in a row of dedicated innovation work every other month. We used to have one day every month, but we found that this was not enough time to really get into the flow of things so we started doing two days every 2 months, which worked better. We also have a rule that you can't work on anything that's on the roadmap, as this is work that we're already going to do. This is a great way to get new ideas and to work on things that might not otherwise get done.    There's a little bit of structure around these days, but not too much.  First, it is understood by everyone that these days are for things that are &quot;useful for Kartverket&quot;. This means that you can't work on your own pet project, but it's vague enough that you can work on pretty much anything that you think will be useful for the organization.  Then, a week before the innovation day we will have a &quot;pitching session&quot;, where everyone who has an idea can pitch it to the rest of the team. This is a great way to get feedback on your idea and to get others to join you in working on it.  Finally, we have a &quot;show and tell&quot; session at the end of the last day where everyone shows what they've been working on. This way we can share our experiences and discuss if this work can be improved and put into production. We encourage everyone to show something, even if it's not finished or you did video lessons, as this creates discussion and further ideas.    There's plenty of examples of features that are results of work done on these days. On-premise Web Application Firewall with Wasm, Grafana features, open source tools like Skiperator andSkyline as well as this very website!  No one has time to prioritize innovation, and we're no different. But we prioritize it anyway, because we know that it's important to keep improving and to keep learning.  ","version":null,"tagName":"h2"},{"title":"Communication is keyâ€‹","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#communication-is-key","content":"   Unfortunately a lot of infrastructure teams don't prioritize communication very well. This is a mistake. Communication is key to building a successful platform.  Your users exist in the context of all the platform features that you have shipped and the changes you will ship in the future. Not informing them and keeping them up to date with what's going on is a surefire way to lose their trust and to make them unhappy.  It starts with simply informing users of the new features that ship. This can be done through a Slack channel, a newsletter, a blog or a town hall meeting. We use a combination of all of these, but the most important thing is that you inform your users of what's coming. An added benefit of this is helps push adoption of new features and excitement around the platform by showcasing innovation.  The next step is informing users on what will ship and when. This will help users plan their work and to know what to expect, but it also helps users feel involved when they see their requests being planned. This can be done through a roadmap, a technical forum, or a blog. We use a combination of all of these, but the easiest way to do this is to have a roadmap that you keep up to date on a regular basis.  Now for the hard part: When things go wrong, you need to communicate this as well. Product teams will want to know when their applications are affected by outages or other issues, and they will want to know what you're doing to fix it. This can be done through a status page, a Slack channel, or postmortems. Again, we use a mix of these so that we can reach as many users as possible at the right time.  Do these things and you will have happy users that feel informed.  ","version":null,"tagName":"h2"},{"title":"Branding is importantâ€‹","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#branding-is-important","content":"   Do you think Spotify would be as successful if it was called &quot;Music Player&quot;? Do you think Apple would be as successful if it was called &quot;Computer Company&quot;? Of course not. Branding is important. It builds a sense of identity and community around your platform.  This is especially important for a platform team, as you're not just building a product, you're building a community. You want your users to feel like they're part of something bigger, and you want them to feel excited to use the platform.  When you're starting out, you want to drive adoption. Here a brand really helps as it's easier to talk about a good brand in a positive way. It's also easier to get leadership buy-in when you have a strong brand.  This holds true when you're more established as well. When you grow larger than your ability to talk to everyone, a brand helps you communicate your values and intent to your users, which will drive organic growth from teams that want to work with you.  A minimum viable brand is a logo, a name, and a color scheme. This is something you should think deeply about, as it's something that will stick with you for a long time. After this you can think about a website, merchandise like stickers and t-shirts, and a mascot. These things are not necessary, but they can help build a sense of identity and community around your platform.  ","version":null,"tagName":"h2"},{"title":"Using the cloud is a long journeyâ€‹","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#using-the-cloud-is-a-long-journey","content":" As a platform team, it's our responsibility to push for modern, user-friendly and secure solutions. This generally means using public cloud solutions like Google Cloud Platform. But for most organizations, pushing this narrative incurs significant friction and to some extent fear due to legal and cost concerns. This is understandable, as the known is always more comfortable than the unknown, and it's a view that's hard to change.  This is why it's important to take a long-term view on this. You're not going to move everything to the cloud overnight, and you're not going to convince everyone to get on board with this idea overnight. It's a long journey, and you need to be patient and persistent.  We've spent years pushing for the cloud, and we're still not there. You're going to have to participate in many (many!) meetings, and you're going to have to fight for every little thing over and over again. But it's necessary. Once everyone has a clear understanding of the risks and how to mitigate them, you will be able to formulate a document guiding the organization's teams on how to get to the cloud from a compliance point of view.  If you asked me for any recommendations on how to get to the cloud as easily as possible, it would be to first get leadership buy-in across the organization. This is important, as it will make any large initiative like cloud migration easier. After this and a competent platform team is in place, you can start pushing for the cloud technologies and eventually cloud migration. Here you need to talk directly with the legal team, not via other people. Have representatives of the platform team sit down with the lawyers and talk through the risks and how to mitigate them. This is the only way you can combine the technical and legal aspects of this work. Working in silos and not talking to each other is a surefire way to fail.  ","version":null,"tagName":"h2"},{"title":"Autonomy and platform as a productâ€‹","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#autonomy-and-platform-as-a-product","content":" Your platform is a product, and so you need to work as a product team. This means continuously improving your product, listening to your users, and building the features that they need.  Research-based literature like &quot;Team Topologies&quot; establishes the importance of autonomous teams in modern organizations. Traditional top-down organizations are just not going to be able to have as close of a relationship with their stakeholders as a team that is able to proactively understand the needs of their users and make their own decisions that push continuous improvement of their products. This is why it's important, even for infrastructure teams, to be able to own their roadmap and make decisions on what to build when.  As a team you're obviously limited to the amount of resources you have and not able to do everything, so understanding the needs of your stakeholders and prioritizing them is essential. You need to do research to know the needs of your users; sometimes requests don't align well with the actual needs. Just because someone asks loudly for something, doesn't mean it's the right fit for your platform. Saying yes to everything does not result in a good product. Dare to challenge assumptions and ask why.  ","version":null,"tagName":"h2"},{"title":"Abstractions save timeâ€‹","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#abstractions-save-time","content":" It should go without saying that a platform team's job is to make tools that make product teams' jobs easier. But it really can't be said enough. The better the tooling you provide, the less you have to do support. This is a win-win for everyone.  When building tools, think about how you can abstract away complexity. This can be done in many ways, but we've had great success building an operator that abstracts away the complexity of managing applications on Kubernetes. The operator is called Skiperator and makes deploying applications on Kubernetes as easy as writing a configuration manifest.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: namespace: sample name: sample-two spec: image: nginxinc/nginx-unprivileged port: 80 replicas: 2 ingresses: - foo.com - bar.com   The key takeaway here is that abstractions like Skiperator are designed to speak the language of the user. There is no mention of NetworkPolicies or Istio VirtualServices in the configuration, as these are things that the user generally doesn't have any knowledge of. Instead, the user can specify things like &quot;I want to expose this service to the internet&quot; or &quot;I want to run this job every day at midnight&quot;. This simplifies the user experience of Kubernetes, which is a complex system, and makes it easier for users to get started.  Work smarter not harder.  ","version":null,"tagName":"h2"},{"title":"Build forward- and backwards compatibilityâ€‹","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#build-forward--and-backwards-compatibility","content":" We've had multiple experiences where we've weighed our options and decided to make a breaking change. Just recently we asked our users to migrate their apps from one cluster to another in order to improve the architecture of the platform. Multiple options were considered, but in the end the scale of the changes meant that upgrading the clusters in-place would not be practical, so we commissioned new clusters with the new architecture and asked users to migrate their apps.  In our case, we had a simple way to migrate, only requiring moving a config file from one directory to another to make the change. But even so, this was a time consuming process for our users, and a laborious process for us to support. This is because even though the change was simple, it was still a change that required testing and validation, and it was a change that was not necessarily the highest priority for the teams that were asked to make it. So even though the change was simple, it took months.  If you ask your users to make changes to their applications, you're asking a team that is already busy to do more work. Any changes you ask them to make will take time, as it would not necessarily be the highest priority for them. Therefore avoiding breaking changes should be a primary goal, so wherever possible building in forward and backwards compatibility by inferring as much as possible from the existing configuration is a good thing.  When building operators, don't change or remove fields that are in use. Use default values for new fields, and use lists of objects instead of raw values like lists of strings as they are easier to extend.  ","version":null,"tagName":"h2"},{"title":"Documentation is keyâ€‹","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#documentation-is-key","content":" One thing we keep hearing from our users is the need for more and better documentation. This is understandable. When you're using a platform, you don't want to have to ask for help all the time - you want to be able to discover platform features and implement them yourself with the support of good documentation.  The point here is that as a platform team you need to prioritize documentation. A task is not done until it has been documented. This way announcing new features will always include a link to the documentation where users can dive deeper into the feature and how to use it, like the example below.    The bigger challenge here is preventing documentation from going stale. It's too easy to forget about updating documentation to reflect changes in the code. Here we can share a few tips from our experience:  First, the obvious way to keep docs up to date is to allocate time to update them. One way we do this is that a few times a year we will do a documentation grooming session where we huddle together and review documentation, rewriting it when we find out of date information.  A more interesting way to keep docs up to date is changing how you respond to questions. Instead of answering questions immediately, we should be asking ourselves: &quot;How can we make sure that this question never gets asked again?&quot;. In our case we spend some time to write documentation or improve existing documentation and reply with a link to the documentation page. This is a triple win, as you will now have more updated documentation, save time in the future by being able to refer to the improved docs instead of writing a lengthy response and the user will now know where to look for answers.  ","version":null,"tagName":"h2"},{"title":"Learn from othersâ€‹","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#learn-from-others","content":" When building a platform you'll quickly learn that you don't have all the answers. You might discuss how to implement a feature with your team, but you might not have the experience to know what works well in this context. When you get into this situation, an outside perspective can be crucial to avoid making costly mistakes.  One great advantage of working in the public sector is that we can ask other public sector platform teams for advice and learn from their experiences. We can also share our experiences with others, which is usually interesting. Invest some time in building these relationships of mutual benefit.    I also want to give special credit to Hans Kristian Flaatten and the Public PaaS network here. Having a shared forum to discuss platform issues is a strong asset and helps the Norwegian public sector get ahead and stay competitive.  Even if you work in the private sector, you can still learn from other organizations. Honestly, if you want to learn from someone's experiences it never hurts to ask. Teams generally want to help each other out, and it's usually possible to make a trade of some sort. I suggest to offer to give a talk on your experiences and ask if they can do the same. It's a win-win for both parties.  ","version":null,"tagName":"h2"},{"title":"Conclusionâ€‹","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#conclusion","content":" You may think building a platform is mostly technology, and we've written a lot about technology in previous blog posts. But it's important to remember that building a platform is also about building a community, and communities have expectations and needs that go beyond technology. This is a strength, and not a weakness, as if you're able to inspire and motivate your users you will be able to build a platform that is sustainable and that drives positive change in your organization.  Best of luck in your endeavors! ","version":null,"tagName":"h2"},{"title":"Hybrid Kubernetes in production pt. 1","type":0,"sectionRef":"#","url":"/blog/hybrid-kubernetes-in-production-part-1","content":"","keywords":"","version":null},{"title":"So why a hybrid cloud?â€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#so-why-a-hybrid-cloud","content":"   Were you to take the time machine back a few years, you would see Kartverket as a traditional enterprise with a lot of knowledge and experience in running on-premise workloads. This knowledge served us well, but also slightly held us back in terms of our imagination. We knew that there had to be a better way, but our enterprise was simply not mature enough to adopt a pure cloud strategy. The fear of the unknown cloud weighed heavily on many people, and therefore few people wanted to take the risk of moving to the cloud.  This is something we've worked on for a long time, and still are. After a long time of working with the stakeholders in the organization, we eventually built a cloud strategy, which in simple terms stated that we would prefer SaaS-products over hosting things ourselves, and that we would gradually move our workloads to the cloud.  This cloud strategy however, which cleared up a lot of blockers, came too late for us on SKIP. At that point we had already done most of the work on our on-premise platform, building on the assumptions the organization held at the time, which was that we met our needs through existing infrastructure and that using public cloud had disqualifying cost and compliance implications. For SKIP it was therefore full steam ahead, building the on-prem part first, then adding the hybrid and cloud part later.  It's not like we would have ended up with a pure cloud setup in any case, though. If you're at all familiar with large enterprises, you will know that they are often very complex. This is also true for Kartverket, where we have a lot of existing systems that are not easy to move to the cloud. We also have a lot of systems that are not suitable for the cloud, mostly because they are designed to run in a way that would not be cost effective in the cloud. In addition we have absolutely massive datasets (petabyte-scale) that would be very expensive to move to the cloud.  Because of these limitations, a pure cloud strategy is not considered to be a good fit for us.  A hybrid cloud, however, can give us the scalability and flexibility of the cloud, while still allowing us to run some of our systems on-prem, with the experience being more or less seamless for the developers.  ","version":null,"tagName":"h2"},{"title":"Why we chose Anthosâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#why-we-chose-anthos","content":" After some disastrous issues with our previous hybrid cloud PoC (that's a whole story in itself) we decided to to look at what alternatives existed on the market. We considered various options, but eventually decided to run a PoC on Anthos. This was based on a series of conditions at the time, to name a few:  We had a decent pool of knowledge in GCP compared to AWS and Azure at the timeSome very well established platform teams in the public sector were also using GCP, which meant it would be easier to share work and learningsAnthos and GCP seemed to offer a good developer experience, which for us as a platform team is of paramount importanceA provider like Google is well established in the cloud space (especially Kubernetes), and would have a fully featured, stable and user friendly product  SKIP ran the Anthos PoC over a few months, initially as an on-prem offering only. Drawing on the knowledge of internal network and infrastructure engineers, this took us all the way from provisioning clusters and networking, to iterating on tools and docs and finally onboarding an internal product team on the platform. Once we felt we had learned what we could from the PoC, we gathered thoughts from the product team, infrastructure team and of course the SKIP platform team.  The results were unanimous. All the participants lauded the GCP user interfaces that allowed visibility into running workloads, as well as the new self-service features that came with it. Infrastructure engineers complimented the installation scripts and documentation, which would make it easier to keep the clusters up to date.  Based on the total package we therefore decided to move ahead with Anthos. To infinity and beyond! ðŸš€  ","version":null,"tagName":"h2"},{"title":"What is Anthos anyway?â€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#what-is-anthos-anyway","content":"   Anthos is Google's solution to multicloud. It's a product portfolio where the main product is GKE (Google Kubernetes Engine) on-premise. Using GKE on-prem you can run Kubernetes clusters on-premise and manage them from the same control plane in Google Cloud, as if they were proper cloud clusters.  In fact, Anthos is truly multi-cloud. That means you can deploy Anthos clusters to GKE and on-prem, but also AWS and Azure. On other cloud platforms it uses the provider's Kubernetes distribution likeAKS, but you can still manage it from GKE alongside your other clusters.  In addition to GKE, the toolbox includes:  ","version":null,"tagName":"h2"},{"title":"Anthos Service Mesh (ASM)â€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#anthos-service-mesh-asm","content":" A networking solution based on Istio. This is sort of the backbone of the hybrid features of Anthos, as provided you've configured a hybrid mesh it allows applications deployed to the cloud to communicate with on-premise workloads automatically and without manual steps like opening firewalls.  All traffic that flows between microservices on the mesh is also automatically encrypted with mTLS.  ","version":null,"tagName":"h3"},{"title":"Anthos Config Managment (ACM)â€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#anthos-config-managment-acm","content":" A way to sync git repos into a running cluster. Think GitOps here. Build a repo containing all your Kubernetes manifests and sync them into your cluster, making cluster maintenance easier.  ACM also includes a policy controller based on Open Policy Agent Gatekeeper (OPA) which allows platform developers to build guardrails into developers' workflows using policies like &quot;don't allow containers to run as root&quot;.  ","version":null,"tagName":"h3"},{"title":"Anthos Connect Gatewayâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#anthos-connect-gateway","content":" The connect gateway allows developers to log on to the cluster using gcloudand kubectl commands, despite the cluster potentially being behind a firewall. From a user experience standpoint this is quite useful, as devs will be logged in to GCP using two factor authentication, and the same strong authentication allows you to access kubernetes on-premise.  Connect Gateway also integrates with GCP groups, enabling RBAC in Kubernetes to be assigned to groups instead of manually administered lists of users.  Currently the connect gateway only supports stateless requests, for examplekubectl get pods or kubectl logs (including -f). It does not supportport-forward, exec or run, which can be a bit annoying.  ","version":null,"tagName":"h3"},{"title":"Summaryâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#summary","content":" As you can see, the above tools gives us a lot of benefits.  Combined with the power of Google Cloud and Terraform, they give us a good combination of flexibility through cloud servicesEase the maintenance by using the tools that Anthos and Terraform supply usEases the compliance and modernization burden by allowing a gradual or partial migration to cloud, allowing parts to remain on-premise while still retaining most of the modern tooling of the cloud  That's it for now! ðŸ™‚ We'll be back with more details on how we run Anthos as well as the pros and cons we've seen so far in the coming weeks. Stay tuned!  Disclaimer - Google, GKE and Anthos are trademarks of Google LLC and this website is not endorsed by or affiliated with Google in any way. ","version":null,"tagName":"h2"},{"title":"Hybrid Kubernetes in production pt. 3","type":0,"sectionRef":"#","url":"/blog/hybrid-kubernetes-in-production-part-3","content":"","keywords":"","version":null},{"title":"Do you really need hybrid?â€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#do-you-really-need-hybrid","content":" When we started out, there was an assumption that it was simply impossible to use the cloud. This came from all sides of the organization, so this was taken as a given. SKIP was therefore started as a project to build an on-premise Kubernetes platform to service our needs as a transition to cloud native development principles.  As we moved along, a lot of these assumptions got challenged. We found that most of these assumptions were based on misunderstandings or a lack of a deeper understanding of cloud technologies and the surrounding legal aspects. This led to a fear of the unknown, and subsequent inaction. In the end it turned out that quite a lot of our workloads could indeed run in the public cloud, given some minor adjustments.  Had we started out with the knowledge we have now, we would probably have started with a public cloud provider, and then moved to hybrid when and if we saw a need for it. Using a cloud provider's managed Kubernetes offering is significantly easier than running your own, and you can get started much faster, with less risk.  Given our organization, we would probably have ended up with hybrid anyway, but that complexity could potentially have been moved down the timeline to a point where the platform was more mature.  Starting with hybrid is a massive undertaking, and you should have a good reason for doing so. Do you need hybrid, or do you just need to mature your organization? If you do, reduce the scope of the initial work to get to a workable platform, and preferably start in the cloud, adding hybrid features later. If you're not sure, you probably don't need hybrid.  ","version":null,"tagName":"h2"},{"title":"Hybrid gives your organization flexibilityâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#hybrid-gives-your-organization-flexibility","content":"   Now that we've built a platform that seamlessly runs workloads in both public cloud and on-premise, we have a lot of flexibility in where we run our workloads and how we manage them. Our experience is that this makes it easier for the organization to mature legacy workloads.  All our greenfield projects are written with cloud native principles in mind, which makes it trivial to run them in the cloud. Legacy workloads, however, are not so lucky. They are often written with a lot of assumptions about the underlying infrastructure and are not cognizant of the resources they use. This means they are a poor fit to lift and shift to the cloud, as they will often be expensive and inefficient.  With a hybrid platform, we can use our on-premise offering as a spring board for modernization. Product teams will start by shifting their app to our on-premise Kubernetes platform, and then gradually modernize it to be cloud native. This method gives a few immediate benefits from the lift and shift like better observability, developer experience and security features but also gives fewer of the drawbacks, as the on-premise cloud is closer to the existing dependencies than a public cloud. Once this is done, smaller chunks kan be rewritten as microservices and moved to the cloud, communicating with the monolith seamlessly over the hybrid network. This is sometimes referred to as the strangler application.  This method significantly reduces the scope of refactoring, as one can focus on gradually rewriting smaller modules instead of rewriting the entire application.  ","version":null,"tagName":"h2"},{"title":"Service mesh is hard, but maybe a necessary evil to make hybrid less painfulâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#service-mesh-is-hard-but-maybe-a-necessary-evil-to-make-hybrid-less-painful","content":"   Oh my word how we have struggled with service mesh.  Starting from nothing with a goal of providing a secure-by-default zero-trust network layer with observability and traffic control is quite an undertaking, especially when you pair that with setting up a new kubernetes-based infrastructure from scratch. Istio is famously complex, and we've had our fair share of that.  So how do we feel about Istio? There are various opinions in the team, but if we average them all out, we're content. It's quite complex and can be hard to debug, but it does the job. As we've matured and gotten more experience with Istio, we've also started to see more benefits, like extensions for handling OAuth2and the traffic control features for gradual rollouts which we used for canary-testing the migration of some of our larger applications to SKIP. Not all of these features, like EnvoyFilters, are supported by Anthos Service Mesh (ASM), which is why we're exploring using upstream Istio instead of ASM.  One thing we quickly learned is to not let the product teams configure the service mesh directly using service mesh resources. This is a recipe for disaster. We tried this in the beginning, and first of all it's a huge complexity burden for the product teams. We also started getting a lot of weird issues when product teams would configure the mesh in ways that broke their encapsulation. Since the service mesh is a cluster-wide feature, if one team makes an invalid configuration, it can break other teams' workloads. Kubernetes namespaces be damned. We've therefore moved to a model where the platform team provides an abstraction throughSkiperator which configures the service mesh on their behalf.  Finally, I think it's prudent to ask yourself wether or not you actually need a service mesh. If you're running a small cluster with a few services, you'll probably be fine with using the built-in Kubernetes features like Ingress and Network Policies. The observability features are nice, but you can get most of them with a combination of instrumentation and Grafana.  If you need service mesh then limit the scope until you get comfortable with the mesh, for example start with just mTLS and observability, and then add zero trust networking features later.  Also keep in mind there is a lot of competition in the service mesh space, and there are some interesting alternatives to Istio, likeLinkerd and the up-and-coming Cilium Service Mesh.  ","version":null,"tagName":"h2"},{"title":"Anthos helps you as a platform team getting started with best practices.. Even if you plan to move to open source components laterâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#anthos-helps-you-as-a-platform-team-getting-started-with-best-practices-even-if-you-plan-to-move-to-open-source-components-later","content":"   When our platform team started out a few years ago, we picked some of the brightest cloud engineers from within the organization and combined them with some consultants to work on the platform. Most of these engineers had some experience working with Kubernetes and cloud, but not building something of this scale from scratch. The first months would therefore be a learning experience for most of the team.  I think a lot of teams will be in a similar situation, and this is where a managed service like Anthos can be a huge help. Anthos is built with best practices in mind, so a lot of the architecture decisions were built-in to the installer. Choosing a managed offering, even when running on-prem has therefore helped us deliver value to the product teams much quicker than if we had to build everything from scratch.  What's important to point out is that choosing something that is managed does not rule out using open source components later. We started out using all the parts that Anthos gave us, including service mesh, logging, monitoring and configuration management. Managed services do come with some tradeoffs, however, as you lose some of the finer control of the platform. As the team has matured and gained experience, we've started to replace some of these components with open source alternatives, which has helped us save money and gain more control over our platform. This has the downside of having to maintain these components ourselves, but with more experience in the team, this is a tradeoff we feel is worth it.  Even though we're increasingly using more open source components, we don't regret using a paid managed offering in the beginning. It helped us get started and make the right decisions early on, and we're now in a position where we can capitalize on that great start.  ","version":null,"tagName":"h2"},{"title":"Keep in mind autoscaling when choosing licensing modelsâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#keep-in-mind-autoscaling-when-choosing-licensing-models","content":"   This may be an obvious point to some of the more experienced platform engineers out there, but it was still something that we had to learn. When we started out, we appreciated the simplicity of SaaS products that billed per node, as it made it easy to predict costs. We could simply look at the number of nodes we had running and multiply that with the price per node to get a relatively accurate estimate of what this offering would cost. This would turn out to be a double edged sword, however.  It is safe to assume that one of the reasons people choose Kubernetes is the ability to scale workloads easily. This could be scaling up to handle more traffic, or scaling down to save money. This is a great feature, but as the number of workloads grow, the provisioned nodes will start to become insufficient and new nodes will be provisioned. With Kubernetes and Anthos on VMware this can be done automatically, which is a fantastic feature.  The problem arises when you scale out more nodes and have a static license that bills per node. We've made the mistake of getting contracts with two (now just one) SaaS providers where we order a set of nodes, let's say 10, and when workloads scale up, we end up with more than 10 nodes. This means we're not running that SaaS-service's agents on the new nodes, which can be anything from inconvenient to critical, depending on the service. In the end we've had to restrict our node scaling to avoid this issue, which goes against the whole ethos of Kubernetes. We're also provisioning bigger nodes than we need to avoid scaling out, which can be suboptimal.  We're now working with the vendors to get a more flexible license that bills per node on demand, but this is something to keep in mind when choosing a SaaS offering. Try to factor in the future scaling needs of your platform when purchasing SaaS services.  ","version":null,"tagName":"h2"},{"title":"Summaryâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#summary","content":" To summarize: We've learned a lot on our journey to building a hybrid Kubernetes platform. Over the last few years we've iterated on our platform and learned lots of great lessons. It's been a huge help and privilege to have the support of our organization, especially in terms of us being allowed to fail and learn from our mistakes. The Norwegian saying &quot;it's never too late to turn around&quot; comes to mind, as we've changed course several times on our journey, sometimes to the annoyance of our product teams who depend on a stable platform - but in the end we've ended up with a better product - a platform we can be proud of and that our product teams love using.  Thanks for reading this series on Anthos and hybrid Kubernetes. We hope you've learned something from our experiences, and that our hard earned lessons can help you on your journey to building a hybrid Kubernetes platform.  Disclaimer - Google, GKE and Anthos are trademarks of Google LLC and this website is not endorsed by or affiliated with Google in any way. ","version":null,"tagName":"h2"},{"title":"Hybrid Kubernetes in production pt. 2","type":0,"sectionRef":"#","url":"/blog/hybrid-kubernetes-in-production-part-2","content":"","keywords":"","version":null},{"title":"Installation and upgradesâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#installation-and-upgrades","content":"   We have been early adopters of Anthos, so when doing the install we did not have options for controlplane architecture. We wanted to use existing underlying VMware infrastructure, so the nodes in our clusters are VMs, provisioned by scripts provided by Google. Our cluster is installed withkubeceptioncontrolplane architechture, this no longer the only, or recommended way. The recommended model is Controlplane V2, where the controlplane nodes for the user cluster are in the user cluster itself.  In the kubeception model, Kubernetes clusters are nested inside other Kubernetes clusters. Specifically, the control plane of the user clusters runs in an admin-cluster. For each on-premise cluster created, a new set of nodes and a namespace are created in the admin cluster.  To install and make changes to the admin cluster, an admin workstation is required, which must be located in the same network as the admin cluster. All configurations are done using a CLI tool called gkectl. This tool handles most cluster administration tasks, and the cluster specific configuration is provided in YAML files.  Our cluster setup is more or less static, and most cluster administration tasks involve upgrading or scaling existing clusters. The SKIP team has a cluster referred to as â€œsandboxâ€, which is always the first recipient of potentially breaking changes. After testing in sandbox, we'll deploy changes to both development and test environments, and if nothing breaks, we roll out the changes to our production environment. This is mostly done outside work-hours, although we have not experienced downtime during cluster upgrades. Here is the general workflow for upgrading:  Upgrade your admin workstation to the target version of your upgrade.From your admin workstation, upgrade your user clusters.After all of the user clusters have been upgraded, you can upgrade your admin cluster from the admin workstation.  We have tried using Terraform where possible to simplify the setup. This can not be done in the same way for clusters using the kubeception model. When we migrate to Controlplane V2 however, clusters can be managed via GCP, and we can finally start using terraform for our on-premise cluster config in the same way as for our GKE clusters, and GCP configuration in general.  ","version":null,"tagName":"h2"},{"title":"GCP integrationâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#gcp-integration","content":" When working with an on-premise Anthos cluster, some of the nice-to-have features of a standard GKE cluster have been lost. However, recently Anthos on VMware clusters have gradually received more and more features compared to GKE clusters.  ","version":null,"tagName":"h2"},{"title":"IAM and Groupsâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#iam-and-groups","content":" Since we were early adaptors of Anthos, we had to endure not being able to delegate clusterroles to IAM groups, and had to add single users to clusterrole/rolebindings in Kubernetes. This was not a huge problem for us, since we were working with a very limited number of teams and devs, but it was apparent that this was not going to scale well. Luckily we got support for groups before it was a problem, and our config files went from containing way too many names and email addresses, to only containing groups.  Our Google Workspace receives groups and users from our Microsoft Active Directory. Groups are initially created either in Entra ID, or on our local Domain Controllers, and at set intervals changes are pushed to Google Workspace.Role-based access control (RBAC) based on membership in these groups was needed. We wanted to manage this through Terraform, and created a repo with where we store and configure our entire IAM configuration. Since we have had growing adoption of Kubernetes and public cloud in our organization, more teams, projects and apps have been onboarded to SKIP, and this IAM repo has grown. We've tried to simplify the structure more than once, but since this is a problem not affecting dev teams, we have chosen to prioritize other tasks.  ","version":null,"tagName":"h3"},{"title":"Workloadsâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#workloads","content":" All clusters created in in Anthos can be viewed from the GCP console, and theConnect gatewaymakes it possible to do management from the console (or via kubectl) as well. The GCP console can be used to get information about, or manage the state of the cluster, workloads and resources present. This is a web GUI, part of the GCP console, and not as snappy as cli-tools, but still usable, and intuitive to use.  This view shows workloads running in the argocd namespace. All workloads displayed here can be clicked, and explored further.  When accessing the cluster via the Connect gateway there are some limits. The Connect gateway does not handle persistent connections, and this makes it impossible to do exec, port-forward, proxy or attach. This is not a problem for a production environment, where containers should never be used in this way. But for a dev, or sandbox environment, this is a bit of a pain-point.  This issue should be partially fixed in Kubernetes 1.29 and should be completely resolved in Kubernetes 1.30.  ","version":null,"tagName":"h3"},{"title":"Service Meshâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#service-mesh","content":" A Service Mesh in Kubernetes is an infrastructure layer that manages communication between services. We are using Anthos Service Mesh (ASM), which is based on Istio and nicely integrated with the GCP console. It's easy to get an overview of services, the connection between them, and what services are connected to either our internal or external gateways. This can be displayed in a Topology view, or if you click on a service, you'll get a more detailed drilldown.  A snippet of services running in our sandbox cluster.  When we deploy services to our cluster we create almost all Kubernetes and service-mesh resources with our custom operator;Skiperator. This operator configures the resources to fit our setup, and applies &quot;best practices&quot; the easy way. This has been one of the great success stories in SKIP, and Skiperator is in continuous development.  ","version":null,"tagName":"h3"},{"title":"Deploymentâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#deployment","content":" Deployment is a very interesting subject when it comes to Anthos. As a platform team, it is our job to make sure that deployment is as quick and convenient as possible for the product teams. This ambition has led us to iterate on our processes, which has finally led us to a solution that both we and the developers enjoy using.  ","version":null,"tagName":"h2"},{"title":"Iteration 1 - Terraformâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#iteration-1---terraform","content":" When we first started out with Anthos, we had a very manual process for deploying applications. A service account was provisioned in GCP, which allowed the developers to impersonate a service account in Kubernetes, which in turn allowed them to deploy apps using Terraform. This approach worked, but had a decent amount of rough edges, and also would fail in ways that was hard to debug.  With this approach the developers would have to manage their own Terraform files, which most of the time was not within their area of expertise. And while SKIP was able to build modules and tools to make this easier, it was still a complex system that was hard to understand. Observability and discoverability was also an issue.  Because of this we would consistently get feedback that this way of deploying was too complicated and slow, in addition handling Terraform state was a pain. As a platform team we're committed to our teams' well being, so we took this seriously and looked at alternatives. This was around the time we adopted Anthos, so thus Anthos Config Managment was a natural choice.  ","version":null,"tagName":"h3"},{"title":"Iteration 2 - Anthos Config Managment (ACM)â€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#iteration-2---anthos-config-managment-acm","content":"   ACM is a set of tools that allows you to declaratively manage your Kubernetes resources. Here we're mostly going to talk about Config Sync, which is aGitOps system for Kubernetes.  In a GitOps system, a team will have a Git repository that contains all the Kubernetes resources that they want to deploy. This repository is then synced to the Kubernetes cluster, and the resources are applied.  This can be likened to a pull-based system, where the GitOps tool (Config sync) watches the repo for changes and pulls them into the cluster. This is in contrast to a push-based system, where a script pushes the changes to a cluster. It is therefore a dedicated system for deployment to Kubernetes, and following the UNIX philosophywhich focuses on doing that one thing well.  Using this type of a workflow solves a lot of the issues around the Terraform based deployment that we had in the previous iteration. No longer do developers need to set up a complicated integration with GCP service accounts and impersonation, committing a file to a Git repo will trigger a deployment. The Git repo and the manifests in them also works as a state of truth for the cluster, instead of having to reverse engineer what was deployed based on terraform diffs and state.    It started well, however we soon ran into issues. The system would often take a long time to reconcile the sync, and during the sync we would not have any visibility into what was happening. This was not a deal breaker, but at the same time this was not a particularly good developer experience.  We also ran into issues with implementing a level of self-service that we were satisfied with. We wanted to give the developers the ability to provision their own namespaces, but due to the multi-tenant nature of our clusters we also had to make sure that teams were not able to write to each others' namespaces. This was not a feature we were able to implement, but luckily our next iteration had this built in, and we'll get back to that.  The final nail was the user interface. We simply expected more from a deployment system than what ACM was able to provide. The only view into the deployment was a long list of resources, which to a developer that is not an expert in Kubernetes, was not intuitive enough.  ","version":null,"tagName":"h3"},{"title":"Final iteration - Argo CDâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#final-iteration---argo-cd","content":"   This finally brought us to our current iteration. We had heard about Argo CD before, but initially we were hesitant to add another system to our stack. After ACM had introduced us to GitOps and we looked deeper into Argo CD, it was obvious to us that Argo was more mature and would give our developers a better user experience.  The killer feature here is the UI. Argo CD has an intuitive and user-friendly UI that gives the developers a good overview of what is deployed. Whenever anything fails, it's immediately obvious which resource is failing, and Argo allows you to drill down into the resource to see the details of the failure, logs for deployments, Kubernetes events, etc.    The above photo illustrates this well. Here you can see a project with a number of Skiperator applications. The green checkmarks indicate that the application is synced and the green heart indicates that the application is healthy. A developer can see the underlying &quot;owned&quot; resources that Skiperator creates (such as a deployment, service, etc), and get a look &quot;behind the curtain&quot; to see what is actually deployed. This helps debugging and gives the developers a better insight into what is happening during a deployment.  In terms of multi tenancy, Argo CD has a concept of projects. A project is a set of namespaces that a team has access to, and a team can only use Argo to sync to namespaces that are part of their project. The namespace allowlist can also include wildcards, which sounds small but this solved our self-service issue! With our apps-repo architecture, we would give a team a &quot;prefix&quot; (for example seeiendom-), and that team would then be able to deploy to and create any namespace that started with that prefix. If they tried to deploy to another team's namespace they would be stopped, as they would not have access to that prefix.  The prefix feature allows product teams to create a new directory in their apps repo, which will then be synced to the cluster and deployed as a new namespace. This is a very simple and intuitive workflow for creating short-lived deployments, for example for pull requests, and it has been very well received by the developers.  The apps-repo architecture will be a blog post itself at some point, so I won't go too much into it.  And finally, if you're wondering what disaster recovery of an entire cluster looks like with Argo CD, I leave you with the following video at the end.    ","version":null,"tagName":"h3"},{"title":"Hybrid Meshâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#hybrid-mesh","content":" A hybrid mesh service mesh configuration is a setup that allows for service networking across different environments. For Kartverket this includes a hybrid cloud environment. The setup involves several steps, including setting up cross-cluster credentials, installing the east-west gateway, enabling endpoint discovery, and configuring certificate authorities. All clusters in a hybrid mesh are registered to the same fleet host project, and istiod in each cluster must be able to communicate with the Kube-API on the opposing clusters.  ASM is as previously mentioned based on Istio, and after some internal discussion we decided to experiment with running vanilla upstream Istio in our GKE clusters running in GCP. Pairing it with ASM in our on-premise clusters worked as expected (after a bit of config), and we are now running upstream Istio in GKE, with ASM on-prem in a multi-cluster setup. We also looked into using managed ASM in our GKE cluster, this was hard for us however, due to it requiring firewall openings on-prem for sources we could not predict.    We have chosen the Multi-Primary on different networksafter reviewing our network topology and configuration. We connect our on-premise network, with the GCP VPC through a VPN connection (using host and service projects). To have a production ready environment, the VPN connection must be configured with redundancy.  We're working towards getting this architecture into production, as this will enable us to seamlessly use GKE clusters in GCP together with our on-premise clusters. The elasticity of cloud infrastructure can be utilized where needed, and we can handle communication between services on different clusters much more smoothly. This has been a bit of a journey to configure, but as a learning experience it has been valuable. Being able to address services seamlessly and communicate with mTLS enabled by default across sites, zones and clusters without developers having to think about it feels a bit like magic.  ","version":null,"tagName":"h2"},{"title":"Monitoringâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#monitoring","content":" ","version":null,"tagName":"h2"},{"title":"Google Cloud Monitoringâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#google-cloud-monitoring","content":"   GKE Enterprise includes an agent that collects metrics from the cluster and sends them to Google Cloud. This is a great feature which makes it relatively easy to get started with metrics and monitoring. However, we have decided not to use the agent, and instead use Grafana and LGTM for metrics and monitoring.  This is mainly due to a couple of challenges:  The amount of metrics that are collected out of the box and sent to GCP contributes a significant part of our total spend. It's not that we have a lot of clusters, but the amount of metrics that are collected out of the box is very high, and Anthos' default setup didn't give us the control we needed to be able to manage it in a good way.  Note that this was before Managed Service for Prometheus was released with more fine grained control over what metrics are collected. It is now the recommended default, which should make metrics collection easier to manage.  Second, while Google Cloud Monitoring has a few nice dashboards ready for Anthos, it feels inconsistent which dashboards work on-premise and which only work in cloud as they are not labeled as such. This is not a big issue, but it's a bit annoying. The bigger issue is that all the dashboards feel sluggish and slow to load. Several of us have used Grafana before, so we're used to a snappy and responsive UI. In our opinion, Google Cloud Monitoring feels clunky in comparison.  So the cost and the user experience were the main reasons we decided to look at alternatives to Google Cloud Monitoring. We ended up using Grafana and LGTM, which we'll talk about next.  ","version":null,"tagName":"h3"},{"title":"Grafana with the LGTM stackâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#grafana-with-the-lgtm-stack","content":"   When we realized that our needs were not entirely met by Google Cloud Monitoring, we started a project to develop a monitoring stack that would meet our needs. Since Grafana is open source and has a large community, we decided to use that as our frontend. Our backend is the LGTM stack, which is a set of open source tools that are designed to work well together for ingesting, storing and querying logs, traces and metrics.  What we noticed immediately was that the product teams were much more engaged with this stack than they were with Google Cloud Monitoring. Previously they would not really look at the dashboards, but now they are using them and even creating their own. This is a huge win for us, as we want the teams to be engaged with the monitoring and observability of their services.  It definitely helps that most developers on the product teams are familiar with Grafana, which makes it easier for them to get started as the learning curve is not as steep.  There was a discussion about what the backend should be, if we should useGrafana Cloud or host it ourselves. There would be a lot of benefits of using the cloud, as we would not have to maintain the stack or worry about performance or storage. There was, however, a concern about cost and whether or not log files could be shipped to a cloud provider. In the end we decided to host it ourselves, mostly because we didn't have control over what quantities of data we're processing. Now that we have a better understanding of our usage we can use that to calculate our spend, so we're not ruling out migrating to Grafana Cloud in the future.  The collection (scraping) of data is done by Grafana Agent, which is an &quot;all-in-one&quot; agent that collects metrics, logs and traces. This means a few less moving parts for the stack, as we don't have to run both Prometheus,Fluent Bit and someOpenTelemetry compatible agent for traces. It's a relatively new project, but it's already relative stable and has a lot of features. It uses a funky format for configuration called river, which is based on Hashicorp's HCL. The config enables forming pipelines to process data before it's forwarded to Loki, Tempo or Mimir. It's a bit different, but it works well and is easy to understand and configure to our needs.    Using a system like Grafana also enables us to build an integrated experience that also includes alerting. Using Grafana alerting and OnCall, we configure alerts that are sent to the correct team based on the service that is failing. This helps the teams get a better overview of what is happening in their services, and also helps us as a platform team to not have to be involved in every alert that is triggered.  Overall we're very happy with the LGTM stack, even though it's a fair bit of work to maintain the stack (especially with Istio and other security measures). We're also happy with Grafana, and we're looking forward to seeing what the future holds for monitoring and observability in Kubernetes.  ","version":null,"tagName":"h3"},{"title":"Summaryâ€‹","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#summary","content":" To summarize: We like Anthos, and we think it's a great platform for running hybrid Kubernetes. As a platform team we look at each feature on a case-by-case basis, with the goal of giving our developers the best possible experience instead of naively trying to use as much as possible of the platform. Because of this we've decided to use Anthos for Kubernetes and service mesh, but not for config sync and monitoring. This has given us a great platform that we're confident will serve us well for years to come.  Stay tuned for the third and final part of this series, where we'll talk about the benefits we've seen from Anthos, and what we would have done differently if we were to start over.  Disclaimer - Google, GKE and Anthos are trademarks of Google LLC and this website is not endorsed by or affiliated with Google in any way. ","version":null,"tagName":"h2"},{"title":"SKIP on Plattformpodden!","type":0,"sectionRef":"#","url":"/blog/skip-on-plattformpodden","content":"Very recently, SKIP was featured on thePlattformpodden podcast! Vegar and Eline were invited to talk about SKIP, how it came to be and what it's like to work on it. Give it a listen! https://plattformpodden.no/episode/6","keywords":"","version":null},{"title":"Intro til SKIP","type":0,"sectionRef":"#","url":"/docs","content":"","keywords":"","version":"Next"},{"title":"Velkommen til SKIP! ðŸŽ‰â€‹","type":1,"pageTitle":"Intro til SKIP","url":"/docs#velkommen-til-skip-","content":" SKIP stÃ¥r for Statens Kartverks Infrastrukturplattform. SKIP uttales som det norske ordet skip, et stÃ¸rre sjÃ¸gÃ¥ende fartÃ¸y.  SKIP-teamet jobber med en utviklingsplattform hvor kjernekomponentene er Kubernetes, Google Cloud, Argo CD og GitHub.  Hensikten er Ã¥ ha en helhetlig plattform for moderne utvikling hvor utviklere enkelt skal kunne lage, teste og kjÃ¸re containerbaserte applikasjoner basert pÃ¥ Cloud Native-prinsipper pÃ¥ en enkel og sikker mÃ¥te.  Under finner du en presentasjon som gir en introduksjon til SKIP. Presentasjonen er laget av Eline Henriksen, som er en av utviklerne bak SKIP. Trykk pÃ¥ pilene for Ã¥ gÃ¥ gjennom presentasjonen.   ","version":"Next","tagName":"h2"},{"title":"SKIP has a tech blog!","type":0,"sectionRef":"#","url":"/blog/welcome","content":"SKIP is starting a tech blog! ðŸš€ Or call it a newsletter if you're tired of blogs ðŸ¤ª Our first entry is already out, and it's about why we chose Anthos for hybrid cloud. We're working on more entries into that series and other exciting topics, so stay tuned! SKIP is Statens Kartverks Infrastruktur Plattform, or in English, the Infrastructure Platform of the Norwegian Mapping Authority. We're the platform team at Kartverket. We tame Kubernetes and the Cloud. With SKIP, developers in Kartverket are empowered to run, not walk, using a comprehensive toolbox of modern cloud technology. Using SKIP, developers can deploy applications to Kubernetes in a matter of minutes, while still being able to use the tools they know and love. Like what you see? We're a small team, but we're growing fast. We're also hiring, so if you're interested in working with us, check out our open positions.","keywords":"","version":null},{"title":"ArgoCD Notifications","type":0,"sectionRef":"#","url":"/docs/argo-cd/argocd-notifications","content":"","keywords":"","version":"Next"},{"title":"Slackâ€‹","type":1,"pageTitle":"ArgoCD Notifications","url":"/docs/argo-cd/argocd-notifications#slack","content":" For Slack er det satt opp en notifikasjonskanal for hvert team pÃ¥ mÃ¸nster &lt;teamnavn&gt;-argocd-alerts, f.eks. #nrl-argocd-alerts. Disse kanalene er videre satt opp med integration mot Slack-appen â€œArgoCD Notificationsâ€ som tar imot meldinger fra ArgoCD og dytter de inn i korrekt kanal.  (NB: Hvis du ikke finner en slik kanal for teamet ditt, kontakt en administrator for Kartverkets Slack og be om Ã¥ fÃ¥ opprettet en kanal med korrekt navnemÃ¸nster og integrasjon mot â€œArgoCD Notificationsâ€).    ","version":"Next","tagName":"h3"},{"title":"Githubâ€‹","type":1,"pageTitle":"ArgoCD Notifications","url":"/docs/argo-cd/argocd-notifications#github","content":" For Github er det satt opp en app kalt â€œKV ArgoCD Notificationsâ€ som har mulighet til Ã¥ skrive til Github workflow statuser til de forskjellige apps-repoene. Kontakt en av Kartverkets Github-administratorer dersom flere apps-repoer skal legges til her.  Eksempler pÃ¥ notifikasjoner:  ","version":"Next","tagName":"h3"},{"title":"Standardnotifikasjonerâ€‹","type":1,"pageTitle":"ArgoCD Notifications","url":"/docs/argo-cd/argocd-notifications#standardnotifikasjoner","content":" FÃ¸lgende triggers er lagt til som standard for alle apps-repoer:  Trigger\tKommunikasjonskanal\tNÃ¥r trigges denne?notifications.argoproj.io/subscribe.on-sync-failed.slack\tSlack\tSynkronisering av applikasjon feilet notifications.argoproj.io/subscribe.on-sync-failed.github\tGithub\tSynkronisering av applikasjon feilet notifications.argoproj.io/subscribe.on-sync-succeeded.github\tGithub\tSynkronisering av applikasjon gikk bra notifications.argoproj.io/subscribe.on-sync-running.github\tGithub\tSynkronisering av applikasjon kjÃ¸rer notifications.argoproj.io/subscribe.on-health-degraded.github\tGithub\tHelsesjekk av applikasjonen returnerer et â€œdegradedâ€-resultat notifications.argoproj.io/subscribe.on-sync-status-unknown.github\tGithub\tUkjent synkroniseringsstatus notifications.argoproj.io/subscribe.on-deployed.github\tGithub\tNy versjon av applikasjonen deployet til miljÃ¸ notifications.argoproj.io/subscribe.on-outofsync-one-day.slack\tSlack\tApplikasjonen har status OutOfSync i minst en dag (det har blitt sjekket inn endringer i apps-repoet som ikke har blitt deployet) notifications.argoproj.io/subscribe.on-outofsync-one-week.slack\tSlack\tApplikasjonen har status OutOfSync i minst en uke (det har blitt sjekket inn endringer i apps-repoet som ikke har blitt deployet)  ","version":"Next","tagName":"h3"},{"title":"Ekstra triggersâ€‹","type":1,"pageTitle":"ArgoCD Notifications","url":"/docs/argo-cd/argocd-notifications#ekstra-triggers","content":" I tillegg er det mulig Ã¥ spesifisere andre triggers (sÃ¥ lenge disse er lagt inn i ArgoCD) per team i objektet triggerSubscriptions i https://github.com/kartverket/skip-apps/blob/main/lib/argocd/argocd.libsonnet .  info Husk Ã¥ spesifisere om det er slack eller github notifikasjon man Ã¸nsker ved Ã¥ legge til suffikset .slack eller .github pÃ¥ slutten av trigger, og husk Ã¥ spesifisere kanalnavn ved bruk av slack notifikasjon  { name: 'teamnavn', oidcGroup: 'aabbbcc-123-321-ccbbbaa', allowlistedPrefixes: [{ name: 'teamnavn' }], triggerSubscriptions: { 'notifications.argoproj.io/subscribe.on-sync-succeeded.slack': 'navn-paa-slack-kanal', 'notifications.argoproj.io/subscribe.eksempel-trigger.github': '', # denne er blank siden det ikke er en kanal Ã¥ sende til pÃ¥ github } },  ","version":"Next","tagName":"h3"},{"title":"Configuring apps repositories with config.json","type":0,"sectionRef":"#","url":"/docs/argo-cd/configuring-apps-repositories-with-configjson","content":"","keywords":"","version":"Next"},{"title":"Supported optionsâ€‹","type":1,"pageTitle":"Configuring apps repositories with config.json","url":"/docs/argo-cd/configuring-apps-repositories-with-configjson#supported-options","content":" Key\tType\tDescriptiontool (required)\tdirectory / kustomize / helm\tWhich tool should Argo CD use to sync this directory? The â€œDirectoryâ€ option supports yaml and jsonnet files. See also tools . autoSync\tboolean ( true / false )\tWhen set to true , the directory is automatically synced when changes are detected. The default value is true in dev and false in prod. prune\tboolean ( true / false )\tWhen enabled, Argo CD will automatically remove resouces that are no longer present in Git. Default is true . See prune . Only used when autoSync is true allowEmpty\tboolean ( true / false )\tSafety mechanism. When prune is enabled it deletes resources automatically, but it will not allow empty syncs (delete all) unless allowEmpty also is enabled. Default is false . See allowEmpty . Only used when autoSync is true selfHeal\tboolean ( true / false )\tWhen changes are made on the cluster directly, Argo will not revert them unless selfHeal is provided. Default is true . See self heal . Only used when autoSync is true ","version":"Next","tagName":"h2"},{"title":"ðŸš€ Argo CD","type":0,"sectionRef":"#","url":"/docs/argo-cd","content":"","keywords":"","version":"Next"},{"title":"Lenker til Argoâ€‹","type":1,"pageTitle":"ðŸš€ Argo CD","url":"/docs/argo-cd#lenker-til-argo","content":" Dev (argocd.dev.skip.statkart.no)Test (argocd.test.skip.statkart.no)Prod (argocd.prod.skip.statkart.no)  ","version":"Next","tagName":"h2"},{"title":"GitOpsâ€‹","type":1,"pageTitle":"ðŸš€ Argo CD","url":"/docs/argo-cd#gitops","content":" Argo CD er et GitOps-verktÃ¸y, det vil si at kilden til sannhet ligger i git og synkes inn i clusteret derfra. GitOps er beskrevet i bildet over og er en â€œPull-basertâ€ deployment-flyt kontra den tradisjonelle â€œPush-baserteâ€ deployment-flyten. En operator kjÃ¸rer i clusteret og overvÃ¥ker kontinuerlig ett eller flere git-repoet og synker yaml-filer inn i clusteret. PÃ¥ den mÃ¥ten kan produktteam forholde seg til noe sÃ¥ enkelt som filer i en mappe i git, og nÃ¥r disse filene endres gjÃ¸res en deploy helt automatisk.  GitOps vil gi mange fordeler, men det blir et paradigmeskifte for mange. Istedenfor Ã¥ tenke â€œPushâ€-basert deploy ved Ã¥ kjÃ¸re et skript for Ã¥ deploye vil man legge inn Ã¸nsket state i en fil og sÃ¥ vil systemet jobbe for Ã¥ bringe clusteret i synk med Ã¸nsket state. Denne overgangen kan ogsÃ¥ sammenlignes litt med imperativ vs. deklarativ programmering, som jQuery vs. React. For de fleste som har jobbet med Kubernetes vil det fÃ¸les veldig kjent, siden Kubernetes i praksis er en stor reconciliation loop som kontinuerlig driver clusteret mot Ã¸nsket state.  Det er mange fordeler med et slikt deployment-system. NÃ¥r deployment og CI er to distinktive komponenter i systemet blir deployment-systemet mye mer spisset inn mot sin rolle og vil kunne perfeksjonere den, den sÃ¥kalte â€œDo one thing and do it wellâ€-tankegangen.  I de neste sidene skal vi beskrive hvordan Argo CD fungerer og hvordan dere kan bruke det til Ã¥ deploye til SKIP. ","version":"Next","tagName":"h2"},{"title":"Hente hemmeligheter fra hemmelighetshvelv","type":0,"sectionRef":"#","url":"/docs/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv","content":"","keywords":"","version":"Next"},{"title":"Hvordan bruke External Secretsâ€‹","type":1,"pageTitle":"Hente hemmeligheter fra hemmelighetshvelv","url":"/docs/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv#hvordan-bruke-external-secrets","content":" ESO lytter i clusteret etter ExternalSecret - og SecretStore -manifester. I det Ã¸yeblikket disse blir plukket opp blir de lest som konfigurasjon for ESO og en synk mot hvelvet starter som vil ende opp med Ã¥ opprette en Kubernetes Secret. Kubernetes Secreten vil ogsÃ¥ synkroniseres regelmessig slik at man kan f.eks. rullere hemmeligheter ved Ã¥ endre dem i hvelvet.  ","version":"Next","tagName":"h2"},{"title":"SecretStoreâ€‹","type":1,"pageTitle":"Hente hemmeligheter fra hemmelighetshvelv","url":"/docs/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv#secretstore","content":" SecretStore-manifestet definerer et hvelv, slik som Vault eller GSM, og mÃ¥ settes opp fÃ¸rst. Denne konfigurasjonen vil ogsÃ¥ inneholde hvordan ESO skal autentisere seg og kan gjenbrukes av flere ExternalSecret-manifester. Disse settes typisk opp av et produktteam for deres namespace for Ã¥ definere hvor de har lagret sine hemmeligheter.  Se GCPSMProvider for alle gyldige verdier.  apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: gsm spec: provider: gcpsm: projectID: &lt;YOUR_PROJECT_ID&gt;   For at det skal vÃ¦re lov Ã¥ hente ut secrets mÃ¥ i tillegg fÃ¸lgende gjÃ¸res:  Man mÃ¥ gÃ¥ inn pÃ¥ secreten som skal eksponeres til ESO og gi rollen roles/secretmanager.secretAccessor til servicekontoen: Dev - eso-secret-accessor@skip-dev-7d22.iam.gserviceaccount.comTest - eso-secret-accessor@skip-test-b6e5.iam.gserviceaccount.comProd - eso-secret-accessor@skip-prod-bda1.iam.gserviceaccount.com Namespacene dere oppretter mÃ¥ allowlistes for Ã¥ kunne hente ut fra prosjektene deres, kontakt SKIP sÃ¥ setter vi skip.kartverket.no/gcpProject pÃ¥ prosjektene deres og synkroniserer Argo pÃ¥ nytt  ","version":"Next","tagName":"h3"},{"title":"ExternalSecretâ€‹","type":1,"pageTitle":"Hente hemmeligheter fra hemmelighetshvelv","url":"/docs/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv#externalsecret","content":" NÃ¥r man har definert et hemmelighetshvelv med SecretStore kan man definere hvilke hemmeligheter som skal hentes ut. Dette gjÃ¸res med ExternalSecret-manifestet. ExternalSecret-manifestet vil referere til et SecretStore for Ã¥ definere backenden og bruker autentiseringen derfra. ESO vil bruke dette manifestet til Ã¥ hente ut de definerte feltene fra den gitte hemmeligheten og putte dem inn i en Kubernetes Secret i det formatet som blir spesifisert. Det betyr at man kan mappe om verdier fra et felt til et annet, for eksempel om man skal uppercase navnene nÃ¥r man bruke dem som miljÃ¸variabler.  I eksempelet under vises hvordan man synker inn enkeltverdier til Kubernetes. Det er ogsÃ¥ mulig Ã¥ synke alle nÃ¸klene i en secret som dokumentert i All keys, One secret .  Det er ogsÃ¥ mulig Ã¥ bruke templates som dokumentert i Advanced Templating .  Se ExternalSecret for alle gyldige verdier.  apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: dbpass spec: # A list of the remote secrets to sync data: - remoteRef: # The name of the secret in the GCP project key: db-pass # Will be written into the Kubernetes secret under this key secretKey: DB_PASSWORD # Refresh the secret every hour refreshInterval: 1h # Uses the gsm secret backend secretStoreRef: kind: SecretStore name: gsm # Creates a kubernetes secret named dbpass target: name: dbpass   Se ogsÃ¥ Get all keys from one GSM secret  ","version":"Next","tagName":"h3"},{"title":"Mounting av hemmelighetâ€‹","type":1,"pageTitle":"Hente hemmeligheter fra hemmelighetshvelv","url":"/docs/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv#mounting-av-hemmelighet","content":" NÃ¥r ESO har synkronisert inn hemmeligheten og opprettet en Kubernetes Secret er det ofte slik at man Ã¸nsker Ã¥ bruke dette i en Pod. Vanligvis gjennom Ã¥ mounte dette som miljÃ¸variabler eller som en fil pÃ¥ filsystemet, eksempelvis for sertfikater. Bruker man Skiperator er dette veldig rett frem.  Se ogsÃ¥ Using Secrets as files from a Pod og Using Secrets as environment variables , men merk at spec er annerledes med Skiperator.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: teamname-frontend spec: # Each key will be set as an env var with its value as the value envFrom: - secret: dbpass # Each key will be created as a file with the key as filename and value as content filesFrom: - secret: dbpass mountPath: /var/run/secret   ","version":"Next","tagName":"h3"},{"title":"Hva hindrer andre Ã¥ hente min hemmelighet?â€‹","type":1,"pageTitle":"Hente hemmeligheter fra hemmelighetshvelv","url":"/docs/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv#hva-hindrer-andre-Ã¥-hente-min-hemmelighet","content":" Med External Secrets gis en sentral servicekonto tilgang til Ã¥ hente ut hemmelighetene i GSM. Man skulle derfor tro at det var mulig for andre som bruker den samme servicekontoen Ã¥ hente ut hemmeligheten. Det er ikke tilfellet og er lÃ¸st med andre policies i clusteret.  Ditt team oppretter en SecretStore, og det finnes policies i clusteret som sÃ¸rger for at kun prosjekter som dere eier kan knyttes opp her. SecretStore-en er det som brukes for Ã¥ hente fra GCP. Dermed er det kun prosjektet som ligger her som kan hentes fra, og kun ditt team som kan hente fra ditt prosjekt. ","version":"Next","tagName":"h3"},{"title":"Hva er et apps-repo","type":0,"sectionRef":"#","url":"/docs/argo-cd/hva-er-et-apps-repo","content":"","keywords":"","version":"Next"},{"title":"Mappestrukturâ€‹","type":1,"pageTitle":"Hva er et apps-repo","url":"/docs/argo-cd/hva-er-et-apps-repo#mappestruktur","content":" Du vil se at et apps-repo har en predefinert mappestruktur. Den ser omtrent slik ut:  dev/ # 1 foo-main/ # 2 app.yaml # 3   PÃ¥ toppnivÃ¥ (1) finner man mapper som gjenspeiler hvilket miljÃ¸ det skal synkroniseres til. Dette er enten dev, test eller prod.  PÃ¥ nivÃ¥ 2 finner man navnet pÃ¥ namespacet som det skal deployes til. Dette mÃ¥ starte med et gitt prefiks, vanligvis produktnavnet (i dette tilfellet heter produktet foo ). Etter prefikset kan man skrive hva man vil, vanligvis navnet pÃ¥ branchen i git som er deployed her. Dette kan vÃ¦re nyttig om man Ã¸nsker Ã¥ deploye en mer stabil main branch deployed i tillegg til Ã¥ deploye pull requests som testes live fÃ¸r de merges.  NivÃ¥ 3, altsÃ¥ innholdet av mappen over, er et sett med en eller flere manifestfiler som beskriver applikasjonen. I eksempelet over vil app.yaml inneholde en Skiperator Application manifest som for eksempel kan se slik ut:  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: foo-frontend spec: image: kartverket/example port: 8080     NÃ¥r vi putter hele dette eksemplet sammen vil fÃ¸lgende skje:  Produktteamet gjÃ¸r en endring i apps-repoetArgo CD vil etter kort tid lese apps-repoet og finne den endrede app.yaml filenArgo CD ser at den er plassert i dev og foo-main mappene og oppretter foo-main namespacet pÃ¥ dev-clusteretArgo CD legger Application definisjonen inn i namespacet pÃ¥ KubernetesSkiperator plukker opp endringen i namespacet og bygger ut Kubernetes-definisjonen for en applikasjon som skal kjÃ¸re kartverket/example imagetKubernetes puller container imaget og starter podder som kjÃ¸rer applikasjonen  ","version":"Next","tagName":"h2"},{"title":"Gjenbruke konfigurasjonâ€‹","type":1,"pageTitle":"Hva er et apps-repo","url":"/docs/argo-cd/hva-er-et-apps-repo#gjenbruke-konfigurasjon","content":" Man vil ofte fÃ¥ gjentagende konfigurasjon nÃ¥r man fÃ¥r flere applikasjoner, namespacer og miljÃ¸er. Det finnes metoder i Argo CD for Ã¥ gjÃ¸re konfigurasjonen gjenbrukbar, og du vil finne dokumentasjon om disse pÃ¥ Argo CD Tools .  Flere produktteam har lÃ¸st gjenbruk ved Ã¥ bruke http://jsonnet.org/ som er stÃ¸ttet ut av boksen med Argo. Man kan se et eksempel av dette pÃ¥ eiet-apps . SKIP jobber med et bibliotek med gjenbrukbare jsonnet-objekter .  Vi pÃ¥ SKIP anbefaler at dere starter med Ã¥ sjekke inn vanlige YAML-filer mens dere lÃ¦rer dere systemet. NÃ¥r dere blir komfortable med Argo kan dere se pÃ¥ alternativene som er beskrevet over, da blir ikke lÃ¦ringskurven brattere enn nÃ¸dvendig.  ","version":"Next","tagName":"h2"},{"title":"Kildekode-repoerâ€‹","type":1,"pageTitle":"Hva er et apps-repo","url":"/docs/argo-cd/hva-er-et-apps-repo#kildekode-repoer","content":" Apps-repoer skal ikke inneholde kildekode. Apps-repoer har kun metadata om applikasjonen i form av manifest-filer. Dette kan man ogsÃ¥ lese om i Best Practices for Argo CD.  Dette gjÃ¸r at man fÃ¥r et tydelig skille mellom kildekoderepoer og apps-repoer. Kildekoderepoer har ansvaret for Ã¥ lagre kode, bygge artefakter og container-imager. Apps-repoer beskriver den Ã¸nskede staten til applikasjonen pÃ¥ clusteret og Argo jobber mot Ã¥ bringe clusteret i synk med denne staten. Dette gjÃ¸r det ogsÃ¥ enkelt Ã¥ forholde seg til apps-repoene som en â€œsingle source of truthâ€ til applikasjonsstaten pÃ¥ clusteret.  ","version":"Next","tagName":"h2"},{"title":"Deploye automatisk ved pushâ€‹","type":1,"pageTitle":"Hva er et apps-repo","url":"/docs/argo-cd/hva-er-et-apps-repo#deploye-automatisk-ved-push","content":"   Man Ã¸nsker ofte Ã¥ deploye ut nye versjoner av applikasjoner ved push til kildekoderepoer. Hvordan kan man gjÃ¸re dette med Argo CD?  Ved hvert push til et kildekoderepo kjÃ¸res et bygg for Ã¥ bygge et byggartefakt og bygge et container image. SÃ¥ snart dette imaget er pushet til et registry som ghcr.io vil man at dette skal legges ut pÃ¥ clusteret, og da mÃ¥ man oppdatere manifest-filene i apps-repoet. Man kan oppdatere disse filene manuelt for Ã¥ trigge en synk, men det er ogsÃ¥ mulig Ã¥ gjÃ¸re dette automatisk som en del av samme pipeline.  Etter imaget er publisert til ghcr.io puller bygget apps-repoet ved Ã¥ bruke https://github.com/actions/checkout. Deretter endres filene til Ã¥ inneholde referansen til det nye imaget, og disse filene commites lokalt. Hvordan disse filene endres er opp til produktteamet, men et forslag ligger i Automation from CI Pipelines. Til slutt pushes filene til repoet som vil trigge en synk med de oppdaterte manifestene.  Dette kan ogsÃ¥ gjÃ¸res med en PR istedenfor Ã¥ pushe rett til apps-repoet om man vil ha en godkjenning fÃ¸r deploy.  For Ã¥ logge inn pÃ¥ apps-repoet brukes metoden som beskrives i Tilgang til repoer med tokens fra GitHub Actions.  info Dersom man bruker Argo CD til Ã¥ opprette namespacer for alle branches og pull requests er det viktig Ã¥ slette branchene nÃ¥r de ikke lenger er i bruk. Det er begrenset med kapasitet pÃ¥ clusterene og Ã¥ anskaffe hardware, bÃ¥de on-prem og i sky, er ekstremt kostbart. Det holder Ã¥ slette filene i apps-repoet for Ã¥ rydde opp, noe som kan gjÃ¸res automatisk ved sletting av branches.  ","version":"Next","tagName":"h2"},{"title":"Eksempel pÃ¥ Github Actionsâ€‹","type":1,"pageTitle":"Hva er et apps-repo","url":"/docs/argo-cd/hva-er-et-apps-repo#eksempel-pÃ¥-github-actions","content":" name: build-and-deploy on: pull_requests: target: - main workflow_dispatch: push: branches: - main env: prefix: prefix jobs: build: # Her bygges et artefakt og et container image pushes til ghcr.io deploy-argo: needs: build runs-on: ubuntu-latest strategy: matrix: env: ['dev', 'test', 'prod'] steps: - uses: octo-sts/action@6177b4481c00308b3839969c3eca88c96a91775f # v1.0.0 id: octo-sts with: scope: kartverket/example-apps identity: example_name - name: Checkout apps repo uses: actions/checkout@v3 with: repository: kartverket/example-apps token: ${{ steps.octo-sts.outputs.token }} - name: Deploy to ${{ matrix.version }} run: | namespace=&quot;${{ env.prefix }}-${{ github.ref_name }}&quot; mkdir -p ./${{ matrix.version }}/$namespace cp -r templates/frontend.yaml ./${{ matrix.version }}/$namespace/frontend.yaml kubectl patch --local \\ -f ./${{ matrix.version }}/$namespace/frontend.yaml \\ -p '{&quot;spec&quot;:{&quot;image&quot;:&quot;${{needs.build.outputs.new_tag}}&quot;}}' \\ -o yaml git config --global user.email &quot;noreply@kartverket.no&quot; git config --global user.name &quot;GitHub Actions&quot; git commit -am &quot;Deploy ${{ matrix.version }} version ${{github.ref_name}}&quot; git push   name: clean-up-deploy on: delete: env: prefix: prefix jobs: delete-deployment: runs-on: ubuntu-latest strategy: matrix: env: ['dev', 'test', 'prod'] steps: - uses: octo-sts/action@6177b4481c00308b3839969c3eca88c96a91775f # v1.0.0 id: octo-sts with: scope: kartverket/example-apps identity: example_name - name: Checkout apps repo uses: actions/checkout@v3 with: repository: kartverket/example-apps token: ${{ steps.octo-sts.outputs.token }} - name: Delete ${{ matrix.version }} deploy run: | namespace=&quot;${{ env.prefix }}-${{ github.ref_name }}&quot; rm -rfv ./${{ matrix.version }}/$namespace git config --global user.email &quot;noreply@kartverket.no&quot; git config --global user.name &quot;GitHub Actions&quot; git commit -am &quot;Delete ${{ matrix.version }} deploy ${{github.ref_name}}&quot; git push  ","version":"Next","tagName":"h3"},{"title":"Scaling with Argo CD: Introducing the Apps Repo Architecture","type":0,"sectionRef":"#","url":"/blog/introducing-apps-repositories","content":"","keywords":"","version":null},{"title":"Multi-tenancy in Argo CDâ€‹","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#multi-tenancy-in-argo-cd","content":" So you've deployed Argo CD on your multi-tenant cluster and given your teams access to the user interface. Let's imagine we now have tens of teams and hundreds of applications in the Argo UI. When we start scaling out to more than a handful of users we get into some issues with scale. Examples of these issues can be:  How do you organize your apps and projects?How do you make sure no two teams accidentally (or maliciously) use the same namespace?How can we make sure teams clean up unused deployment resources?How do you seamlessly deploy to multiple clusters?  As a platform team we often find ourselves thinking that everyone loves infrastructure and Kubernetes as much as we do. This is not the case! Most people have not had the joy of having their childhood ruined by installing Linux on their school laptops and configuring WLAN drivers using ndiswrapper. Believe it or not, most people just want tools to get out of their way and let them do their job, be that programming, testing or anything else. Not every team is going to be experts in Kubernetes and Argo. So should we expect all teams to know what a deletion finalizer is? What about the intricacies of serverside apply vs. clientside apply?  It's our responsibility as a platform team to make the user experience of deploying to Kubernetes as user friendly as possible. After implementing an architecture built with UX in mind we've had the joy of seeing people who are extremely skeptical of Kubernetes and the cloud be won over by how easy it is to get your workloads running on Kubernetes. This is thanks to the consistent user experience and built-in best practices of the apps-repo architecture. But we're getting ahead of ourselves, first we need to talk about a few abstractions that make this possible.  ","version":null,"tagName":"h2"},{"title":"What are ApplicationSets?â€‹","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#what-are-applicationsets","content":" In Argo CD there's an advanced feature that allows for automating creation of Argo CD Applications calledApplicationSets. Using an ApplicationSet we can essentially make a template that generates Argo CD applications based on files or folders in a Git repository, sort of like a ReplicaSet for Pods. Using ApplicationSets we can build in features and assumptions and provide the teams with a user experience that essentially boils down to &quot;add a file to a repo and it gets deployed to the cluster&quot;. The purest form of GitOps. No messing around with Argo CD applications and projects.  A core Argo CD component called the ApplicationSet controller will detect anyApplicationSet resources deployed to the cluster and read them. After this, it will periodically scan the a repo configured in the ApplicationSet resource and generate Application resources, which in turn scan a repo for manifest files and sync them to the cluster. So in other words: ApplicationSet -&gt;Application -&gt; Deployments  For this to work you need a Git repo containing manifest files. You could have the teams put these manifest files into their source code repositories, but this is not considered best practice. Usually you would put your manifests into a separate repo so that changes to the manifests don't conflict with changes in the source code. At Kartverket we call this manifest repo an apps repo.  ","version":null,"tagName":"h2"},{"title":"Introducing apps repositoriesâ€‹","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#introducing-apps-repositories","content":"   The apps repo is where the product teams put their manifests. It has a consistent structure and is designed to be read by an Argo CD ApplicationSet. It also has a lot of nifty features that enable self-service which we'll get back to.  First, let's have a look at the structure of an apps repo.  teamname-apps/ env/ clustername/ namespace/ example.yaml   In the simplest of terms, this tree describes where to deploy a given manifest. By using a directory tree it makes setting up an ApplicationSet for this repo trivial.  Consider this example ApplicationSet:  apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: exampleteam-apps namespace: argocd spec: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD goTemplate: true goTemplateOptions: - missingkey=error template: metadata: name: '{{.path.basename}}' spec: destination: namespace: '{{ index .path.segments 2 }}' name: '{{ index .path.segments 1 }}' project: exampleteam source: path: '{{.path.path}}' repoURL: 'https://github.com/kartverket/exampleteam-apps.git' targetRevision: HEAD syncPolicy: syncOptions: - CreateNamespace=true automated: prune: true allowEmpty: true selfHeal: true   With this ApplicationSet any directory within env/*/* will be picked up by the ApplicationSet controller and a new Argo CD Application will be created based on the template in the template object. This enables a product team to create any number of applications for their products.    An example use for this is a product team wanting a namespace for each of their products. Instead of having to order a new namespace from the platform team when they create a new product, they can simply create it themselves by adding a new directory with the same name as the namespace they want. A new Kubernetes namespace will be automatically created thanks to theCreateNamespace=true sync option.  Ephemeral namespaces, aka. preview namespaces, is another usecase. Say a team wants to review a change before merging it to main. They could review the change in the Pull Request, but this removes us from the end user's perspective and is not suitable for non-technical people. With a preview environment the team will automatically create a new directory in the apps repo when a PR is created, and thus get a complete deployment with the change in question. This enables end-to-end testing in a browser, and also allows non-technical people to do QA before a change is merged. When it is merged another workflow can automatically delete the directory, which cleans up and deletes the preview environment.  Our convention is that namespaces are formatted with productname-branch. This allows teams to have multiple deploys per product, and also multiple products per team. So when a new PR is created all a team needs to do to automate the creation of a new directory using CI tools like GitHub actions to create a new commit in the apps-repo. This also enables the flexibility to create it as a PR in the apps-repo, but for ephemeral namespaces, this is usually not necessary.  For example:  footeam-apps/ env/ foo-cluster/ foo-main/ app.yaml foo-feature-123/ app.yaml   ","version":null,"tagName":"h2"},{"title":"Automating and avoiding duplicationâ€‹","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#automating-and-avoiding-duplication","content":" Depending on the complexity of the apps repo, the amount of products and branches and a subjective &quot;ickyness&quot; with duplicating files (can you spell DRY?), you have several options on how to automate creating new namespaces.  Simple repos will probably be fine with directories containing simple yaml-files that are synced to the cluster. Newer product teams especially appreciate the simplicity of this approach. To optimize for this you may consider using atemplate directory at the base containing some example files that are copied into the sub-directories. A pseudo-coded GitHub action that uses afrontend.yaml template from the templates directory could look like the following:  jobs: build: # Build a container image and push it deploy: strategy: matrix: env: ['dev', 'test', 'prod'] steps: # .. Checkout repo &amp; other setup .. - name: Deploy to ${{ matrix.version }} run: | namespace=&quot;myapp-${{ github.ref_name }}&quot; path=&quot;./env/atkv3-${{ matrix.env }}/$namespace&quot; mkdir -p $path cp -r templates/frontend.yaml $path/frontend.yaml kubectl patch --local \\ -f $path/frontend.yaml \\ -p '{&quot;spec&quot;:{&quot;image&quot;:&quot;${{needs.build.outputs.container_image_tag}}&quot;}}' \\ -o yaml git config --global user.email &quot;github-actions@github.com&quot; git config --global user.name &quot;GitHub Actions&quot; git commit -am &quot;Deploy ${{ matrix.env }} version ${{ github.ref_name }}&quot; git push   This works for most simple apps. Our experience, however, is that as a team matures and gets more experienced with Kubernetes and Argo CD, they add more complexity and want more control. At this point most teams will migrate to usingjsonnet to enable referencing and extending a reusable library shared between multiple components. SKIP also provides some common manifests via ArgoKit, a jsonnet library.  Kustomize is also a common choice, widely used by SKIP for our own infrastructure, but not really widespread with other teams.  Despite Argo supporting Helm we mostly avoid using it to create reusable templates due to the complexity of templating YAML. Jsonnet is superior in this regard.  Fixing indentation errors in YAML templates in a Helm chart pic.twitter.com/Dv2JUkCdiM â€” memenetes (@memenetes) December 8, 2022  ","version":null,"tagName":"h2"},{"title":"Security considerationsâ€‹","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#security-considerations","content":" You may be wondering: &quot;This seems great and all, but what about the security implications of allowing teams to create and edit namespaces in a multi-tenant cluster? That seems really dangerous!&quot;.  First of all, I love you for thinking about security. We need more people like you. Second, Argo CD has some great features we can leverage to make this work without removing the self-service nature of the apps repo architecture.  ","version":null,"tagName":"h2"},{"title":"Prefixesâ€‹","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#prefixes","content":" In order to make this work we need to give each team a set of prefixes. A prefix will usually be the name of a product that a product team has responsibility for maintaining. The only important part is that it is unique and that no other teams have been allocated the same prefix. At Kartverket this is done by the platform team as part of the team onboarding process.  The prefix is used as part of all namespaces that are created by the teams. In the example namespace product-feature-123, product is the prefix. By giving each team a set of prefixes it helps them separate products into easily identifiable namespaces and it ensures that a product team does not accidentally use another team's namespace.  Since each product team has an apps repo with the ability to name their directories as they wish, how can we enforce this? This is where Argo CD's Projects come into play.  Argo CD Projectsprovide a logical grouping of applications, which is useful when Argo CD is used by multiple teams. It also contains a field that allows allowlisting which clusters and namespaces are usable by a project.  Add the following to a Project to only allow this project to create and sync to namespaces prefixed with myprefix-.  metadata: name: exampleteam spec: destinations: - namespace: 'myprefix-*' server: '*'   If you scroll back up to the ApplicationSet example above, you will see that it only creates applications with the project exampleteam. This will automatically wire any applications created to the destination rules we've defined in this project and therefore deny any attempts by a team to use prefixes that they have not been allocated.  The crucial part here is that ApplicationSets and Projects are provisioned by the platform team, and therefore build in these security features. These resources must not be accessible to the teams, or an attacker can simply add exclusions.  ","version":null,"tagName":"h3"},{"title":"Namespace resourcesâ€‹","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#namespace-resources","content":" Another way this could be abused is if a team is able to create Namespace resources in their apps repository. This should be denied using Argo and/or cluster policies.  If a team is able to create namespace resources (or other cluster scoped resources) in their namespace an attacker can use this to break their namespace &quot;encapsulation&quot;. Imagine for example if one could use their apps repo to sync a namespace resource named kube-system into their env/foo-cluster/foo-maindirectory. Argo CD would allow this, as the manifests are read into an Argo CD application. Then the attacker could delete the namespace and take down the cluster.    It's useful in this multi-tenancy scenario to think of namespaces as resources owned by the platform team and namespace-scoped resources as owned by the product teams. This is considered a best practice, and was reiterated at KubeCon Europe 2024 by Marco De Benedictis. Allowing product teams to edit namespaces can open up a ton of attack vectors, like disabling Pod Security Admissioncontrollers, allowing an attacker to create privileged containers which can compromise the host node.  Friends don't let friends edit namespaces!  ","version":null,"tagName":"h3"},{"title":"Self service customizationâ€‹","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#self-service-customization","content":" So we set up an ApplicationSet that configures best practices and secure defaults for product teams! Great! But now that team with experienced cloud engineers really wants to customize their Argo configuration. Maybe they want to configure that one app has auto sync on, but another app has it turned off. Maybe they want to disable self-healing for a short period to manually edit in the cluster. In any case, how can we let teams change this configuration self-service when applications are provisioned by theApplicationSet resource?  We could let the teams edit the ApplicationSet. In our case this would mean the teams need to learn about the ApplicationSet abstraction, gotemplate and SKIP's internal GitOps repo structure. This is overkill when a team usually just wants to flip a flag between true or false for a directory. There could also be security implications with allowing teams to edit ApplicationSet resources that could break encapsulation, which we want to avoid.  Another option would be to contact the platform team and tell us to change some config for them. This is not in line with our thinking, as we want the teams to be able to work autonomously for most operations like this. It would also mean we were given a lot of menial tasks which would mean we have less time to do other more meaningful things or become a bottleneck for the teams.  A third option is setting the ApplicationSet sync policy to create-only. This would confifure the ApplicationSet controller to create Application resources, but prevent any further modification, such as deletion, or modification of Application fields. This would allow a team to edit the application in the UI after creation, for example disabling auto sync. This last option is user friendly, but in violation of GitOps principles where config lives in git and not in a database. If you run Argo stateless like we do this would also mean the changes disappear when the pod restarts.  Because none of these options seemed to be the best, we created a better solution. By using a combination of generators and the new template patchfeature in Argo CD 2.8 we can look through every directory in the apps repo for a configuration file called config.json.  Let's look at an example config.json file. This example file is commited in the apps repo to the env/foo-cluster/foo-main directory.  { &quot;tool&quot;: &quot;kustomize&quot;, &quot;autoSync&quot;: false }   This file is not required, but if this file is found the values configured there overrides a set of default values in the ApplicationSet template. These flags are then used to determine how the resulting Application will behave. This means the team is able to change the values they care about per directory of their apps repo  footeam-apps/ env/ foo-cluster/ foo-main/ config.json app.yaml foo-feature-123/ config.json app.yaml foo-feature-with-default-config/ app.yaml   Additionaly, since the platform team is in control of the template we can eliminate the ability to maliciously change the template by parsing the inputs in a secure way.  ","version":null,"tagName":"h2"},{"title":"Example ApplicationSetâ€‹","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#example-applicationset","content":" Let's look at how we can write an ApplicationSet that allows us to useconfig.json files.  First, we need to configure the ApplicationSet to look through all directories, and at the same time use a config.json file if it is found. This is perhaps the least intuitive part of this new ApplicationSet, so let's walk through it step by step.  First we create a merge generator, which will merge two generators. The key thing here is that it only merges if the key matches in both generators, so this allows us to first find all directories (the default), then directories that contain config.json files (the override).   generators: - merge: generators: - # default - # override mergeKeys: - key   Now we're going to add the generator from before into the default. The only difference is we're doing this using a matrix generator. Doing this combines the parameters generated by the two child generators, which gives us the values from the git generator like before, but also a set of default values we can use in our template later if the config.json file is not provided.  We're also using a value from the git generator to assign a key that will uniquely identify this directory for the merge generator later.   generators: - merge: generators: - matrix: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory - # override mergeKeys: - key   Now we use a variant of the git generator to find all config.json files in the same repo and extract the values from it. Again we're using the key field to uniquely identify this directory so that it will be merged with the correct directory in the merge generator.  We're repeating the default values here as well, since not all fields are required and we don't want them to be overwritten as null in the resulting merge.   generators: - merge: generators: - matrix: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory - matrix: generators: - git: files: - path: env/*/*/config.json repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory mergeKeys: - key   That's it for the generator! Now we can use these variables in thetemplatePatch field (and other fields). In this case we want to set syncPolicy options, so we need to use the templatePatch, as gotemplates don't work for objects.  We're also adding a special case where for directory sources (the default) we exclude config.json files, as we don't want to sync the config file with Argo. This allows us to extend it later to add options for other tools like Kustomize or Helm.  Keep in mind that we don't want users to inject maliciously formed patches, so we cast booleans to booleans.   templatePatch: | spec: source: directory: {{- if eq .tool &quot;directory&quot; }} exclude: config.json {{- end }} {{- if .autoSync }} syncPolicy: automated: allowEmpty: {{ .allowEmpty | toJson }} prune: {{ .prune | toJson }} selfHeal: {{ .selfHeal | toJson }} {{- end }}   ","version":null,"tagName":"h3"},{"title":"Complete ApplicationSetâ€‹","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#complete-applicationset","content":" Here is a complete ApplicationSet containing all the features we've discussed so far.  apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: exampleteam-apps namespace: argocd spec: generators: - merge: generators: - matrix: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory - matrix: generators: - git: files: - path: env/*/*/config.json repoURL: https://github.com/kartverket/exampleteam-apps.git revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory mergeKeys: - key goTemplate: true goTemplateOptions: - missingkey=error template: metadata: name: '{{.path.basenameNormalized}}' spec: destination: namespace: '{{ index .path.segments 2 }}' name: '{{ index .path.segments 1 }}' project: exampleteam source: path: '{{.path.path}}' repoURL: 'https://github.com/kartverket/exampleteam-apps.git' targetRevision: HEAD syncPolicy: managedNamespaceMetadata: labels: app.kubernetes.io/managed-by: argocd pod-security.kubernetes.io/audit: restricted team: exampleteam syncOptions: - CreateNamespace=true - ServerSideApply=true - PrunePropagationPolicy=background templatePatch: | spec: source: directory: {{- if eq .tool &quot;directory&quot; }} exclude: config.json {{- end }} {{- if .autoSync }} syncPolicy: automated: allowEmpty: {{ .allowEmpty | toJson }} prune: {{ .prune | toJson }} selfHeal: {{ .selfHeal | toJson }} {{- end }}   ","version":null,"tagName":"h2"},{"title":"Resultsâ€‹","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#results","content":" With Argo CD and the apps repo architecture, we've seen some real improvements in our deploy system. Teams find it to be incredibly intuitive to just update a file in Git and have it be instantly reflected in Argo CD and Kubernetes, especially when combined with Argo CD auto-sync.  Onboarding new teams is quick and easy, since just putting files into a Git repo is something most developers are already familiar with. We just show them the structure of the apps repo and they're good to go. A team can go from not having any experience with Kubernetes to deploying their first application in a matter of minutes.  Migrating from one cluster to another is also a breeze. Just move manifests from one directory under env to another, and the ApplicationSet will take care of the rest. This is especially useful for teams that want to start developing with new cloud native principles on-premises, modernizing the application and eventually moving to the cloud.  I feel the key part of this architecture is the config.json file. It allows a degree of customization that is not possible with the default ApplicationSettemplate and was to us the last missing piece. It allows teams to change configuration without needing to know about the ApplicationSet abstraction, and it allows the platform team to enforce security and best practices.  ","version":null,"tagName":"h2"},{"title":"Tradeoffsâ€‹","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#tradeoffs","content":" But of course, there are some drawbacks. Like always, it's tradeoffs all the way down.  Since a product team uses an apps repo to organize their apps, moving apps from one team to another will require migrating files from one repo to another. This will require some manual work to prevent Argo deleting the entire namespace when the directory is removed from the old repo. Usually this is not a big issue, and moving projects between teams happens very rarely, but it's something to keep in mind.  There is also a risk that a team could accidentally delete a namespace by removing a directory in the apps repo. We have mitigated this by disabling auto-sync for most mission critical applications in production.  And finally, projects that don't have clear ownership or shared ownership can be tricky to place into a repo. You could make an apps repo for a &quot;pseudo-team&quot; consisting of the teams that need access, but generally we find that it's better that all products have a clear singular main owner. This also preventsdiffusion of responsibility.  ","version":null,"tagName":"h3"},{"title":"Thank you for reading!â€‹","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#thank-you-for-reading","content":" We hope you found this article helpful and informative. Getting intoApplicationSets can be a bit tricky, so we hope we managed to convey the most important parts in a clear and understandable way. Thanks for reading!  We recently created a Mastodon account @kv_plattform! If you want to contact us or discuss this article, feel free to reach out to us there. ","version":null,"tagName":"h2"},{"title":"Provisjonere infrastruktur med Crossplane","type":0,"sectionRef":"#","url":"/docs/argo-cd/provisjonere-infrastruktur-med-crossplane","content":"","keywords":"","version":"Next"},{"title":"Hvordan komme i gangâ€‹","type":1,"pageTitle":"Provisjonere infrastruktur med Crossplane","url":"/docs/argo-cd/provisjonere-infrastruktur-med-crossplane#hvordan-komme-i-gang","content":" La oss si vi har en applikasjon som er deployed med Argo CD og vi Ã¸nsker Ã¥ sette opp en database for denne applikasjonen med Cloud SQL. Da vil vi ha en mappestruktur i vÃ¥rt apps-repo som ser slik ut:  dev/ namespace/ app.yaml # Skiperator-manifest for applikasjonen db.yaml # Crossplane-manifester for databasen pÃ¥ GCP   Det fÃ¸rste steget er Ã¥ fÃ¥ autentisert mot GCP slik at Crossplane fÃ¥r tilgang til Ã¥ opprette ressurser i prosjektet deres. Dette gjÃ¸res ved Ã¥ kontakte SKIP og fÃ¥ lagt inn mapping for prefikset deres i skip-apps .  Deretter kan man opprette ressurser som er stÃ¸ttet av SKIP dokumentert lenger ned. Crossplane stÃ¸tter mye mer, se CRD-er i GCP provideren , men det mÃ¥ lages stÃ¸tte for disse, se â€œTilgang til ressurserâ€.  For Ã¥ provisjonere opp ressurser oppretter produktteamet manifester pÃ¥ Kubernetes som blir lest av Crossplane. Et eksempel pÃ¥ Ã¥ opprette lagring (bucket).  apiVersion: skip.kartverket.no/v1alpha1 kind: BucketInstance metadata: name: my-bucket spec: parameters: bucket: name: dsa-test-bucket-123 serviceAccount: name: crossplane-test displayName: Testing Crossplane Integration   Etter dette er lagt ut vil man kunne se status pÃ¥ crossplane ressursene som et hvilket som helst annen kubernetes-ressurs.  $ kubectl get bucketinstance   Man kan ogsÃ¥ bruke kubectl describe for Ã¥ hente ut events pÃ¥ disse ressursene. Events sier mer om hva som skjer og er nyttig til feilsÃ¸king.  Mer om feilsÃ¸king finnes pÃ¥ https://docs.crossplane.io/knowledge-base/guides/troubleshoot/ .  ","version":"Next","tagName":"h2"},{"title":"StÃ¸ttede ressurserâ€‹","type":1,"pageTitle":"Provisjonere infrastruktur med Crossplane","url":"/docs/argo-cd/provisjonere-infrastruktur-med-crossplane#stÃ¸ttede-ressurser","content":" FÃ¸lgende ressurser er stÃ¸ttet for Ã¥ provisjoneres med Crossplane i dag:  Buckets (Lagring i Google Cloud Storage)GCP Service AccountsBucket Access (Kubernetes SA to Bucket)Workload Identity (Kubernetes SA to GCP SA)  ","version":"Next","tagName":"h2"},{"title":"Oppsettâ€‹","type":1,"pageTitle":"Provisjonere infrastruktur med Crossplane","url":"/docs/argo-cd/provisjonere-infrastruktur-med-crossplane#oppsett","content":" For Ã¥ komme i gang med Crossplane mÃ¥ du gjÃ¸re noe setup. Alle produktteam fÃ¥r automatisk opprettet en servicekonto pÃ¥ GCP som vil brukes av Crossplane til Ã¥ autentisere mot GCP, og for at Crossplane skal fÃ¥ brukt denne mÃ¥ det ligge en secret i namespacet deres. For Ã¥ fÃ¥ inn denne kan dere opprette en secret ved hjelp av en ExternalSecret (se Hente hemmeligheter fra hemmelighetshvelv) som kopierer hemmeligheten fra Google Secret Manager inn i Kubernetes. Dette mÃ¥ dere sette opp for hvert prefiks i &lt;prefix&gt;-main mappen deres i apps-repoet:  apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: crossplane-secret spec: refreshInterval: 1h secretStoreRef: name: gsm kind: SecretStore target: name: crossplane-secret data: - secretKey: creds remoteRef: conversionStrategy: Default decodingStrategy: None key: crossplane-credentials metadataPolicy: None   SKIP setter automatisk opp en ProviderConfig nÃ¥r man fÃ¥r knyttet sitt prefix i Argo CD mot GCP. Denne forutsetter en secret i -main namespacet deres som heter crossplane-secret . Hvis ikke denne secreten blir plukket opp sÃ¥ hÃ¸r med SKIP om knytningen til GCP mangler.  For Ã¸vrig mÃ¥ vi bruke JSON keys for GCP service kontoer her siden crossplane stÃ¸tter ikke Workload Identity on-prem.  ","version":"Next","tagName":"h2"},{"title":"Tilgang til ressurserâ€‹","type":1,"pageTitle":"Provisjonere infrastruktur med Crossplane","url":"/docs/argo-cd/provisjonere-infrastruktur-med-crossplane#tilgang-til-ressurser","content":" I utgangspunktet kan ikke produktteamene fÃ¥ tilgang til crossplane CRD-er direkte ettersom disse ikke er namespaced-ressurser og produktteamene kun har tilgang til Ã¥ opprette ressurser i sitt eget namespace. Dette betyr at SKIP mÃ¥ opprette sÃ¥kalte â€œCompositionsâ€ for hver ting som produktteamene skal kunne opprette gjennom Crossplane.  Dersom du som utvikler pÃ¥ et produktteam har et Ã¸nske om Ã¥ f.eks. kunne opprette en database eller provisjonere andre ressurser gjennom Crossplane som ikke allerede er stÃ¸ttet mÃ¥ det bestilles en ny Composition fra SKIP.  For at SKIP skal opprette en ny composition mÃ¥ det lages en XRD og en composition .  Se stÃ¸ttede ressurser over. ","version":"Next","tagName":"h2"},{"title":"Hvordan bruke Argo CD","type":0,"sectionRef":"#","url":"/docs/argo-cd/hvordan-bruke-argocd","content":"","keywords":"","version":"Next"},{"title":"Applikasjonerâ€‹","type":1,"pageTitle":"Hvordan bruke Argo CD","url":"/docs/argo-cd/hvordan-bruke-argocd#applikasjoner","content":" Det fÃ¸rste man gjÃ¸r nÃ¥r man skal ta i bruk Argo er Ã¥ gÃ¥ til nettsiden og logge inn. Lenkene til nettsiden finner man pÃ¥ Argo CD og alle kan logge inn med kartverket-brukeren sin hvis man er pÃ¥ et team som har fulgt Komme i gang med Argo CD.    Det neste som mÃ¸ter deg er en oversikt over applikasjonene som Argo leser ut, avbildet over. Dersom man ikke sere noen applikasjoner her, sjekk om dere har fulgt alle stegene i Komme i gang med Argo CD og at dere har manifester som er satt opp til Ã¥ bli synket inn fra apps-repoet deres. Disse prosjektene blir automatisk opprettet basert pÃ¥ mappestrukturen i apps-repoet deres, sÃ¥ det er ingen behov for Ã¥ opprette eller rydde opp prosjekter manuelt.  Klikk pÃ¥ et av kortene pÃ¥ denne siden og dere vil gÃ¥ inn i en mer detaljert visning hvor man ser alle ressursene som blir synkronisert.    Dersom man bruker Skiperator og eksponererer en URL via ingresses vil man ogsÃ¥ kunne se smÃ¥ ikoner som er lenker og om man klikker pÃ¥ dem Ã¥pnes applikasjonen i nettleseren.  Det er ogsÃ¥ et sett med filtere pÃ¥ venstre side som er lurt Ã¥ bli kjent med, spesielt dersom applikasjonene blir store og vanskelige Ã¥ se pÃ¥ en skjerm uten Ã¥ scrolle.  ","version":"Next","tagName":"h2"},{"title":"Syncâ€‹","type":1,"pageTitle":"Hvordan bruke Argo CD","url":"/docs/argo-cd/hvordan-bruke-argocd#sync","content":"   info Merk at i dev synkroniseres applikasjoner automatisk  PÃ¥ prosjektsiden ser man alle kubernetes-ressurser som er en del av applikasjonen. Legg merke til de smÃ¥ fargede symbolene pÃ¥ hvert kort som sier noe om statusen pÃ¥ ressursen. Hvis de er grÃ¸nne viser det at den ressursen er â€œhealthyâ€. Dersom den er rÃ¸d er det et tegn pÃ¥ at noe er galt med ressursen. Dersom den er gul er den â€œute av synkâ€, og da mÃ¥ man synkronisere applikasjonen.  Bildet over viser hvordan man kan synkronisere ut endringene til kubernetes-miljÃ¸et. Sync-knappen i menylinjen lar deg velge hvordan ting skal synkroniseres ut, og man kan til og med gjÃ¸re en Selective Sync av kun noen av ressursene. Det vanligste og tryggeste er vel Ã¥ merke Ã¥ synkronisere alt med default-innstillingene.  Dersom en synk ikke har fungert vil man se en feilmelding i menylinjen Ã¸verst. I det tilfellet kan det vÃ¦re lurt Ã¥ trykke pÃ¥ â€œsync statusâ€-knappen Ã¸verst for Ã¥ fÃ¥ en mer detaljert oversikt over hva som har gÃ¥tt galt.  ","version":"Next","tagName":"h2"},{"title":"Rollbackâ€‹","type":1,"pageTitle":"Hvordan bruke Argo CD","url":"/docs/argo-cd/hvordan-bruke-argocd#rollback","content":" I noen tilfeller kan man tenke seg at en uÃ¸nsket endring er kommet ut i kjÃ¸remiljÃ¸et. Da vil den raskeste og enkleste mÃ¥ten Ã¥ gjenopprette funksjonaliteten for brukerene ofte vÃ¦re en rollback til en tidligere kjent fungerende versjon.  Rollbacks er det innebygget stÃ¸tte for i Argo CD som en del av applikasjonsvisningen. Klikk â€œHistory and rollbackâ€ for Ã¥ fÃ¥ en liste over alle tidligere synker som er gjort i denne applikasjonen. Dersom man Ã¸nsker Ã¥ rulle tilbake finner man versjonen man Ã¸nsker i listen og trykker pÃ¥ de tre prikkene og velger rollback. â€œRevisjoneneâ€ i listen peker pÃ¥ en commit i git-historikken til apps-repoet.  Ved en rollback gjÃ¸r Argo CD en synk som vanlig, men mot en tidligere kjent tilstand. Den vil da ikke bruke tilstanden som ligger i git, men tilstanden til en tidligere synk. Etter en rollback vil applikasjonen stÃ¥ som â€œout of syncâ€, og det er forventet siden den ikke matcher tilstanden i git.  info Husk at container imaget mÃ¥ finnes for at det skal vÃ¦re mulig Ã¥ rulle tilbake. Om container imaget er slettet i ghcr.io , for eksempel av en oppryddingsjobb, sÃ¥ vil det ikke vÃ¦re mulig Ã¥ starte opp den tidligere versjonen.  ","version":"Next","tagName":"h2"},{"title":"Detaljer og Web Terminalâ€‹","type":1,"pageTitle":"Hvordan bruke Argo CD","url":"/docs/argo-cd/hvordan-bruke-argocd#detaljer-og-web-terminal","content":"   Dersom man klikker pÃ¥ en ressurs i prosjektvisningen vil man se flere detaljer om denne ressursen. Man finner blant annet en oversikt over metadata, manfiest-filen som Argo CD skal synke ut, events og logger.  Det er ogsÃ¥ mulig Ã¥ endre pÃ¥ manifestfilen som ligger i clusteret om man gÃ¥r pÃ¥ â€œlive manifestâ€ og trykker â€œeditâ€. Dette vil fÃ¸re til at applikasjonen kommer ut av synk, og i miljÃ¸er hvor auto-synking er skrudd pÃ¥ vil det tilbakestilles med en gang. Men i noen tilfeller kan det vÃ¦re nyttig.    Legg ogsÃ¥ merke til â€œterminalâ€-fanen. Denne er kun synlig om man velger en pod. Velger man denne fanen fÃ¥r man en live terminaltilkobling inn til podden som man kan bruke til feilsÃ¸king.  info Web terminal er ikke tilgjengelig i prod  ","version":"Next","tagName":"h2"},{"title":"Hvordan bruke Argo gjennom APIâ€‹","type":1,"pageTitle":"Hvordan bruke Argo CD","url":"/docs/argo-cd/hvordan-bruke-argocd#hvordan-bruke-argo-gjennom-api","content":" Visst du Ã¸nsker Ã¥ automatisere oppgaver, for eksempel synk ved ny image versjon sÃ¥ kan det vÃ¦re greit Ã¥ ha muligheten til Ã¥ gjÃ¸re dette fra Github. Det fÃ¸rste du trengre da er nettverkstilgang fra Github, det fÃ¥r du med tailscale.  For Ã¥ autentisere mot Argo sÃ¥ mÃ¥ du generere en JWT, dette kan du gjÃ¸re i Argo UIet. GÃ¥ inn pÃ¥ f.eks https://argo-dev.kartverket.dev, trykk pÃ¥ settings oppe til venstre â†’ Projects â†’ ditt prosjekt â†’ trykk pÃ¥ â€œRolesâ€ fanen, og deretter pÃ¥ apiuser. Scroll helt ned pÃ¥ modalen som kommer opp og trykk Create under JWT Tokens. Det er samme framgangsmÃ¥te i andre miljÃ¸.    Etter at token er generert kan du testen den med kommandoen:  curl https://argo-dev.kartverket.dev/api/v1/applications/&lt;min-app&gt; -H &quot;Content-Type: application/json&quot; -H &quot;Authorization: Bearer &lt;token&gt;&quot;   Argos API spec kan man finne her: https://argo-dev.kartverket.dev/swagger-ui ","version":"Next","tagName":"h2"},{"title":"Komme i gang med Argo CD","type":0,"sectionRef":"#","url":"/docs/argo-cd/komme-i-gang-med-argocd","content":"","keywords":"","version":"Next"},{"title":"Sjekklisteâ€‹","type":1,"pageTitle":"Komme i gang med Argo CD","url":"/docs/argo-cd/komme-i-gang-med-argocd#sjekkliste","content":" For Ã¥ starte med Argo CD mÃ¥ du gjÃ¸re fÃ¸lgende:  SÃ¸rg for at teamet ditt oppfyller Hva skal til for Ã¥ bruke Kompass?Produktteamet deres mÃ¥ ha en team-gruppe i Azure AD Samle en liste med alle teammedlemmerVelg to av teammedlemmene som skal ha hÃ¸yere tilganger, for eksempel tech lead og team leadSend denne listen til produkteier SKIP som bestiller opprettelse av CLOUD_SK_TEAM-gruppePass pÃ¥ at team-gruppen legges inn i Enterprise Applicationen til ArgoCD for alle relevante miljÃ¸ Det mÃ¥ settes opp et apps-repo Les Hva er et apps-repo for Ã¥ forstÃ¥ hvordan apps-repoer fungererRepoet opprettes fra apps-template malenGitHub teamet deres mÃ¥ gis tilgang til apps-repoet som adminSKIP mÃ¥ gi Argo CD-appen pÃ¥ GitHub tilgang slik at Argo kan pulle apps-repoet, dette gjÃ¸res gjennom Github IAC repoet Det bestemmes et â€œprefiksâ€ som dere deployer til Vanligvis er dette navnet pÃ¥ applikasjonen som skal deploye til SKIPDere kan administrere alle Kubernetes namespacer som starter med dette prefikset SKIP mÃ¥ konfigurere Argo til Ã¥ lese og synkronisere fra apps-repoet SKIP gjÃ¸r en endring i skip-apps repoet NÃ¥ skal du kunne logge inn pÃ¥ Argo CD og se applikasjonen din! ðŸš€ Du finner lenker til Argo pÃ¥ Argo CDVidere dokumentasjon finnes pÃ¥ Hvordan bruke Argo CD ","version":"Next","tagName":"h2"},{"title":"Tilgang til GCP","type":0,"sectionRef":"#","url":"/docs/gcp/access","content":"Tilgang til GCP SKIP benytter Google Cloud Platform som Ã¸kosystem rundt Kubernetes/Anthos. Det gjÃ¸r at man kan benytte seg av andre Google-produkter selv om applikasjonen kjÃ¸rer pÃ¥ et on-premise cluster. Man kan ogsÃ¥ autentisere seg mot GCP og benytte kubectl gjennom Google sin Connect Gateway for Ã¥ aksessere on-premise cluster uten Ã¥ vÃ¦re pÃ¥ det interne nettverket/VDI. For Ã¥ kunne logge pÃ¥ GCP med Kartverket-brukeren mÃ¥ brukeren vÃ¦re medlem i en CLOUD_SK_TEAM AD-gruppe. Vi anbefaler at leads (produkteier, team lead, tech lead) pÃ¥ teamet sender inn en ticket til PureService, eksempelvis med fÃ¸lgende informasjon: Hei! Kan dere legge til fÃ¸lgende medlemmer i AD-gruppen CLOUD_SK_TEAM_Eiet? Navn NavnesenKari Nordmann Hilsen Navnesen Navnemann CLOUD_SK_TEAM AD-gruppen mÃ¥ ogsÃ¥ vÃ¦re synket inn i GCP. Dette kan ta opp til en time, selv etter du har fÃ¥tt bekreftelse pÃ¥ at medlemmet har blitt lagt inn i AD-gruppen.","keywords":"","version":"Next"},{"title":"Oppsett og bruk av Google Secret Manager","type":0,"sectionRef":"#","url":"/docs/gcp/oppsett-og-bruk-av-secret-manager","content":"","keywords":"","version":"Next"},{"title":"Hvordan komme i gang?â€‹","type":1,"pageTitle":"Oppsett og bruk av Google Secret Manager","url":"/docs/gcp/oppsett-og-bruk-av-secret-manager#hvordan-komme-i-gang","content":" GSM fungerer ganske likt Vault. Vault har noe mer funksjonalitet for avansert bruk, men vi bruker for det meste som et KV secret store. For Ã¥ bruke GSM mÃ¥ det opprettes en secret, og denne secreten mÃ¥ tilgangsstyres.  ","version":"Next","tagName":"h2"},{"title":"Hvordan opprette Secretâ€‹","type":1,"pageTitle":"Oppsett og bruk av Google Secret Manager","url":"/docs/gcp/oppsett-og-bruk-av-secret-manager#hvordan-opprette-secret","content":"   Velg Security under navigasjonsmenyen (de tre strekene ved Google Cloud i hÃ¸yre hjÃ¸rne).Velg Secret Manager i venstre kolonne.Hvis APIâ€™et ikke er skrudd pÃ¥, skru pÃ¥ APIâ€™et ved Ã¥ trykke â€œEnableâ€Trykk pÃ¥ + CREATE SECRET    Name, og Value kan tenkes pÃ¥ som et Key/Value par. Resten av valgene trenger man ikke gjÃ¸re noe med med mindre man har spesielle behov. Noen felter man kan merke seg er:  Replication Policy: Dette er hvor hemmeligheten lagres. Det kan vÃ¦re en fordel Ã¥ lagre hemmeligheter i flere datacenter for redundans, vi har vanligvis holdt oss i europe-north1.  Encryption: Om det er spesielle behov for Ã¥ administrere krypteringsnÃ¸kkel selv er det ogsÃ¥ en mulighet. Dette mÃ¥ produktteamene ta ansvar for selv. SKIP teamet administrerer ikke krypteringsnÃ¸kler.  ","version":"Next","tagName":"h3"},{"title":"Tilgangsstyringâ€‹","type":1,"pageTitle":"Oppsett og bruk av Google Secret Manager","url":"/docs/gcp/oppsett-og-bruk-av-secret-manager#tilgangsstyring","content":" NÃ¥r en secret er opprettet, kan man klikke seg inn pÃ¥ den, og velge PERMISSIONS fanen. Man fÃ¥r da opp hvem som har tilgang til denne secreten, og hvilke rettigheter de har.    I de fleste tilfeller vil man bruke External Secret til Ã¥ hente ut disse hemmelighetene. Det kan gjÃ¸res ved Ã¥ opprette ExternalSecrets-ressurser i Kubernetes som henter ned hemmeligheten til en Kubernetes Secret. Det stÃ¥r mer om dette inkludert tilgangsstyring pÃ¥ Hente hemmeligheter fra hemmelighetshvelv . ","version":"Next","tagName":"h3"},{"title":"Dynamisk tilgangskontroll (JIT)","type":0,"sectionRef":"#","url":"/docs/gcp/jit","content":"Dynamisk tilgangskontroll (JIT) Most developers will at some point experience not having the correct permissions to operate on Google Cloud resources. This is intentional and is part of the principle of least privilege . In order to operate on the resources you want to access, you need to elevate your privileges. A system exists to make this operation self-service, and it is called Just-In-Time access. It can be accessed at https://jit.skip.kartverket.no . After logging in with your Kartverket google account, it will take you to the below screen. First step is filling in the ID of the project you wish to get access to. This can be found by searching in the box or by finding the ID from console.cloud.google.com. Second step, select the roles you want. It is often possible to see which role you need from the error message you got when trying to do an operation and getting denied. A common role that is used for administering secrets in Google Secret Manager is secretmanager.admin. Select a suitable duration using the slider and click continue. Note that some sensitive roles are not compatible with longer durations. Now for the final step, enter a reason for the access request. This is mostly for auditing, as generally speaking requests are granted automatically. The reason entered will be possible to see in the logs if we need to investigate a security breach. In less common cases, for example when restricted roles are to be granted, a manual approval is required. In that case the reason will be visible to the person who approves the request. When you click request access, you will be taken to a summary screen which gives you the result of your request. In the example above, my request was granted automatically. You now have access, and that's just in time!","keywords":"","version":"Next"},{"title":"ðŸš¢ Generelt","type":0,"sectionRef":"#","url":"/docs/generelt","content":"ðŸš¢ Generelt","keywords":"","version":"Next"},{"title":"Legge til eller fjerne personer fra et team","type":0,"sectionRef":"#","url":"/docs/generelt/add-remove-team-member","content":"","keywords":"","version":"Next"},{"title":"Mitt team Ã¸nsker tilgang til selvbetjente team-grupperâ€‹","type":1,"pageTitle":"Legge til eller fjerne personer fra et team","url":"/docs/generelt/add-remove-team-member#mitt-team-Ã¸nsker-tilgang-til-selvbetjente-team-grupper","content":" Dersom du ikke har tatt i bruk de nye gruppene enda, vil det kreves noe jobb for SKIP Ã¥ flytte tilganger fra de gamle CLOUD_SK-gruppene til TF - AAD - TEAM-gruppene. Dette er noe SKIP mÃ¥ gjÃ¸re, sÃ¥ ta kontakt med oss for Ã¥ gjennomfÃ¸re disse endringene.  Ta kontakt med SKIP i #gen-skip for Ã¥ fÃ¥ oppgradert til selvbetjente grupper.  FÃ¸rst mÃ¥ det bekreftes at gruppen som skal oppgraderes eksisterer. SÃ¸k etter den i Entra ID . Hvis den ikke finnes er ikke teamet onboardet pÃ¥ SKIP riktig og mÃ¥ legges inn i entra-id-config.  NÃ¥ mÃ¥ gruppene som gis tilgang i skip-core-infrastructure-repoet (tidligere IAM) byttes over fra CLOUD_SK-gruppene til TF - AAD-gruppene. Dette gjÃ¸res i teams-modulen. Stort sett er det bare Ã¥ endre fra en e-post til en annen slik at man ender opp med de nye aad-tf-gruppene. Dette vil vÃ¦re litt problematisk dersom teamet bruker locals.teams-abstraksjonen, sÃ¥ der bÃ¸r noe skrives om for Ã¥ stÃ¸tte TF - AAD-grupper i tillegg til de gamle â€œCLOUD_SKâ€-gruppene.  Etter IAM er oppdatert og kjÃ¸rt mÃ¥ gruppene som gis tilgang til Argo CD oppdateres. Dette gjÃ¸res i argocd.libsonnet. Finn UUID-en fra teamet i Entra ID og kopier Object ID inn. Deretter mÃ¥ argocd-apps synkes ut i alle miljÃ¸er.  Etter dette skal teamet vÃ¦re byttet over til det nye oppsettet. SpÃ¸r teamet om de fortsatt har tilgangene de forventer. Merk at det kan ta noe tid fÃ¸r tilgangene er ordentlig inne, sÃ¥ om det ikke funker med en gang, alt ser riktig ut og alt er kjÃ¸rt kan det lÃ¸nne seg Ã¥ prÃ¸ve igjen etter litt tid. ","version":"Next","tagName":"h2"},{"title":"â›… Google Cloud Platform (GCP)","type":0,"sectionRef":"#","url":"/docs/gcp","content":"â›… Google Cloud Platform (GCP) Under denne siden finner du artikler som omhandler oppsett og bruk av Google Cloud Platform.","keywords":"","version":"Next"},{"title":"Onboarding new product teams onto SKIP","type":0,"sectionRef":"#","url":"/docs/generelt/onboarding-new-teams","content":"","keywords":"","version":"Next"},{"title":"SKIP team tasksâ€‹","type":1,"pageTitle":"Onboarding new product teams onto SKIP","url":"/docs/generelt/onboarding-new-teams#skip-team-tasks","content":" ","version":"Next","tagName":"h2"},{"title":"Before onboardingâ€‹","type":1,"pageTitle":"Onboarding new product teams onto SKIP","url":"/docs/generelt/onboarding-new-teams#before-onboarding","content":" Invite a representative from the product team to the plattformlaug Dedicate a SKIP team member as a point of contact for the migration process (TAM) (Only for the migration process, after this is finished, a regular support flow is started) Invite to a meeting to clarify expectations between SKIP and the product team Invite to review applications Agree on the frequency of on-boarding standups with the product team and invite to these Ensure that a process is started around risk assessment (&quot;ROS-analyse&quot;). This assessment must be ready in time for production Create a channel on slack for collaboration during on-boarding Invite to #gen-skip, #gen-argo and other relevant common channels for using SKIP Invite to GCP and Kubernetes courses if the product team wants it Give an introduction to ArgoCD and best practices for this tool  ","version":"Next","tagName":"h3"},{"title":"During onboardingâ€‹","type":1,"pageTitle":"Onboarding new product teams onto SKIP","url":"/docs/generelt/onboarding-new-teams#during-onboarding","content":" Invite to a kickoff meeting where points of contact, distribution of responsibilities, support, roadmap and any other relevant issues are discussed.GitHub, given that the team has not used this beforeCreate groups by adding them to entra-id-configThe team need to be labeled with security in the admin.google.com . This can only be done through click-ops and only BÃ¥rd and Eline have access, sadly.The team needs to be added to the IAM repositoryWorkflow in the IAM repository needs to be run by a SKIP member with access to do this.The teams are synced from AD into IAMIf the team requires Terraform: Service account for Terraform is set up through gcp-service-accounts and are granted access to its Kubernetes namespace with WIF.Terraform state is migrated/set upThe team and app-repository is set up in accordance with Komme i gang med Argo CD  ","version":"Next","tagName":"h3"},{"title":"Product team tasksâ€‹","type":1,"pageTitle":"Onboarding new product teams onto SKIP","url":"/docs/generelt/onboarding-new-teams#product-team-tasks","content":" The product team is responsible for delegating tasks among themselves.  Inform SKIP who is the team lead so they can administer the AD group Consider which team members need extra Kubernetes/GCP courses If ArgoCD is going to be used: Create new Apps repository in GitHub based on this SKIP template Ensure the application completes an IP and/or DPIA Adapt the application in order to satisfy SKIP's security requirements Read, understand and follow the GitHub security requirements: Sikkerhet pÃ¥ GitHub Finish the risk assessment document (ROS-analyse) Prepare information for the SKIP team, including technical expectations and service design/architecture Take responsibility for your own requirements and communicate these clearly and concisely to SKIP Ensure all team members are invited to meetings and Slack groups during the on-boarding process Read and understand the SKIP documentation Make the expected/required go-live date known to SKIP ","version":"Next","tagName":"h2"},{"title":"Oversikt over tjenester SKIP tilbyr","type":0,"sectionRef":"#","url":"/docs/generelt/oversikt-over-tjenester-SKIP-tilbyr","content":"","keywords":"","version":"Next"},{"title":"Grafanaâ€‹","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/generelt/oversikt-over-tjenester-SKIP-tilbyr#grafana","content":" Grafana Loki er et logglagringsverktÃ¸y som brukes som datakilde for Grafana.  Grafana Mimir lagrer metrikker fra appliasjoner, og brukes som datakilde for Grafana.  Grafana Tempo lagrer tracing for applikasjoner, og brukes som datakilde for Grafana  Brukes for Ã¥ sende ut varslinger basert pÃ¥ data i grafana.  ","version":"Next","tagName":"h2"},{"title":"Google Secret Managerâ€‹","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/generelt/oversikt-over-tjenester-SKIP-tilbyr#google-secret-manager","content":" For hemmelighetshÃ¥ndtering anbefaler vi bruk av Google Secret Manager (GSM). Her har vi solid adgangskontroll og kan enkelt hente hemmeligheter bÃ¥de i build time og run time til applikasjoner vi kjÃ¸rer i Kubernetes.  I GSM opprettes hemmeligheter per prosjekt, og man kan adgangskontrollere bÃ¥de for et helt prosjekt og for individuelle hemmeligheter. Hemmeligheter kan versjoneres og rulleres automatisk.  Ved hjelp av et system kalt External Secrets er det enkelt Ã¥ hente disse hemmelighetene til build time. Se Hente hemmeligheter fra hemmelighetshvelv .  For Ã¥ hente hemmeligheter fra GSM under run time, se Autentisering mot GCP fra Applikasjon .  ","version":"Next","tagName":"h2"},{"title":"GitHubâ€‹","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/generelt/oversikt-over-tjenester-SKIP-tilbyr#github","content":" GitHub er en skybasert git-repository-tjeneste som vi bruker til Ã¥ lagre kildekoden til Kartverkets prosjekter. Med GitHub fÃ¥r vi ogsÃ¥ mye annet ogsÃ¥ som kontinuerlig integrasjon, kodescanning.  ","version":"Next","tagName":"h2"},{"title":"Objektlagringâ€‹","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/generelt/oversikt-over-tjenester-SKIP-tilbyr#objektlagring","content":" SKIP tilbyr flere objektlagringstjenester som blant annet gir deg mulighet Ã¥ lagre filer i sky eller on-prem.  For Ã¥ lagre filer i sky anbefaler vi Ã¥ benytte Google cloud storage . Dette er en lagringstjeneste som fÃ¸lger med Google Cloud Platform. Her kan du f. eks provisjonere bÃ¸tter via terraform, og laste opp filer til denne bÃ¸tten via en applikasjon pÃ¥ et Kubernetes cluster. Se Autentisering mot GCP fra Applikasjon for Ã¥ koble seg til GCP via en applikasjon.  I tillegg til lagring med Google cloud storage sÃ¥ har man mulighet til Ã¥ benytte Scality on-prem som er et AWS S3-kompatibel lÃ¸sning.  ","version":"Next","tagName":"h2"},{"title":"Continuous Deploymentâ€‹","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/generelt/oversikt-over-tjenester-SKIP-tilbyr#continuous-deployment","content":" ArgoCD er et deklarativt, GitOps-kontinuerlig leveranseverktÃ¸y for Kubernetes-applikasjoner. Det automatiserer distribusjon og administrasjon av applikasjoner i Kubernetes ved Ã¥ synkronisere den Ã¸nskede tilstanden som er definert i Git-repositorier med den faktiske cluster konfigurasjonen. ","version":"Next","tagName":"h2"},{"title":"Vedlikehold av applikasjoner","type":0,"sectionRef":"#","url":"/docs/generelt/maintenance-of-apps","content":"","keywords":"","version":"Next"},{"title":"Stoppe kjÃ¸rende applikasjon i ArgoCDâ€‹","type":1,"pageTitle":"Vedlikehold av applikasjoner","url":"/docs/generelt/maintenance-of-apps#stoppe-kjÃ¸rende-applikasjon-i-argocd","content":" For Ã¥ kunne stoppe en kjÃ¸rende applikasjon som er administrert av ArgoCD mÃ¥ man fÃ¸rst vÃ¦re sikker pÃ¥ at autosync/self heal er deaktivert for produktteamet som eier applikasjonen. Hvis ikke vil bare applikasjonen spinne opp igjen automatisk.  Se denne filen for Ã¥ sjekke hva som er status, eventuelt spÃ¸r noen pÃ¥ SKIP hvis du er usikker. Hvis ikke annet er satt kan du gÃ¥ ut i fra at autosync er skrudd pÃ¥ i dev og test, men avslÃ¥tt i prod.  For Ã¥ stoppe en applikasjon trykker du pÃ¥ menyen til en application-ressurs og velger â€œStopâ€. Dette vil midlertidig sette antall kopier til 0 slik at skiperator skalerer ned applikasjonen. Du vil da kunne se at pods forsvinner fra grensesnittet, og â€œSync Statusâ€ for applikasjonen vil stÃ¥ som â€œOutOfSyncâ€NÃ¥r man er ferdig med vedlikeholdet og Ã¸nsker Ã¥ gjennopprette tidligere konfigurasjon trenger man bare Ã¥ trykke â€œSyncâ€ for at applikasjonen skal spinne opp igjen.  ","version":"Next","tagName":"h2"},{"title":"Stoppe kjÃ¸rende applikasjon manueltâ€‹","type":1,"pageTitle":"Vedlikehold av applikasjoner","url":"/docs/generelt/maintenance-of-apps#stoppe-kjÃ¸rende-applikasjon-manuelt","content":" For Ã¥ stoppe en applikasjon som kjÃ¸rer pÃ¥ SKIP mÃ¥ man i praksis skalere ned antallet kjÃ¸rende kopier til 0. Den stÃ¸rste hindringen ved dette er en policy som vi hÃ¥ndhever i prod-miljÃ¸et, som heter â€œK8sReplicaLimitsâ€. Denne krever at en applikasjon skal ha mellom 2 og 30 kjÃ¸rende kopier til en hver tid.  For Ã¥ manuelt stoppe en skiperator-applikasjon er det to ting man mÃ¥ gjÃ¸re:  Sette en annotation for Ã¥ ignorere k8sReplicaLimits policySette antall replicas til 0  Se fÃ¸lgende eksempel pÃ¥ manifest som skalerer til 0  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: some-app annotations: skip.kartverket.no/k8sReplicaLimits: ignore spec: replicas: 0  ","version":"Next","tagName":"h2"},{"title":"Sjekkliste fÃ¸r internett-eksponering","type":0,"sectionRef":"#","url":"/docs/generelt/sjekkliste-fÃ¸r-internett-eksponering","content":"Sjekkliste fÃ¸r internett-eksponering info Denne siden er under utarbeidelse og er et samarbeid mellom utvikling og sikkerhet For Ã¥ eksponere en applikasjon som kjÃ¸rer pÃ¥ SKIP mot internett mÃ¥ man: Opprette en DNS-record som ikke er under statkart.no-domenet, f.eks. applikasjonX.kartverket.no . Det gjÃ¸res ved Ã¥ opprette en ticket i PureService og be om at dette domenet skal peke mot SKIP-lastbalansereren (lb01.kartverket.no)Legge til det nye domenenavnet under ingresses i Skiperator-manifestet eller hostname for Routing-manifestet , slik at applikasjonen registrerer seg mot ekstern ingress gateway FÃ¸r dette kan gjÃ¸res mÃ¥ man gÃ¥ igjennom denne sjekklisten: GjÃ¸r dere kjent med Overordnede fÃ¸ringer og spesielt Ansvarsfordeling fra SikkerhetshÃ¥ndboka Opprett metadata om applikasjonen i henhold til Sikkerhet i repoet . Dette gjÃ¸r at applikasjonen blir knyttet opp i Utviklerportalen (fortsatt under arbeid) Foranalyse mÃ¥ vÃ¦re gjennomfÃ¸rt(Kommer lÃ¸ype for det i ServiceNow) Det er gjort IP (Innledende Personvernsvurdering) og eventuelt DPIA. Lag en kopi av malen pÃ¥ IP, DPIA og ROS-analyse for [det som vurderes]. IKKE SKRIV INN I MALEN, men kopier sidene. ROS-analyse gjennomfÃ¸rt og godkjent av risikoeier/systemeier Codeowners definert i koderepo CODEOWNERS GjennomfÃ¸rt initiell penetrasjonstesting (hvem og hvordan?) eller manuell avsjekk med SKIP rundt konfigurasjon FÃ¸lgende headere blir sendt pÃ¥ alle kall: HTTP Strict Transport Security , Content Security Policy , X-Frame-Options , X-Content-Type-Options , Referrer Policy , Permissions Policy NÃ¥r appen er eksponert er sikkerhetsheaders testet med https://securityheaders.com og https://observatory.mozilla.org Monitorering og varsling er satt opp i Grafana, og vaktlaget er onboardet disse alarmene Metrics with Grafana Logs with Loki Alerting with Grafana Denne sjekklisten gjelder eksponering av tjenster som skal vÃ¦re tilgjengelig pÃ¥ internett, uavhengig av miljÃ¸ (dev/test/prod). Hvis man har planer om Ã¥ eksponere en applikasjon idevellertestmÃ¥ man i tillegg kontakte SKIP for Ã¥ sikre at alle sikkerhetskrav overholdes. Navnekonvensjon for eksternt tilgjengelig domenenavn vil i sÃ¥ fall vÃ¦re &lt;applikasjonX&gt;.dev.kartverket.no&lt;applikasjonX&gt;.test.kartverket.no","keywords":"","version":"Next"},{"title":"ðŸ—ƒï¸ GitHub","type":0,"sectionRef":"#","url":"/docs/github","content":"ðŸ—ƒï¸ GitHub Kartverket lagrer kildekode pÃ¥ github.com, og gjennom organisasjonen vÃ¥r distribuerer vi tilgang ved Ã¥ fordele lisensene vi har kjÃ¸pt inn. For Ã¥ fÃ¥ tilgang fÃ¸lg sjekklisten under. info Ved spÃ¸rsmÃ¥l vedrÃ¸rende tilgang eller behov for stÃ¸tte, ta kontakt pÃ¥ #gen-github pÃ¥ slack Kom igang ved Ã¥ sjekke ut lenkene under: Tilgang til GitHubAutentisering til GitHub i terminalen (git clone / push med SSH)Opprette nytt repo pÃ¥ GitHubGitHub Actions som CI/CDHÃ¥ndtering av sensitiv data som er kommet pÃ¥ repositorietBruk av GitHub med JenkinsTilgang til on-prem infrastruktur fra GitHub ActionsTilgang til repoer med tokens fra GitHub Actions","keywords":"","version":"Next"},{"title":"ðŸ§° GitHub Actions","type":0,"sectionRef":"#","url":"/docs/github-actions","content":"","keywords":"","version":"Next"},{"title":"Genereltâ€‹","type":1,"pageTitle":"ðŸ§° GitHub Actions","url":"/docs/github-actions#generelt","content":" GitHub actions er GitHubs CI/CD-system. Med dette systemet kan man kjÃ¸re bygg som er tett integrert med kodebasen og bruke et Ã¸kosystem av integrasjoner og ferdiglagde actions via GitHub Marketplace .  Dere kommer til Ã¥ mÃ¸te pÃ¥ en del forskjellige verktÃ¸y nÃ¥r dere skal deploye til SKIP:  SKIP er kjÃ¸remiljÃ¸et for containere i Kartverket. Vi regner ikke GitHub som en del av SKIP, men det er en sÃ¥ sentral komponent i Ã¥ deploye til SKIP-teamet er med Ã¥ drifte GitHub-organisasjonen til KartverketGitHub Actions som er CI/CD-miljÃ¸et for Ã¥ kjÃ¸re jobber som Ã¥ bygge containere fra kildekode og kjÃ¸re terraform plan og applyTerraform som er IaC -verktÃ¸yet som lar oss beskrive det Ã¸nskede miljÃ¸et i kode og eksekverer kommandoer for Ã¥ modifisere miljÃ¸et slik at det blir slik som beskrevetgithub-workflows som er gjenbrukbare jobber man kan bruke i sine pipelines for Ã¥ gjÃ¸re oppsettet lettere. Denne inneholder hovedsakelig den gjenbrukbare jobben â€œrun-terraformâ€. Denne kan benyttes for Ã¥ enkelt autentisere seg mot GCP og bruke terraform pÃ¥ en sikker mÃ¥te.Google Cloud og Google Anthos som er miljÃ¸et som kjÃ¸rer Kubernetes -miljÃ¸et hvor containerene kjÃ¸rerskiperator er en operator som gjÃ¸r det enklere Ã¥ sette opp en applikasjon som fÃ¸lger best practices. Skiperator definerer en Application custom resource som blir fylt ut av produktteamene og deployet med TerraformNacho SKIP signerer container images med en kryptografisk signatur etter de er bygget  GItHub Actions er et CI-systemet som SKIP legger opp til at alle produktteam skal kunne bruke for Ã¥ automatisere bygging av Docker-images i tillegg til muligheter for Ã¥ opprette infrastruktur i skyen ved hjelp av Terraform pÃ¥ en automatisert mÃ¥te.Actions lages ved Ã¥ skrive YAML-filer i .github/workflows -mappa i roten av repoet. Man kan ogsÃ¥ trykke pÃ¥ â€œActionsâ€ og â€œNew workflowâ€ i GitHub og fÃ¥ opp dialogen over. Der kan man velge fra et eksisterende bibliotek med eksempler pÃ¥ Actions som kan hjelpe med Ã¥ komme i gang med en action. For eksempel kan man trykke â€œView allâ€ pÃ¥ â€œContinous Integrationâ€ for Ã¥ finne eksempler pÃ¥ hvordan man bygger med java eller node.js. DIsse er ofte gode utgangspunkt nÃ¥r man skal sette opp et nytt bygg.  Les https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions for en introduksjon til Actions.  Se https://docs.github.com/en/actions/learn-github-actions/workflow-syntax-for-github-actions for referanse av mulige verdier.  ","version":"Next","tagName":"h2"},{"title":"Lagring av imagesâ€‹","type":1,"pageTitle":"ðŸ§° GitHub Actions","url":"/docs/github-actions#lagring-av-images","content":" Det anbefalte mÃ¥ten Ã¥ publisere images er nÃ¥ til GitHub Container Registry ( ghcr.io ). Dette kan gjÃ¸res enkelt ved hjelp av GitHub Actions.  Se denne artikkelen for mer informasjon om ghcr: https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry .  Eksempler for publisering av container images til GitHub finnes her .  Dersom dere bruker metoden over vil dere merke at dere ikke trenger Ã¥ sette tags pÃ¥ docker imaget dere bygger. Dette vil settes automatisk basert pÃ¥ en â€œsane defaultâ€ ut i fra hvilke branch man er pÃ¥ og hvilke kontekst bygget gjÃ¸res i (commit, PR, tag). De resulterende taggene er dokumentert her . Tags kan ogsÃ¥ tilpasses om ikke default er passende for prosjektet.  Resultatet blir Ã¥ finne pÃ¥ GitHub repositoriet til koden og ser slik ut:    ","version":"Next","tagName":"h2"},{"title":"Deploymentâ€‹","type":1,"pageTitle":"ðŸ§° GitHub Actions","url":"/docs/github-actions#deployment","content":" For deployment brukes Argo CD som det dedikert deployment-verktÃ¸y. Se Argo CD for mer informasjon om hvordan man tar i bruk dette.  Det vil finnes prosjekter som bruker Terraform, enten fordi de hadde oppstart fÃ¸r Argo CD eller fordi de har spesielle behov som tilsier at de trenger Terraform. Disse prosjektene kan se pÃ¥ Bruk av Terraform for videre dokumentasjon. For nye prosjekter anbefaler vi Argo CD. ","version":"Next","tagName":"h2"},{"title":"Autentisering med Workload Identity Federation","type":0,"sectionRef":"#","url":"/docs/github-actions/autentisering-med-workload-identity-federation","content":"","keywords":"","version":"Next"},{"title":"Oppsett av GitHub Actionâ€‹","type":1,"pageTitle":"Autentisering med Workload Identity Federation","url":"/docs/github-actions/autentisering-med-workload-identity-federation#oppsett-av-github-action","content":" NÃ¥r man skal sette opp autentisering mot GCP med Workload Identity Federation er det en fordel Ã¥ ha lest gjennom GitHub sin artikkel om https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/configuring-openid-connect-in-google-cloud-platform#updating-your-github-actions-workflow , og spesifikt kapittelet som heter â€œUpdating your GitHub Actions workflowâ€. Her beskriver de de to trinnene man mÃ¥ gjÃ¸re:  Konfigurere tilgang til Ã¥ generere ID-tokensBruke https://github.com/google-github-actions/auth actionen til Ã¥ autentisere mot GCP  SKIP-teamet vil ha konfigurert en workload identity provider og service account som dere kan putte rett inn i provideren over. Disse er ikke hemmelige men vil variere avhengig av miljÃ¸ man skal deploye mot, sÃ¥ det kan vÃ¦re hensiktsmessig Ã¥ ha de som variabler, som vist lenger nede.  permissions: contents: read id-token: write jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - id: auth name: Authenticate to GCP uses: google-github-actions/auth@v0 with: workload_identity_provider: projects/your-project-number/locations/global/workloadIdentityPools/your-pool/providers/your-provider service_account: your-account@your-project.iam.gserviceaccount.com project_id: kubernetes-dev-94b9   Eventuelt kan du ha en egen setup-env jobb som lager outputs du kan bruke senere, slik at provider, service account og project id er variabler i stedet for hardkodede strings.  Eksempel:  permissions: contents: read id-token: write env: PROJECT_ID: kubernetes-dev-94b9 SERVICE_ACCOUNT: your-account@your-project.iam.gserviceaccount.com WORKLOAD_IDENTITY_PROVIDER: projects/your-project-number/locations/global/workloadIdentityPools/your-pool/providers/your-provider jobs: setup-env: runs-on: ubuntu-latest outputs: project_id: ${{ steps.set-output.outputs.project_id }} service_account: ${{ steps.set-output.outputs.service_account }} workload_identity_provider: ${{ steps.set-output.outputs.workload_identity_provider }} steps: - name: Set outputs id: set-output run: | echo &quot;project_id=$PROJECT_ID&quot; &gt;&gt; $GITHUB_OUTPUT echo &quot;service_account=$SERVICE_ACCOUNT&quot; &gt;&gt; $GITHUB_OUTPUT echo &quot;workload_identity_provider=$WORKLOAD_IDENTITY_PROVIDER&quot; &gt;&gt; $GITHUB_OUTPUT build: needs: [setup_env] runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - id: auth name: Authenticate to GCP uses: google-github-actions/auth@v0 with: workload_identity_provider: ${{ needs.setup-env.outputs.workload_identity_provider }} service_account: ${{ needs.setup-env.outputs.service_account }} project_id: ${{ needs.setup-env.outputs.project_id }} build-again: needs: [setup_env] runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - id: auth name: Authenticate to GCP uses: google-github-actions/auth@v0 with: workload_identity_provider: ${{ needs.setup-env.outputs.workload_identity_provider }} service_account: ${{ needs.setup-env.outputs.service_account }} project_id: ${{ needs.setup-env.outputs.project_id }}  ","version":"Next","tagName":"h2"},{"title":"Tilgang til repoer med tokens fra GitHub Actions","type":0,"sectionRef":"#","url":"/docs/github-actions/tilgang-til-repoer-med-tokens-fra-github-actions","content":"","keywords":"","version":"Next"},{"title":"Secure Token Service (STS)â€‹","type":1,"pageTitle":"Tilgang til repoer med tokens fra GitHub Actions","url":"/docs/github-actions/tilgang-til-repoer-med-tokens-fra-github-actions#secure-token-service-sts","content":" En Secure Token Service (STS) er en tjeneste som utsteder sikkerhetstokener som kan brukes til autentisering og autorisering i ulike systemer og applikasjoner. I vÃ¥rt tilfelle Ã¸nsker vi Ã¥ utstede kortlevde tokens som kun er gyldige i perioden de brukes som en erstatning for PAT-er. Vi har derfor implementert et verktÃ¸y som heter Octo STS for Ã¥ levere denne funksjonaliteten.  MÃ¥ten STS fungerer pÃ¥ er at man etablerer tillit mellom to repoer. Dette gjÃ¸res ved Ã¥ legge inn en konfigurasjonsfil i repoet du Ã¸nsker Ã¥ ha tilgang til som sier noe om hvem som skal kunne fÃ¥ tilgang til repoet. Deretter bruker man en ferdig GitHub action i repot som skal fÃ¥ tilgang til Ã¥ etablere et kortlevd tiken via STS-tjenesten.  Les denne artikkelen for mer detaljer om Octo STS.  ","version":"Next","tagName":"h2"},{"title":"Etablere tillitâ€‹","type":1,"pageTitle":"Tilgang til repoer med tokens fra GitHub Actions","url":"/docs/github-actions/tilgang-til-repoer-med-tokens-fra-github-actions#etablere-tillit","content":" FÃ¸rst mÃ¥ man etablere tillit ved Ã¥ legge inn en config-fil i repoet man skal fÃ¥ tilgang til. Dette legges i mappen .github/chainguard/&lt;navn&gt;.sts.yaml . Erstatt &lt;navn&gt; med identiteten som skal ha tilgang og bruk dette navnet i GitHub actionen senere.  Eksempelet under viser hvordan man gir tilgang fra GitHub actions som kjÃ¸rer pÃ¥ repoet kartverket/mittrepo pÃ¥ branchen main .  issuer: https://token.actions.githubusercontent.com subject: repo:kartverket/mittrepo:ref:refs/heads/main permissions: contents: write   Dersom du Ã¸nsker Ã¥ bruke et wildcard til Ã¥ gi tilgang, for eksempel dersom det deployes ved hjelp av â€œenvironmentsâ€ i GitHub slik at dette blir subjektet ditt kan man bruke et subject_pattern . Dette er et regex.  issuer: https://token.actions.githubusercontent.com subject_pattern: repo:kartverket\\/mittrepo:environment:(sandbox|prod) permissions: contents: write   ","version":"Next","tagName":"h3"},{"title":"FÃ¥ tilgangâ€‹","type":1,"pageTitle":"Tilgang til repoer med tokens fra GitHub Actions","url":"/docs/github-actions/tilgang-til-repoer-med-tokens-fra-github-actions#fÃ¥-tilgang","content":" NÃ¥r man skal ha tilgang til dette repoet sÃ¥ bruker man en GitHub action til Ã¥ snakke med STS-tjenesten og fÃ¥ en kortlevd token som brukes pÃ¥ samme mÃ¥te som en PAT. For en deploy til et apps-repo kan du for eksempel skrive fÃ¸lgende i din GitHub action:  permissions: id-token: write # Required for Octo STS steps: - uses: octo-sts/action@6177b4481c00308b3839969c3eca88c96a91775f # v1.0.0 id: octo-sts with: scope: kartverket/skip-apps identity: utviklerportal - name: Checkout apps repo uses: actions/checkout@v4 with: repository: kartverket/skip-apps token: ${{ steps.octo-sts.outputs.token }}   NÃ¥r dette blir kjÃ¸rt vil det bli gjort en spÃ¸rring til Octo STS-tjenesten, som deretter sjekker filen vi laget i repoet over og om det har blitt etablert tillit. Dersom dette er tilfellet sÃ¥ genereres en token som brukes i dette eksempelet til Ã¥ sjekke ut et annet repo.  Se ogsÃ¥ https://github.com/octo-sts/action for dokumentasjon pÃ¥ GitHub actionen. ","version":"Next","tagName":"h3"},{"title":"Bruk av Terraform","type":0,"sectionRef":"#","url":"/docs/github-actions/bruk-av-terraform","content":"","keywords":"","version":"Next"},{"title":"Deploye applikasjoner med Terraformâ€‹","type":1,"pageTitle":"Bruk av Terraform","url":"/docs/github-actions/bruk-av-terraform#deploye-applikasjoner-med-terraform","content":" PÃ¥ SKIP har vi laget en enkel mÃ¥te Ã¥ deploye applikasjoner ved hjelp av Skiperator . Dette er en operator som setter opp alt av nettverking, sikkerhetsmekanismer, autoskalering, liveness- og readiness probes for deg sÃ¥ lenge man fyller ut en kort config-fil kalt en Application Custom Resource (CR). Man finner dokumentasjonen for hvorden denne Application CR-en ser ut pÃ¥ skiperator sin GitHub-side, og man kan se et eksempel pÃ¥ dette i Terraform-syntaks under.  resource &quot;kubernetes_manifest&quot; &quot;frontend_application&quot; { manifest = { apiVersion = &quot;skiperator.kartverket.no/v1alpha1&quot; kind = &quot;Application&quot; metadata = { name = local.app_name namespace = local.namespace } spec = { image = &quot;ghcr.io/kartverket/${local.app_name}:${var.image_version}&quot; port = 8080 ingresses = [ var.gateway_host ] replicas = { cpuThresholdPercentage = 80 max = 5 min = 3 } env = [ { name = &quot;BACKEND_URL&quot; value = var.backend-url }, ] liveness = { path = &quot;/&quot; port = 8080 } readiness = { path = &quot;/&quot; port = 8080 } resources = { limits = { cpu = &quot;1000m&quot; memory = &quot;1Gi&quot; } requests = { cpu = &quot;100m&quot; memory = &quot;100M&quot; } } accessPolicy = { outbound = { rules = [ { application = &quot;backend&quot; } ] } } } } }   ","version":"Next","tagName":"h2"},{"title":"Hente hemmeligheter med Vaultâ€‹","type":1,"pageTitle":"Bruk av Terraform","url":"/docs/github-actions/bruk-av-terraform#hente-hemmeligheter-med-vault","content":" Hvis man trenger Ã¥ bruke hemmeligheter deploy-time, for eksempel for Ã¥ deploye sertfikater eller passord til Kubernetes som secrets, sÃ¥ mÃ¥ man hente ut disse med vault_generic_secret . Eksempelet under gjÃ¸r dette for Ã¥ generere en docker pull secret som lar en pulle fra ghcr.io . Innholdet i hemmeligheten blir generert av en JSON template-fil som ikke er en del av eksempelet.  data &quot;vault_generic_secret&quot; &quot;github_token_ghcr_read&quot; { path = &quot;dsa/github_token_ghcr_read&quot; } data &quot;template_file&quot; &quot;docker_config_script&quot; { template = file(&quot;${path.module}/config.json&quot;) vars = { docker-server = data.vault_generic_secret.github_token_ghcr_read.data[&quot;server&quot;] auth = base64encode(&quot;${data.vault_generic_secret.github_token_ghcr_read.data[&quot;username&quot;]}:${data.vault_generic_secret.github_token_ghcr_read.data[&quot;token&quot;]}&quot;) } } resource &quot;kubernetes_secret&quot; &quot;github-auth&quot; { metadata { name = &quot;github-auth&quot; namespace = local.namespace } data = { &quot;.dockerconfigjson&quot; = data.template_file.docker_config_script.rendered } type = &quot;kubernetes.io/dockerconfigjson&quot; }   ","version":"Next","tagName":"h2"},{"title":"Lagre passord til Vaultâ€‹","type":1,"pageTitle":"Bruk av Terraform","url":"/docs/github-actions/bruk-av-terraform#lagre-passord-til-vault","content":" Noen ganger Ã¸nsker man Ã¥ skrive til vault, for eksempel nÃ¥r man genrerer passord. Eksempelet under gjÃ¸r dette.  resource &quot;random_password&quot; &quot;generated-password&quot; { length = 29 special = true lower = true upper = true number = true } resource &quot;vault_generic_secret&quot; &quot;password-for-vault-storage&quot; { path = &quot;skip/skipet&quot; data_json = &lt;&lt;EOT { &quot;username&quot;: &quot;${skip-bruker}&quot;, &quot;password&quot;: &quot;${random_password.generated-password.result}&quot;, &quot;connection_string&quot;: &quot;jdbc:postgresql://${kubernetes_service.skip-db-service.metadata.0.name}:${local.skip-db-port}/${local.skip-db-database-name}&quot; } EOT }   ","version":"Next","tagName":"h2"},{"title":"Lagring av stateâ€‹","type":1,"pageTitle":"Bruk av Terraform","url":"/docs/github-actions/bruk-av-terraform#lagring-av-state","content":" Terraform bruker state for Ã¥ kontrollere og sammenlikne den nÃ¥vÃ¦rende konfigurasjonen mot det som kjÃ¸rer, staten mÃ¥ lagres lokalt eller ekstern. PÃ¥ SKIP bruker vi Google Cloud Storage til Ã¥ lagre state, og oppsettet for dette kan man se under.  terraform { backend &quot;gcs&quot; { bucket = &quot;terraform_state_foobar_1e8e&quot; prefix = &quot;foobar-frontend&quot; } }   For at backenden over skal kunne nÃ¥ denne bucketen mÃ¥ service-kontoen den kjÃ¸rer som vÃ¦re autentisert mot Google Cloud med riktige tilganger. Dette gjÃ¸res i byggelÃ¸ypa fÃ¸r Terraform blir kjÃ¸rt, se avsnittet under for hvordan man autentiserer med Google Cloud som en del av Github Actionen.  ","version":"Next","tagName":"h2"},{"title":"KjÃ¸re Terraform i GitHub Actionsâ€‹","type":1,"pageTitle":"Bruk av Terraform","url":"/docs/github-actions/bruk-av-terraform#kjÃ¸re-terraform-i-github-actions","content":" Se https://github.com/kartverket/github-workflows for hvordan man bruker Terraform som en del av GitHub Actions. ","version":"Next","tagName":"h2"},{"title":"Tilgang til on-prem infrastruktur fra GitHub Actions","type":0,"sectionRef":"#","url":"/docs/github-actions/tilgang-til-on-prem-infrastruktur-fra-github-actions","content":"","keywords":"","version":"Next"},{"title":"Bakgrunnâ€‹","type":1,"pageTitle":"Tilgang til on-prem infrastruktur fra GitHub Actions","url":"/docs/github-actions/tilgang-til-on-prem-infrastruktur-fra-github-actions#bakgrunn","content":" warning Tailscale i denne konteksten er ment som et hjelpemiddel for Ã¥ migrere pakker ut til et ekstern pakkeregister, og som et verktÃ¸y for Ã¥ bli kvitt interne avhengigheter. Anbefales ikke for allmenn bruk.  For Ã¥ understÃ¸tte produktteamene med Ã¥ migrere bort fra intern kode- og artifakthosting, samt avhengigheter pÃ¥ interne databaser har SKIP introdusert Tailscale.  Tailscale er en mesh-basert peer-to-peer VPN-lÃ¸sning, som du kan lese mer om i deres egen dokumentasjon .  ","version":"Next","tagName":"h2"},{"title":"Komme i gangâ€‹","type":1,"pageTitle":"Tilgang til on-prem infrastruktur fra GitHub Actions","url":"/docs/github-actions/tilgang-til-on-prem-infrastruktur-fra-github-actions#komme-i-gang","content":" Kontakt en GitHub-administrator for Ã¥ be om tilgang for ditt repository  Hei $NAVN! Teamet mitt trenger tilgang til Ã¥ benytte Tailscale pÃ¥ repoet https://github.com/kartverket/mittRepo . Jeg trenger at du granter organisasjonshemmelighetene TS_OAUTH_CLIENT_ID og TS_OAUTH_SECRET (+ tilsvarende for Dependabot org-wide) pÃ¥ repoet, sÃ¥ klarer vi resten selv.  PÃ¥ forhÃ¥nd takk ðŸ™Œ  Etter du har fÃ¥tt tilgang til hemmelighetene, legg til fÃ¸lgende i din GitHub workflow  - name: Tailscale uses: tailscale/github-action@v2 with: oauth-client-id: ${{ secrets.TS_OAUTH_CLIENT_ID }} oauth-secret: ${{ secrets.TS_OAUTH_SECRET }} tags: tag:github-runner   Du kan nÃ¥ benytte deg av utvalgte interne tjenester. Lykke til!  Vil du vite hvilke tjenester du fÃ¥r tilgang til eller behov for flere tjenester enn dagens utvalg? Ta kontakt med SKIP pÃ¥ Slack. ","version":"Next","tagName":"h2"},{"title":"Autentisering til GitHub i terminalen","type":0,"sectionRef":"#","url":"/docs/github/autentisering-til-github-i-terminalen","content":"","keywords":"","version":"Next"},{"title":"Oppdater Gitâ€‹","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#oppdater-git","content":" warning Ikke hopp over dette steget . Du finner oversikt over sÃ¥rbare versjoner av git her: https://github.com/git/git/security/advisories  Velg ditt operativsystem og fÃ¸lg instruksene for Ã¥ installere den nyeste versjonen av Git.  Oppdater Git for LinuxOppdater Git for macOSOppdater Git for Windows  Du kan sjekke hvilken versjon du har med denne kommandoen:  git --version   ","version":"Next","tagName":"h2"},{"title":"Generer SSH nÃ¸kkelâ€‹","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#generer-ssh-nÃ¸kkel","content":" Du kan velge mellom ed25519 og RSA-4096.  (det finnes flere alternativer, men disse er vurdert som akseptable)  Bruk ssh-keygen for Ã¥ generere en ny nÃ¸kkel lokalt pÃ¥ din maskin. Husk Ã¥ bytt ut â€œDINEPOSTâ€ med Kartverket eposten din (f.eks. &quot;jell.fjell@kartverket.no&quot; ).  ssh-keygen -a 50 -t ed25519 -f ~/.ssh/github -C â€œDINEPOSTâ€   Alternativt kan du bruke RSA-4096 ssh-keygen -t rsa -b 4096 -f ~/.ssh/github -C &quot;DINEPOST&quot;   warning NB! Husk Ã¥ sette passord nÃ¥r du blir spurt. Ikke la passordfeltet stÃ¥ tomt.  ","version":"Next","tagName":"h2"},{"title":"Sett lokale rettigheter pÃ¥ SSH nÃ¸kkelenâ€‹","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#sett-lokale-rettigheter-pÃ¥-ssh-nÃ¸kkelen","content":" SSH nÃ¸kkelen er privat for din bruker, og skal kun leses av din bruker.  chmod 600 ~/.ssh/github   ","version":"Next","tagName":"h3"},{"title":"Legg til nÃ¸kkelen (public key) i GitHubâ€‹","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#legg-til-nÃ¸kkelen-public-key-i-github","content":" Vis og kopier din public key fra~/.ssh/github.pubfra terminalen.  cat ~/.ssh/github.pub   Marker utskriften og kopier innholdet.  Logg inn pÃ¥ GitHub.com med Kartverket kontoen din.Trykk pÃ¥ profilbildet ditt, Ã¸verst i hÃ¸yre hjÃ¸rne.Velg Â« Settings Â».Naviger deg til Â« SSH and GPG keys Â» (under kategorien Â«AccessÂ») i venstre kolonne.Trykk pÃ¥ den grÃ¸nne Â« New SSH key Â» knappen.Skriv inn en passelig tittel (f.eks. â€œMin private SSH nÃ¸kkelâ€).Kopier og lim inn innholdet fra~/.ssh/github.pub(ikke private key), som vist i fÃ¸rste steg.Trykk pÃ¥ Â« Add SSH key Â».Du skal nÃ¥ se oversikten over dine nÃ¸kler, med den nye nÃ¸kkelen i listen.For Ã¥ bruke kartverket nÃ¸kkelen mÃ¥ man bekrefte nÃ¸kkelen med SSO. Dette gjÃ¸res ved Ã¥ trykke configure SSO pÃ¥ nÃ¸kkelen.  ","version":"Next","tagName":"h2"},{"title":"Test nÃ¸kkelenâ€‹","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#test-nÃ¸kkelen","content":" Du kan raskt teste nÃ¸kkelen din mot GitHub ved Ã¥ kjÃ¸re:  ssh -T git@github.com -i ~/.ssh/github   Du skal fÃ¥ tilbakemelding om vellykket autentisering:  Hi! You've successfully authenticated, but GitHub does not provide shell access.   ","version":"Next","tagName":"h2"},{"title":"Automatisk bruk av nÃ¸kkelen dinâ€‹","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#automatisk-bruk-av-nÃ¸kkelen-din","content":" note Det finnes flere mÃ¥ter Ã¥ ta i bruk nÃ¸kkelen din. Dette er et eksempel pÃ¥ hvordan, men du stÃ¥r fritt til Ã¥ bruke andre lÃ¸sninger.  Opprett filen~/.ssh/configog fyll den ut med innholdet for GitHub med nÃ¸kkelen din:  Host github HostName github.com User git IdentityFile ~/.ssh/github   ","version":"Next","tagName":"h2"},{"title":"Ta i bruk nÃ¸kkelen nÃ¥r du kloner et repoâ€‹","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#ta-i-bruk-nÃ¸kkelen-nÃ¥r-du-kloner-et-repo","content":" Du kan ta i bruk nÃ¸kkelen din ved Ã¥ refere til github nÃ¥r du skal klone et repo. Husk Ã¥ bytt ut â€œDITTREPOâ€ med navnet pÃ¥ repoet du prÃ¸ver Ã¥ klone.  git clone github:kartverket/DITTREPO.git   NÃ¥r du kloner repoet pÃ¥ denne mÃ¥ten vil Git automatisk ta i bruk remote med din konfigurasjon (tar automatisk i bruk nÃ¸kkelen din ved git pull / push ).  ","version":"Next","tagName":"h3"},{"title":"Legg til navn og epost for riktig eier av commitsâ€‹","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#legg-til-navn-og-epost-for-riktig-eier-av-commits","content":" For at commits du gjÃ¸r pÃ¥ din maskin skal stemme overens med GitHub brukeren din mÃ¥ du sette brukernavn og epost i Git. Husk Ã¥ bytt ut â€œDITT NAVNâ€ med github brukernavnet ditt (f.eks. â€œjellfjellâ€œ) og â€œDINEPOSTâ€ med Kartverket eposten din (f.eks. &quot;jell.fjell@kartverket.no&quot; ).  git config --global user.name &quot;DITT NAVN&quot; git config --global user.email &quot;DINEPOST&quot;   TLDR For deg som ikke leste i gjennom og vil rett pÃ¥ sak uten forklaring. Oppdater Git: https://git-scm.com/downloads - IKKE HOPP OVER DETTE STEGET ssh-keygen -a 50 -t ed25519 -f ~/.ssh/github -C â€œDINEPOSTâ€ chmod 600 ~/.ssh/github cat ~/.ssh/github.pub Kopier og lim inn public key pÃ¥ GitHub https://github.com/settings/ssh/new ssh -T git@github.com -i ~/.ssh/github git config --global user.name &quot;DITT NAVN&quot; git config --global user.email &quot;DINEPOST&quot; git clone URL --config core.sshCommand=&quot;ssh -i ~/.ssh/github&quot; NÃ¥ er du klar for Ã¥ begi deg ut pÃ¥ eventyr. ","version":"Next","tagName":"h2"},{"title":"HÃ¥ndtering av sensitiv data som er kommet pÃ¥ repositoriet","type":0,"sectionRef":"#","url":"/docs/github/hÃ¥ndtering-av-sensitiv-data-som-er-kommet-pÃ¥-repositoriet","content":"","keywords":"","version":"Next"},{"title":"ðŸ“˜ Instruksjonerâ€‹","type":1,"pageTitle":"HÃ¥ndtering av sensitiv data som er kommet pÃ¥ repositoriet","url":"/docs/github/hÃ¥ndtering-av-sensitiv-data-som-er-kommet-pÃ¥-repositoriet#-instruksjoner","content":" for Ã¥ se om secret scanning har avduket noen sensitive data i repositoriet gÃ¥ inn pÃ¥ repositorierts forside og klikk deg inn pÃ¥ security-fanen og deretter trykk deg inn pÃ¥ sidemeny-valget â€œSecret scanning alertsâ€trykk deg inn pÃ¥ det varselet for det sensitive dataen du skal lÃ¸seher fÃ¥r du vite hvilke filer det er snakk om og akkurat hvilken linje det er snakk om.Deretter er det Ã¥ fÃ¸lge denne guiden, Fjerne sensitive data fra repositorier for selve fjerningen av de sensitive dataenenÃ¥r fjerningen er gjor kan man lukke varslet  ","version":"Next","tagName":"h2"},{"title":"ðŸ“‹ Relaterte artiklerâ€‹","type":1,"pageTitle":"HÃ¥ndtering av sensitiv data som er kommet pÃ¥ repositoriet","url":"/docs/github/hÃ¥ndtering-av-sensitiv-data-som-er-kommet-pÃ¥-repositoriet#-relaterte-artikler","content":" Fjerne sensitive data fra repositorier  Secret Scanning ","version":"Next","tagName":"h2"},{"title":"Bruk av GitHub med Jenkins","type":0,"sectionRef":"#","url":"/docs/github/bruk-av-github-med-jenkins","content":"","keywords":"","version":"Next"},{"title":"ðŸ“š Autentisering ðŸ“šâ€‹","type":1,"pageTitle":"Bruk av GitHub med Jenkins","url":"/docs/github/bruk-av-github-med-jenkins#-autentisering-","content":" Det er flere mÃ¥ter Ã¥ autentisere Jenkins mot GitHub pÃ¥, blant annet; deploy keys, personal access tokens, GitHub App. Vi vil se at GitHub Apps er valget vi gÃ¥r for nÃ¥r vi autentiserer.   Deploy keys er enkle men;  ðŸ‘ Eies av repoet og Jenkins (priv + pub nÃ¸kler)ðŸ‘Š Kan kun brukes som â€œGitâ€ source pÃ¥ JenkinsðŸ‘Ž Snakker ikke med GitHub sitt API - kun pulle / pushe kode   Personal access tokens **** (PAT) gir mer;  ðŸ‘ Kan brukes gjennom â€œGitHubâ€ plugin pÃ¥ Jenkins (source)ðŸ‘ Snakker med GitHub APIâ€™et - PR/Commit status triggere etc.ðŸ‘Ž NÃ¸kkelen fÃ¸lger brukeren, selv etter vedkommende bytter team eller slutter (kan slettes fra bruker)ðŸ‘Ž Ikke i utgangspunktet gjenbrukbar (beta- fine grained PATâ€™er kan tilegnes flere repo pr. nÃ¸kkel)   GitHub Apps er litt mer Ã¥ konfigurere, men er en kombinasjon av de over;  ðŸ‘ Gjenbrukbare, som flere repoer kan bruke gjennom Ã©n privat nÃ¸kkel pÃ¥ Jenkins.ðŸ‘ Eies av â€œOrganisasjonenâ€ Kartverket pÃ¥ Github, som da ikke er bundet til en GitHub bruker.ðŸ‘ Kan brukes gjennom â€œGitHubâ€ plugin pÃ¥ Jenkins (source)ðŸ‘ Snakker med GitHub APIâ€™et - PR/Commit status triggere etc.ðŸ‘Ž Ratelimit (men skal ikke vÃ¦re et problem)  ","version":"Next","tagName":"h2"},{"title":"ðŸ§‘â€ðŸš’ Brannmurer ðŸ§‘â€ðŸš’â€‹","type":1,"pageTitle":"Bruk av GitHub med Jenkins","url":"/docs/github/bruk-av-github-med-jenkins#-brannmurer-","content":" I utgangspunktet sÃ¥ skal portene til ditt Jenkins miljÃ¸ vÃ¦re Ã¥pnet, slik at Jenkins nÃ¥r ut til GitHub. Men hvis det dette er fÃ¸rste gang sÃ¥ mÃ¥ de Ã¥pnes for trafikk mot GitHub. PrimÃ¦rt er det HTTPs og SSH trafikk som mÃ¥ tilgjengeliggjÃ¸res pÃ¥ port 443 og 22. Dette mÃ¥ bestilles hos drift.  ","version":"Next","tagName":"h2"},{"title":"ðŸª Webhook ðŸªâ€‹","type":1,"pageTitle":"Bruk av GitHub med Jenkins","url":"/docs/github/bruk-av-github-med-jenkins#-webhook-","content":" Work in progress. Er ikke ferdig testet enda.  For Ã¥ fÃ¥ status pÃ¥ PR/Commits i GitHub sÃ¥ mÃ¥ GitHub ha en vei inn til Jenkins. Dette gjÃ¸res pÃ¥ et webhook endepunkt typisk seende slik ut https://&lt;jenkins-host&gt;/github-webhook/ . Dette er noe som mÃ¥ Ã¥pnes fra drift og spesifiseres inne i GitHub Appen.  âš™ï¸ Legg til hvordan det er med webhook secret.  ","version":"Next","tagName":"h2"},{"title":"ðŸ“ Oppsett av GitHub App ðŸ“â€‹","type":1,"pageTitle":"Bruk av GitHub med Jenkins","url":"/docs/github/bruk-av-github-med-jenkins#-oppsett-av-github-app-","content":" SKIP kontaktes og de setter opp en App for ditt behov. Er denne som fÃ¸lges: Using GitHub App authentication .  info Oppsettet av nÃ¸klen mÃ¥ du gjÃ¸re selv! Og dette FÃ˜R du fÃ¥r brukt Appen, men ETTER at SKIP setter igang med oppsett av app. SKIP sender melding nÃ¥r du mÃ¥ gjÃ¸re dette. nb: skal Appen ha flere/mindre rettigheter enn i oppskriften mÃ¥ du spesifisere dette  NÃ¥r SKIP har satt opp Appen, mÃ¥ du sette den private nÃ¸kkelen, som senere skal deles med Jenkins. Dette gjÃ¸res slik som beskrevet i punktet Generating a private key for authenticating to the GitHub App .  Det er fÃ¸rst nÃ¥r dette er gjort, at SKIP kan installere Appen pÃ¥ organisasjonen. Send en heads-up at du har lagret nÃ¸kkelen.  Hvis Appen er installert i org. og linket til ditt repo, og nÃ¸kkelen er satt opp i App og Jenkins sÃ¥ skal alt vÃ¦re pÃ¥ plass! ðŸŽ‰ ","version":"Next","tagName":"h2"},{"title":"âš™ï¸ Kubernetes","type":0,"sectionRef":"#","url":"/docs/kubernetes","content":"âš™ï¸ Kubernetes","keywords":"","version":"Next"},{"title":"Opprette nytt repo pÃ¥ Github","type":0,"sectionRef":"#","url":"/docs/github/opprette-nytt-repo-pÃ¥-github","content":"","keywords":"","version":"Next"},{"title":"Merknad for produkter som ikke er pÃ¥ SKIPâ€‹","type":1,"pageTitle":"Opprette nytt repo pÃ¥ Github","url":"/docs/github/opprette-nytt-repo-pÃ¥-github#merknad-for-produkter-som-ikke-er-pÃ¥-skip","content":" Merk at det meste av dette dokumentet ogsÃ¥ er gyldig for prosjekter som ikke er pÃ¥ SKIP-plattformen - men at det likevel er skrevet for SKIP-teams, sÃ¥ sikkerhetsreglene kan sees pÃ¥ som gode rÃ¥d dersom du ikke skal bruke SKIP.  For ikke Ã¥ snakke om at du dersom du fÃ¸lger disse sikkerhetsreglene vil fÃ¥ en mye enklere jobb hvis du skal flytte prosjektet over til SKIP i fremtiden   ","version":"Next","tagName":"h3"},{"title":"Hvordan opprette et nytt GitHub Repositoryâ€‹","type":1,"pageTitle":"Opprette nytt repo pÃ¥ Github","url":"/docs/github/opprette-nytt-repo-pÃ¥-github#hvordan-opprette-et-nytt-github-repository","content":" Logg inn pÃ¥ GitHubOpprett et nytt repository ved Ã¥ trykke pÃ¥ pluss-ikonet Ã¸verst til hÃ¸yre pÃ¥ https://github.com og velge â€œNew repositoryâ€. Dette gjelder uansett om du skal lage et nytt prosjekt eller importere et eksisterende prosjekt, siden du ikke vil kunne bruke â€œImportâ€-funksjonaliteten pÃ¥ vanlig mÃ¥te.Dersom du skal importere et eksisterende git-repository, fÃ¸lg denne tutorialen .Fyll ut skjemaet med riktig informasjon. Huskeregler: Alle prosjekter som ikke skal vÃ¦re Ã¥pne skal vÃ¦re Internal . Det er likevel mulig Ã¥ invitere eksterne utviklere. Mer informasjon: https://docs.github.com/en/repositories/creating-and-managing-repositories/about-repositories#about-repository-visibilityPass pÃ¥ at Owner er satt til kartverket , og ikke din private bruker.Ikke velg en lisens med mindre du faktisk skal lage et open-source prosjekt. Ã… velge en Ã¥pen kildekode-lisens her kan Ã¸delegge for sikkerhetsverktÃ¸yene i Kartverket og i siste instans skape legale problemer for Kartverket. Hvis du er i tvil, ta kontakt med SKIP-teamet. Dokumenter hvilket team som er ansvarlig for repositoriet ved Ã¥ opprette en Codeowners fil.Dette er dokumentert her .Som regel er det nok med en linje - slik (bytt ut skip med ditt eget team).Gi teamet ditt rettigheter til repoet. Dette er dokumentert her . Det er vanlig Ã¥ sette Tech Lead som eier for repositoriet, men dette bestemmer dere selv.  ","version":"Next","tagName":"h2"},{"title":"Opprett tilganger til Google Cloud for Github Actionsâ€‹","type":1,"pageTitle":"Opprette nytt repo pÃ¥ Github","url":"/docs/github/opprette-nytt-repo-pÃ¥-github#opprett-tilganger-til-google-cloud-for-github-actions","content":" Dersom du har behov til Ã¥ autentisere deg mot GCP kan du legge til at ditt repo GitHub kan autentisere seg mot Google Cloud med en bestemt bruker. Da mÃ¥ man sette opp Workload Identity Federation . Dette er noe SKIP ordner for produktteamene pÃ¥ en automatisert mÃ¥te ved hjelp av Terraform.  Ã˜nsker du Ã¥ legge til et nytt repo kan du opprette en Pull Request for dette repoet: https://github.com/kartverket/gcp-service-accounts  Eksempel pÃ¥ liste over GitHub repoer for KomReg: https://github.com/kartverket/gcp-service-accounts/blob/main/modules.tf  module &quot;komreg&quot; { source = &quot;./project_team&quot; team_name = &quot;KomReg&quot; repositories = [ &quot;kartverket/komreg-frontend&quot;, &quot;kartverket/komreg-backend&quot;, &quot;kartverket/komreg-frontend-api&quot;, # Legg til flere repoer i denne listen ] env = var.env project_id = var.komreg_project_id kubernetes_project_id = var.kubernetes_project_id can_manage_log_alerts_and_metrics = true can_manage_sa = true extra_team_sa_roles = [ &quot;roles/resourcemanager.projectIamAdmin&quot;, &quot;roles/secretmanager.admin&quot;, &quot;roles/storage.admin&quot; ] }   NÃ¥r PRâ€™en merges inn vil det ved et nytt team bli opprettet en deploy-servicekonto, som heter &lt;teamnavn&gt;-deploy@&lt;prosjekt-id&gt;.iam.gserviceaccount.com . Denne servicekontoen tillater at github-repoene i listen har lov til Ã¥ etterligne den og dens tilganger.  Mer informasjon om Github Actions: GitHub Actions som CI/CD ","version":"Next","tagName":"h2"},{"title":"Tilgang til GitHub","type":0,"sectionRef":"#","url":"/docs/github/tilgang-til-github","content":"","keywords":"","version":"Next"},{"title":"Bistand og diskusjon rundt GitHubâ€‹","type":1,"pageTitle":"Tilgang til GitHub","url":"/docs/github/tilgang-til-github#bistand-og-diskusjon-rundt-github","content":" Logg pÃ¥ slack ved Ã¥ laste ned programmet fra http://slack.com og bruk kartverketgroup.slack.com som workspaceTa kontakt med SKIP pÃ¥ slack i #gen-github for Ã¥ fÃ¥ en invitasjon til GitHub-organisasjonen til Kartverket (send github-brukernavnet ditt). Dersom du vet pÃ¥ forhÃ¥nd at du jobber som del av et team sÃ¥ fortell oss hvilke team dette er sÃ¥ fÃ¥r vi lagt deg i tilsvarende team i GitHub   ","version":"Next","tagName":"h2"},{"title":"Bruk av porter i pods","type":0,"sectionRef":"#","url":"/docs/kubernetes/bruk-av-porter-i-pods","content":"Bruk av porter i pods Porter under 1024 er priviligierte og krever at prosessen som kjÃ¸rer kjÃ¸rer som root. Dette er ikke tillatt pÃ¥ SKIP, og prosessen mÃ¥ derfor binde til en hÃ¸yere port. Dette betyr at man ofte mÃ¥ gjÃ¸re tilpasninger pÃ¥ Docker-imaget man bygger slik at f.eks. nginx binder til en annen port. NÃ¥r prosessen i containeren binder seg til en upriviligert port, kan man spesifisere denne porten i Skiperator-manifestet slik som under. I bakgrunnen vil Skiperator ta seg av Ã¥ lage en Kubernetes-service for denne porten slik at trafikken kan rutes til riktig sted. apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: teamname-frontend namespace: yournamespace spec: image: &quot;kartverket/eksempel-image&quot; port: 8080 additionalPorts: - name: metrics-port port: 8181 protocol: TCP - name: another-port port: 8090 protocol: TCP En annen ting Ã¥ merke seg her er muligheten for Ã¥ spesifisere ekstra porter som kan benyttes til andre formÃ¥l som f.eks. helsesjekker eller prometheus-metrikker. Disse portene vil automatisk fÃ¥ opprettet en service, men det er fortsatt kun hovedporten som kobles opp mot en ingress-gateway. PÃ¥ den mÃ¥ten kan man skille ut endepunktet som ikke trengs eksternt.","keywords":"","version":"Next"},{"title":"Autentisering mot GCP fra applikasjon","type":0,"sectionRef":"#","url":"/docs/kubernetes/autentisering-mot-gcp-fra-applikasjon","content":"","keywords":"","version":"Next"},{"title":"1. Opprett Servicekontoâ€‹","type":1,"pageTitle":"Autentisering mot GCP fra applikasjon","url":"/docs/kubernetes/autentisering-mot-gcp-fra-applikasjon#1-opprett-servicekonto","content":" Dersom man Ã¸nsker Ã¥ fÃ¥ tilgang til GCP-tjenester fra Kubernetes gjÃ¸res dette med Ã¥ fÃ¸rst opprette en servicekonto i GCP og Ã¥ gi den IAM-rettigheter til det man Ã¸nsker at den skal gjÃ¸re.  Servicekontoer bÃ¸r enten opprettes med terraform eller via gcp-service-accounts repoet til SKIP.  ","version":"Next","tagName":"h2"},{"title":"2. Gi WIF IAM Policy til Servicekontoâ€‹","type":1,"pageTitle":"Autentisering mot GCP fra applikasjon","url":"/docs/kubernetes/autentisering-mot-gcp-fra-applikasjon#2-gi-wif-iam-policy-til-servicekonto","content":" To authenticate this service account in GCP from Kubernetes, the service account in Kubernetes needs to be given permission to impersonate the GCP service account. This is done by giving the Kubernetes Service Account the role iam.workloadIdentityUser through a so called Workload Identity Pool.  Given the following variables:  GCP_SA_NAME - Name of the GCP service account GCP_SA_PROJECT_ID - GCP Project ID where the service account resides KUBERNETES_PROJECT_ID - GCP Project ID for the Kubernetes cluster (for example kubernetes-dev-94b9 for dev-cluster) KUBERNETES_NAMESPACE - The Kubernetes namespace where the Pod will run KUBERNETES_SA_NAME - The Kubernetes service account name that your Pod is using (typically same name as Application, and with the -skipjob suffix for SKIPJobs)   Run the following command using the gcloud CLI:  gcloud iam service-accounts add-iam-policy-binding \\ GCP_SA_NAME@GCP_SA_PROJECT_ID.iam.gserviceaccount.com \\ --role=roles/iam.workloadIdentityUser \\ --member=&quot;serviceAccount:KUBERNETES_PROJECT_ID.svc.id.goog[KUBERNETES_NAMESPACE/KUBERNETES_SA_NAME]&quot;   ","version":"Next","tagName":"h2"},{"title":"3. Legg inn config i Skiperatormanifestâ€‹","type":1,"pageTitle":"Autentisering mot GCP fra applikasjon","url":"/docs/kubernetes/autentisering-mot-gcp-fra-applikasjon#3-legg-inn-config-i-skiperatormanifest","content":" Til slutt legger man til gcp config i sin skiperator Application for Ã¥ lage kubernetes-config slik at podden kan autentisere mot GCP.  //yaml format spec: gcp: auth: serviceAccount: GCP_SA_NAME@GCP_SA_PROJECT_ID.iam.gserviceaccount.com   NÃ¥ kan man fÃ¸lge â€œAuthenticate from your codeâ€ under https://cloud.google.com/anthos/fleet-management/docs/use-workload-identity#-python for Ã¥ autentisere mot GCP fra koden sin.  NÃ¥r dette er gjort kan applikasjonen snakke med GCP under runtime.  ","version":"Next","tagName":"h2"},{"title":"Alternativ til 1 / 2â€‹","type":1,"pageTitle":"Autentisering mot GCP fra applikasjon","url":"/docs/kubernetes/autentisering-mot-gcp-fra-applikasjon#alternativ-til-1--2","content":" Dersom man ikke Ã¸nsker Ã¥ legge til roller manuelt har SKIP lagt til en ny mÃ¥te Ã¥ legge til Workload Identity User pÃ¥ en service account, ved hjelp av Crossplane.  apiVersion: 'skip.kartverket.no/v1alpha1' kind: 'WorkloadIdentityInstance' metadata: name: 'service-account-wi' spec: parameters: gcpKubernetesProject: 'some-kubernetes-project' #eks: 'kubernetes-dev-94b9' gcpProject: 'gcp-project-where-service-account-is' #eks: 'dsa-dev-e32c' gcpServiceAccount: 'name-of-service-account-in-gcp' #eks: 'dsa-runtime@dsa-dev-e32c.iam.gserviceaccount.com' serviceAccount: 'name-of-service-account-in-kubernetes' #eks 'dsa-backend', typically same name as your Application   Se Provisjonere infrastruktur med Crossplane om du ikke har brukt Crossplane tidligere. ","version":"Next","tagName":"h2"},{"title":"End-user IP-Addresses in Containers","type":0,"sectionRef":"#","url":"/docs/kubernetes/end-user ip-addresses-in-containers","content":"End-user IP-Addresses in Containers To forward end-user IP-Addresses to a kubernetes container running spring boot, you need to add the following line to your configuration: server.forward-headers-strategy=NONE After testing, we found that this setting should be â€œNONEâ€. Running spring Behind a Front-end Proxy Server Spring server.forward-headers-strategy NATIVE vs FRAMEWORK","keywords":"","version":"Next"},{"title":"Jobbe med Kubernetes cluster","type":0,"sectionRef":"#","url":"/docs/kubernetes/jobbe-med-cluster","content":"","keywords":"","version":"Next"},{"title":"K9sâ€‹","type":1,"pageTitle":"Jobbe med Kubernetes cluster","url":"/docs/kubernetes/jobbe-med-cluster#k9s","content":" K9s er terminalbasert men gir deg mer informasjon enn du ellers ville fÃ¥tt ved enkle kubectl kommandoer. Se her en oversikt over alle Podder som kjÃ¸rer i et namespace.    Her fÃ¥r vi for eksempel en stor toast pÃ¥ at alle poddene kjÃ¸rer med mer minne enn de requester. I tillegg har vi en fin oversikt over generell ressursbruk og forhold mellom request/limit og faktisk bruk.  Man kan enkelt sortere pÃ¥ alle felter, sÃ¸ke pÃ¥ vilkÃ¥rlige ressurstyper, redigere ressurser, filtere basert pÃ¥ sÃ¸k og masse mer.  Nedlasting: K9s - Manage Your Kubernetes Clusters In Style ","version":"Next","tagName":"h2"},{"title":"Logge inn pÃ¥ cluster","type":0,"sectionRef":"#","url":"/docs/kubernetes/logge-inn-pÃ¥-cluster","content":"","keywords":"","version":"Next"},{"title":"CLI (kubectl)â€‹","type":1,"pageTitle":"Logge inn pÃ¥ cluster","url":"/docs/kubernetes/logge-inn-pÃ¥-cluster#cli-kubectl","content":" FÃ¸rst installer gcloud og kubectl .  For Ã¥ logge inn med kubectl gjÃ¸r fÃ¸lgende:  # Login med gcloud hvis du ikke har gjort det allerede $ gcloud auth login # SÃ¸rg for at du stÃ¥r i riktig gcp-prosjekt # Hvis du ikke vet hele navnet pÃ¥ prosjektet kan du finne dette vet Ã¥ liste prosjekter $ gcloud projects list # GCP-prosjektet vil vÃ¦re et kubernetes-prosjekt med format kubernetes-&lt;env&gt;-xxxx # Jobber du f.eks. i dsa-dev-e32c velger du kubernetes-dev-94b9 $ gcloud config set project kubernetes-dev-94b9 # Finn riktig clusternavn $ gcloud container hub memberships list # Per 14.02.2023 er clusternavn alltid pÃ¥ formatet atkv1-&lt;env&gt; (on-premise) # Logg inn, generer kubeconfig og sett som aktiv context $ gcloud container hub memberships get-credentials atkv1-dev # Forrige kommando oppretter en ny context, som kan autentisere deg mot clusteret # Contexten som blir opprettet her ser noe a la slik ut: # connectgateway_kubernetes-&lt;env&gt;-xxxx_global_atkv1-&lt;env&gt; # Eksempel: connectgateway_kubernetes-dev-94b9_atkv1-dev # Har du lastet ned kubectx kan du bytte til contexten slik: $ kubectx connectgateway_kubernetes-dev-94b9_atkv1-dev # Om ikke kan du gjÃ¸re det med fÃ¸lgende kommando i kubectl: $ kubectl config use-context connectgateway_kubernetes-dev-94b9_atkv1-dev # Du kan ogsÃ¥ rename disse contextene til noe litt mer spiselig med fÃ¸lgende kommando # Her er navn 2 vilkÃ¥rlig $ kubectl config rename-context connectgateway_kubernetes-dev-94b9_atkv1-dev atkv1-dev   Se ogsÃ¥ https://cloud.google.com/anthos/multicluster-management/gateway/using .  For Ã¥ ha adgang til Ã¥ logge pÃ¥ clusteret mÃ¥ du vÃ¦re lagt inn i en CLOUD_SK_TEAM AD-gruppe som er synket med GCP. ","version":"Next","tagName":"h2"},{"title":"Certificates outside ACME","type":0,"sectionRef":"#","url":"/docs/kubernetes/certificates-outside-acme","content":"","keywords":"","version":"Next"},{"title":"Create certificate secret resource in istio-gatewaysâ€‹","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kubernetes/certificates-outside-acme#create-certificate-secret-resource-in-istio-gateways","content":" To be able to use a custom certificate we need a secret to mount to the gateway resource. This is a kubernetes.io/tls type secret and can be created via external secrets like this:  apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: star-matrikkel namespace: istio-gateways spec: dataFrom: - extract: conversionStrategy: Default decodingStrategy: Auto key: star-matrikkel-no-key refreshInterval: 1h secretStoreRef: kind: SecretStore name: gsm target: creationPolicy: Owner deletionPolicy: Retain name: star-matrikkel # Secret in Kubernetes template: engineVersion: v2 mergePolicy: Replace type: kubernetes.io/tls   This fetches the secret from Google Secret Manager. This secret should look like this:  { &quot;tls.crt&quot;:&quot;[base64 encoded cert chain]&quot;, &quot;tls.key&quot;:&quot;[base64 encoded tls.key]&quot; }   ","version":"Next","tagName":"h2"},{"title":"Edit the gateway resourceâ€‹","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kubernetes/certificates-outside-acme#edit-the-gateway-resource","content":" The gateway resource should then be updated with the new secret:  apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: gateway-ingress namespace: matrikkel-keycloak spec: selector: app: istio-ingress-external servers: - hosts: - auth.matrikkel.no port: name: http number: 80 protocol: HTTP - hosts: - auth.matrikkel.no port: name: https number: 443 protocol: HTTPS tls: credentialName: star-matrikkel # Secret created by externalsecret mode: SIMPLE   ","version":"Next","tagName":"h2"},{"title":"If Skiperator is the gateway creatorâ€‹","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kubernetes/certificates-outside-acme#if-skiperator-is-the-gateway-creator","content":" When the gateway is created via Skiperator it will have a credentialName corresponding to the secret created by the certificate from Skiperator. Skiperator will reset configurations to its resources unless the resource labeled â€œskiperator.kartverket.no/ignore: &quot;true&quot;â€œ. This will make skiperator ignore this specific resource during reconciliation loops.  apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: labels: skiperator.kartverket.no/ignore: &quot;true&quot;   This is meant to be a temporary solution, and ACME is the prefered way to get certificates in SKIP.  ","version":"Next","tagName":"h3"},{"title":"Change to ACME certificateâ€‹","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kubernetes/certificates-outside-acme#change-to-acme-certificate","content":" ","version":"Next","tagName":"h2"},{"title":"Non-Skiperator appsâ€‹","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kubernetes/certificates-outside-acme#non-skiperator-apps","content":" Using ACME certificate on a non skiperator app requires a certificate resource, and using the resulting secret in the gateway. This resource must be created in the istio-gateways namespace and therefore in the skip-apps :  apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: certificate-name namespace: istio-gateways spec: dnsNames: - appname.kartverket.no issuerRef: kind: ClusterIssuer name: cluster-issuer secretName: desired-secret-name   After this is created and the secret is created, the gateway resource can be edited, and spec.tls.credentialName set to the secret.  ","version":"Next","tagName":"h3"},{"title":"Skiperator appsâ€‹","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kubernetes/certificates-outside-acme#skiperator-apps","content":" Remove the â€œskiperator.kartverket.no/ignore: &quot;true&quot;â€œ label, and skiperator will handle the rest. ","version":"Next","tagName":"h3"},{"title":"Retningslinjer for Kubernetes","type":0,"sectionRef":"#","url":"/docs/kubernetes/retningslinjer-for-kubernetes","content":"","keywords":"","version":"Next"},{"title":"Minstekrav for sikkerhetâ€‹","type":1,"pageTitle":"Retningslinjer for Kubernetes","url":"/docs/kubernetes/retningslinjer-for-kubernetes#minstekrav-for-sikkerhet","content":" I Kubernetes bruker vi Pod Security Standards for Ã¥ sikre at alle pods som kjÃ¸rer i clusteret vÃ¥rt er sikre. Dette er en standard som er satt av CNCF, og som vi fÃ¸lger for Ã¥ sikre at vi ikke har noen Ã¥penbare sikkerhetshull i Kubernetes-clusteret vÃ¥rt. Alle workloads skal fÃ¸lge PSS nivÃ¥ &quot;restricted&quot;, som er et nivÃ¥ som fÃ¸lger dagens best practices for sikring av containere. Applikasjoner som kjÃ¸rer som Skiperator Applications fÃ¸lger allerede denne standarden. Les mer om Pod Security Standards her.  ","version":"Next","tagName":"h2"},{"title":"Namespaces som avgrensning mellom teamsâ€‹","type":1,"pageTitle":"Retningslinjer for Kubernetes","url":"/docs/kubernetes/retningslinjer-for-kubernetes#namespaces-som-avgrensning-mellom-teams","content":" Hvert team kan lage sÃ¥ mange Namespaces som de har behov for. Dette er for Ã¥ kunne skille pÃ¥ ressurser og tilganger mellom forskjellige applikasjoner og team. Dette er ogsÃ¥ for Ã¥ kunne gi teamene mulighet til Ã¥ eksperimentere og teste ting uten at det pÃ¥virker andre team. Les mer om dette pÃ¥ Argo CD.  Kommunikasjon mellom tjenester internt i ett namespace er helt lukket (â€œdefault denyâ€-policy), og det er opp til teamet selv Ã¥ sÃ¸rge for Ã¥ Ã¥pne for kommunikasjon mellom tjenester. Les mer om dette pÃ¥ Skiperator.  Informasjon om kommunikasjon mot tjenester som ligger i andre namespaces finnes her: Anthos Service Mesh Brukerdokumentasjon  Tjenester og fellesfunksjoner som brukes av flere teams skal settes i egne namespaces. Tilgang til disse namespacene gis ved at det opprettes en ny gruppe i AD pÃ¥ samme mÃ¥te som et produktteam.  ","version":"Next","tagName":"h2"},{"title":"Ressursbruk i Kubernetesâ€‹","type":1,"pageTitle":"Retningslinjer for Kubernetes","url":"/docs/kubernetes/retningslinjer-for-kubernetes#ressursbruk-i-kubernetes","content":" Ressursbruk i Kubernetes dreier seg om hvor mye CPU og RAM hver pod skal bruke.  Requests er hvor mye CPU og minne hvercontainerspÃ¸r om nÃ¥r den fÃ¸rst settes pÃ¥ en node. Hvis man for eksempel ber om 500 mCPUer, men noden bare har 250 mCPU ledig, kan containeres ikke kjÃ¸res pÃ¥ den noden.  Merk at man kan spesifisere CPU helt ned i millicpuer (mCPU).  Minne-requests kan settes i mange forskjellige enheter, se dokumentasjonen for detaljer. Vi anbefaler dog at man holder seg til M - megabytes.  Limits er hvor mye ressurser en container maksimalt fÃ¥r lov til Ã¥ bruke. Dette er med andre ord noe som settes for Ã¥ forhindre at en container med en bug tar over alle ressursene, og gjÃ¸r det umulig for andre containere Ã¥ skalere.  Vi anbefaler at du ser pÃ¥ Limiten som en mulighet til Ã¥ finne bugs og memory leaks. Sett den sÃ¥ lavt du er komfortabel med, og fÃ¸lg med pÃ¥ det faktiske forbruket. Hvis noe krÃ¦sjer er det da god sjanse for at det ble innfÃ¸rt en bug.  Dersom det faktiske forbruket nÃ¦rmer seg limiten pÃ¥ grunn av naturlige grunner - flere requests eller tyngre load+ er det pÃ¥ tide Ã¥ Ã¸ke limiten. Ikke vent til appliasjonen krÃ¦sjer - det skaper en dÃ¥rlig brukeropplevelse.  tip En god tommelfingerregel for requests og limits er fÃ¸lgende: For minne bÃ¸r man profilere applikasjonens gjennomsnittlige minnebruk og doble denne som limit. For CPU trenger man ikke limit, men heller definere en fornuftig request.  Logikken bak dette er at dersom en applikasjon bruker altfor mye minne kan det fÃ¸re til at andre applikasjoner gÃ¥r ned. Dersom en applikasjon bruker mye CPU fÃ¸rer det derimot bare i verste fall til at ting gÃ¥r tregere.  Dette er reglene for ressursbruk i Kubernetes pÃ¥ SKIP  Produktteamet velger selv hva som er naturlig ressursbruk for sine containere, og skal ha et bevisst forhold til hvilke grenser som er satt. Produktteamet skal fÃ¸lge med pÃ¥ ressursbruken over tid, og oppdatere grensene slik at de til enhver tid reflekterer hva applikasjonen faktisk trenger. Resource requests og limits skal settes pÃ¥ alle containere slik at det blir tydelig hva som er forventet ressursbruk. Resource limits skal skal alltid settes hÃ¸yere enn requests, men aldri unaturlig hÃ¸yt. Husk at dette fungerer bÃ¥de som dokumentasjon og som en sikring mot bugs og feilkonfigurasjon. Resource limits skal aldri fjernes permanent, men kan fjernes for debugging. Da skal SKIP-teamet gjÃ¸res oppmerksom pÃ¥ dette. Godt blogginnlegg om korrelasjonen mellom JVMs og Kubernetesâ€™ minnebruk  Kubernetesâ€™ dokumentasjon om ressursbruk  Google Clouds dokumentasjon om â€œcost effective appsâ€ (Merk at Google anbefaler Ã¥ sette limit til det samme som requests - vi setter driftsstabilitet over kostnad, og er derfor uenig i dette.) ","version":"Next","tagName":"h2"},{"title":"Helsesjekker i Kubernetes","type":0,"sectionRef":"#","url":"/docs/kubernetes/helsesjekker-i-kubernetes","content":"","keywords":"","version":"Next"},{"title":"Bakgrunnâ€‹","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kubernetes/helsesjekker-i-kubernetes#bakgrunn","content":" Helsesjekker i kubernetes er en veldig viktig del av mikrotjeneste-arkitekturen, og det er derfor lurt Ã¥ sette seg litt inn i hensikten og funksjonen til de ulike helse-endepunktene man kan konfigurere. Det finnes mange mÃ¥ter Ã¥ konfigurere disse pÃ¥, alt i fra helt egenlagde endepunkter til ferdige rammeverk som eksponerer dette automatisk.  I hovedsak finnes det tre typer prober som kubernetes opererer med:  Liveness probe - Sjekker om containeren kjÃ¸rer og fungerer, hvis ikke sÃ¥ restarter kubernetes containeren    Readiness probe - Brukes for Ã¥ bestemme om en pod en klar for Ã¥ ta i mot trafikk. En pod er klar nÃ¥r alle containere i poden er klare.    Startup probe - Hvis denne er konfigurert sÃ¥ avventer kubernetes med liveness og readiness til dette endepunktet svarer. Dette kan brukes for Ã¥ gi trege containere noe mer tid til Ã¥ starte opp.  Det finnes flere mÃ¥ter Ã¥ sette opp helsesjekker pÃ¥, som f.eks. kommandoer, HTTP-requests, TCP-requests og gRPC-requests. Den aller vanligste mÃ¥ten er Ã¥ konfigurere et endepunkt (f.eks /health, /liveness) i applikasjonen som svarer pÃ¥ HTTP-requests, og spesifisere dette som en del av pod-spesifikasjonen.  Litt mer om helsesjekker generelt: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/  Det er viktig Ã¥ merke seg at man ikkemÃ¥benytte seg av alle disse helsesjekkene, men man bÃ¸r ta et bevisst valg om det er hensiktsmessig eller ikke. Sjekk den lenken her for Ã¥ en oversikt over hva man bÃ¸r gjÃ¸re: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#when-should-you-use-a-liveness-probe  ","version":"Next","tagName":"h2"},{"title":"Skiperatorâ€‹","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kubernetes/helsesjekker-i-kubernetes#skiperator","content":" De aller fleste som har applikasjoner pÃ¥ SKIP benytter Skiperator for Ã¥ forenkle oppsettet rundt applikasjonen. I Skiperator-manifestet kan man konfigurere helsesjekker pÃ¥ samme mÃ¥te man ville gjort for en vanilla-pod i kubernetes. Detaljer rundt dette stÃ¥r i dokumentasjonen for Skiperator . Se under ApplicationSpec og Liveness / Readiness / Startup .  ","version":"Next","tagName":"h2"},{"title":"Sikkerhetshensynâ€‹","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kubernetes/helsesjekker-i-kubernetes#sikkerhetshensyn","content":" Det er noen fallgruver Ã¥ vÃ¦re klar over, sÃ¥ denne siden skal prÃ¸ve Ã¥ gi en oversikt over hvordan man typisk bÃ¸r konfigurere dette pÃ¥ en sikker og god mÃ¥te.  Hvis man ikke konfigurerer applikasjonen sin riktig kan man i verste fall ende opp med Ã¥ eksponere de samme endepunktene som kubernetes bruker internt ut pÃ¥ internett. Et enkelt /health endepunkt som svarer med HTTP 200 OK, gjÃ¸r ikke sÃ¥ stor skade. Et feilkonfigurert management-endepunkt derimot kan eksponere interne miljÃ¸variabler, debug-info og minnedump.  Ta en titt pÃ¥ fÃ¸lgende flytskjema fÃ¸r du gÃ¥r videre, og gÃ¥ til det punktet som gjelder deg  ","version":"Next","tagName":"h2"},{"title":"UndersÃ¸ke hva som eksponeres som standardâ€‹","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kubernetes/helsesjekker-i-kubernetes#undersÃ¸ke-hva-som-eksponeres-som-standard","content":" En veldig vanlig mÃ¥te Ã¥ lÃ¸se helsejsekker pÃ¥ nÃ¥r man bruker Spring Boot er Ã¥ benytte seg av sub-prosjektet Spring Boot Actuator .  For Ã¥ ta det i bruk trenger man bare Ã¥ legge til fÃ¸lgende for Maven-prosjekt (pom.xml)  &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;   Eller dette hvis man bruker Gradle (build.gradle)  dependencies { implementation 'org.springframework.boot:spring-boot-starter-actuator' }   Rammeverket setter automatisk opp endepunktet /actuator/health som en trygg default (gjelder versjon 2 og hÃ¸yere av Spring Boot). NÃ¥r man starter opp en Spring-applikasjon i kubernetes vil Spring Boot Actuator ogsÃ¥ automatisk tilgjengeliggjÃ¸re /actuator/health/liveness og /actuator/health/readiness som man benytte for helsesjekker. For Ã¥ teste disse manuelt kan du legge til management.endpoint.health.probes.enabled=true i application.properties .  Disse endepunktene kan du sÃ¥ bruke i Skiperator-manifestet:  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: some-backend namespace: yournamespace spec: # Ã˜vrig konfigurasjon liveness: path: /actuator/health/liveness port: 8080 failureThreshold: 3 timeout: 1 initialDelay: 3 readiness: path: /actuator/health/readiness port: 8080 failureThreshold: 3 timeout: 1 initialDelay: 5   Det er viktig Ã¥ merke seg fÃ¸lgende notat:  warning Konfigurasjon som management.endpoints.web.exposure.include=* frarÃ¥des ettersom det eksponerer alle endepunkt som er skrudd pÃ¥. I sÃ¥ fall mÃ¥ man passe pÃ¥ Ã¥ sette management.endpoints.enabled-by-default=false og manuelt skru pÃ¥ de man Ã¸nsker Ã¥ bruke.  Ã˜nsker man Ã¥ eksponere ytterligere endepunkt , som f.eks. Ã¥ eksponere /info for Ã¥ presentere informasjon om bygget eller lignende er det tryggere Ã¥ eksplisitt man gjÃ¸re det pÃ¥ denne mÃ¥ten i application.properties :  management.endpoint.info.enabled=true management.endpoint.health.enabled=true management.endpoints.web.exposure.include=health,info   info Savner du ditt rammeverk? Legg det til da vel   ","version":"Next","tagName":"h3"},{"title":"Eksponer endepunktene pÃ¥ dedikert port uten serviceâ€‹","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kubernetes/helsesjekker-i-kubernetes#eksponer-endepunktene-pÃ¥-dedikert-port-uten-service","content":" Her vil det variere litt hvordan man gÃ¥r frem avhengig av om man bruker et rammeverk eller ikke, siden prosessen i containeren mÃ¥ lytte pÃ¥ ekstra port.  FÃ¸rst mÃ¥ man velge seg en port, f.eks. 8081, og sÃ¥ eksponere denne i Dockerfile. I dette eksempelet vil 8080 vÃ¦re applikasjonsporten, og 8081 management/helseporten.  EXPOSE 8080 8081   Deretter mÃ¥ man konfigurere management-porten i application.properties pÃ¥ fÃ¸lgende mÃ¥te:  management.server.port=8081 management.endpoint.info.enabled=true management.endpoint.health.enabled=true management.endpoints.web.exposure.include=health,info   Skiperator-manifestet vil vÃ¦re helt likt, men man insturerer kubernetes til Ã¥ gjÃ¸re helsesjekkene pÃ¥ en annen port.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: some-backend namespace: yournamespace spec: port: 8080 # Ã˜vrig konfigurasjon liveness: path: /actuator/health/liveness port: 8081 failureThreshold: 3 timeout: 1 initialDelay: 3 readiness: path: /actuator/health/readiness port: 8081 failureThreshold: 3 timeout: 1 initialDelay: 5   ","version":"Next","tagName":"h3"},{"title":"Eksponer endepunktene pÃ¥ dedikert port med serviceâ€‹","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kubernetes/helsesjekker-i-kubernetes#eksponer-endepunktene-pÃ¥-dedikert-port-med-service","content":" note For Ã¸yeblikket kan man ikke spesifisere hvilken port man skal tillate trafikk til via skiperator sin accessPolicy  Hvis man har behov for at en annen applikasjon skal kunne nÃ¥ endepunktet pÃ¥ en annen port mÃ¥ man gjÃ¸re ytterligere konfigurasjon. Man bÃ¸r ta en ekstra vurdering pÃ¥ om det er hensiktmessig Ã¥ eksponere denne typen informasjon via actuator-endepunkter.  Sett opp konfigurasjonen pÃ¥ samme mÃ¥te som punktet over, men manifestet vil nÃ¥ inkludere spesifikasjon for ekstra porter og tilgangssstyring.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: some-backend namespace: yournamespace spec: port: 8080 additionalPorts: - name: actuator port: 8081 protocol: TCP # .. Ã¸vrig konfigurasjon liveness: path: /actuator/health/liveness port: 8081 failureThreshold: 3 timeout: 1 initialDelay: 3 readiness: path: /actuator/health/readiness port: 8081 failureThreshold: 3 timeout: 1 initialDelay: 5 accessPolicy: inbound: rules: - application: some-frontend port: 8081 # Ikke mulig akkurat nÃ¥  ","version":"Next","tagName":"h3"},{"title":"ðŸ’¾ Lagring","type":0,"sectionRef":"#","url":"/docs/lagring","content":"ðŸ’¾ Lagring","keywords":"","version":"Next"},{"title":"Databaser","type":0,"sectionRef":"#","url":"/docs/lagring/databaser","content":"","keywords":"","version":"Next"},{"title":"Lokal Postgresâ€‹","type":1,"pageTitle":"Databaser","url":"/docs/lagring/databaser#lokal-postgres","content":" Dersom man Ã¸nsker en lokal postgres tar man kontakt med DBA-ene for Ã¥ bestille opp en server. Da vil man fÃ¥ en Postgres-database og en administratorbruker som man kan bruke til Ã¥ opprette tabeller.  For Ã¥ bestille dette sender man ticket gjennom service desken med hvor mye lagring man trenger og circa hvor mye CPU-kraft man trenger.  NÃ¥r man har fÃ¥tt en database sÃ¥ er det to ting man mÃ¥ gjÃ¸re fÃ¸r man kan ta den i bruk fra en applikasjon pÃ¥ SKIP:  Bestill brannmursÃ¥pning for databasen ved Ã¥ opprette en sak i ServiceNow. F.eks. Jeg Ã¸nsker Ã¥ bestille en brannmursÃ¥pning for en database som skal aksesseres fra SKIP. Det er clusteret â€œatkv1-devâ€ som trenger Ã¥ nÃ¥ â€œXXXX.statkart.noâ€ pÃ¥ TCP port XXXX.Sett opp tilgang til databasen i Kubernetes. I Skiperator gjÃ¸res dette ved hjelp av external accessPolicies. Her mÃ¥ applikasjonen definere at den skal kunne snakke med den eksterne serveren som databasen lever pÃ¥.  accessPolicy: outbound: external: - host: XXXX.statkart.no ip: &quot;XXX.XXX.XXX.XXX&quot; ports: name: db port: 5432 protocol: TCP   ","version":"Next","tagName":"h2"},{"title":"Database i skyâ€‹","type":1,"pageTitle":"Databaser","url":"/docs/lagring/databaser#database-i-sky","content":" Det er mulig Ã¥ bruke database i GCP, men her gjenstÃ¥r det noe utforsking fÃ¸r vi har en god lÃ¸ype for produktteamene. Ta kontakt med SKIP sÃ¥ tar vi en dialog rundt database i sky. ","version":"Next","tagName":"h2"},{"title":"Metrikker og varslinger","type":0,"sectionRef":"#","url":"/docs/metrics","content":"Metrikker og varslinger PÃ¥ SKIP gir vi deg verktÃ¸yene som trengs for Ã¥ mestre den komplekse verden av skybasert teknologi. En av vÃ¥re allierte i denne reisen er Grafana - en kraftig plattform som gir deg visuell innsikt og kontroll over ytelse og helse i sanntid. Grafana er ditt verktÃ¸y til Ã¥ forstÃ¥ og optimalisere dine tjenester pÃ¥ SKIP-plattformen. Med Grafana kan du overvÃ¥ke og analysere data fra en rekke kilder, og dermed fÃ¥ en dypere forstÃ¥else av hvordan dine applikasjoner kjÃ¸rer. Men Grafana gÃ¥r utover bare overvÃ¥king. Det gir deg muligheten til Ã¥ lage skreddersydde dashbords som gir deg akkurat den innsikten du vil ha. Du kan spore trender, identifisere potensielle problemer fÃ¸r de eskalerer, og automatisere reaksjoner pÃ¥ avvikende hendelser. Ved Ã¥ bruke Grafana pÃ¥ SKIP-plattformen fÃ¥r teknologer som deg selv muligheten til Ã¥ gjÃ¸re mer enn bare Ã¥ reagere pÃ¥ hendelser - du kan vÃ¦re proaktiv ved Ã¥ tolke data og ta beslutninger fÃ¸r avviket skjer. Grafana gir deg verktÃ¸yene til Ã¥ ta velinformerte beslutninger, optimalisere ytelse og levere stabile og gode brukeropplevelser. SÃ¥ ta pÃ¥ deg vÃ¥tdrakten og dykk inn i dataene til Grafana pÃ¥ SKIP-plattformen!","keywords":"","version":"Next"},{"title":"URLer og sertifikat for tjenester pÃ¥ SKIP","type":0,"sectionRef":"#","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-pÃ¥-skip","content":"","keywords":"","version":"Next"},{"title":"Interne tjenesterâ€‹","type":1,"pageTitle":"URLer og sertifikat for tjenester pÃ¥ SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-pÃ¥-skip#interne-tjenester","content":" ","version":"Next","tagName":"h2"},{"title":"kartverket-intern.cloudâ€‹","type":1,"pageTitle":"URLer og sertifikat for tjenester pÃ¥ SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-pÃ¥-skip#kartverket-interncloud","content":" Dersom du har en tjeneste som kun skal vÃ¦re tilgjengelig for folk pÃ¥ kartverkets nettverk og VPN og ikke pÃ¥ internett for allmennheten har man flere forskjellige alternativer. Avhengig av bruksomrÃ¥de og hva slags URL man Ã¸nsker seg fungerer dette litt forskjellig, og beskrives i paragrafene under.  For tjenester som skal nÃ¥s pÃ¥ et domene under kartverket-intern.cloud hÃ¥ndteres alt automatisk, inkludert utstedelse og fornying av sertfikater. Det ligger et wildcard record i DNS som hÃ¥ndterer innkommende trafikk, og bruker cluster-leddet i URL-en pÃ¥ Load Balanceren til Ã¥ rute denne inn til riktig cluster. Deretter rutes denne til applikasjonen din basert pÃ¥ URL-konfigurasjonen din i Skiperator.  info Eksempel: minapp.atkv3-prod.kartverket-intern.cloud  ","version":"Next","tagName":"h3"},{"title":"Vanity URL-erâ€‹","type":1,"pageTitle":"URLer og sertifikat for tjenester pÃ¥ SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-pÃ¥-skip#vanity-url-er","content":" note Akkurat nÃ¥ stÃ¸ttes kun kartverket-intern.cloud URL-er pga. en begrensning i utstedelse av sertfikater ( SKIP-1459 ) og en begrensning i lastbalanserer pÃ¥ atkv3-dev cluster ( SKIP-1458 ). Dette skal utbedres.  Dersom du Ã¸nsker et annet hostname enn app.&lt;cluster&gt;.kartverket-intern.cloud er dette mulig, men krever noe mer setup. Den nye URL-en mÃ¥ registreres i DNS og skiperator-applikasjonen din mÃ¥ settes opp til Ã¥ lytte pÃ¥ denne. Utstedelse og fornying av sertfikater vil fremdeles hÃ¥ndteres automatisk av Skiperator.  For Ã¥ sette opp DNS mÃ¥ du gjÃ¸re fÃ¸lgende: FÃ¸rst bestem hvilke URL du vil ha, deretter sett opp et CNAME for denne URL-en til &lt;cluster&gt;.kartverket-intern.cloud . Dersom du Ã¸nsker et CNAME som ligger under kartverket-intern.cloud (for eksempel minapp.kartverket-intern.cloud) kan dette gjÃ¸res av SKIP, for alle andre domener ta kontakt med eier av domenet via bestilling i pureservice. NÃ¥r dette er gjort vil alle spÃ¸rringer som gÃ¥r mot URL-en du har bestemt ende opp host lastbalansereren foran clusteret, og sendes videre inn til Kubernetes.  Neste steg er at Kubernetes sender spÃ¸rringen videre til din applikasjon. Da mÃ¥ du registere URL-en i Skiperator som vanlig under ingresses .  ","version":"Next","tagName":"h3"},{"title":"Cluster-internâ€‹","type":1,"pageTitle":"URLer og sertifikat for tjenester pÃ¥ SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-pÃ¥-skip#cluster-intern","content":" Alle applikasjoner som kjÃ¸rer pÃ¥ SKIP har en kubernetes Service tilknyttet seg. Med denne servicen kan man sende spÃ¸rringer direkte til applikasjonen uten Ã¥ sende trafikken ut av clusteret.  Merk at man her bruker http og ikke https. Trafikken vil allikevel krypteres av service meshet sÃ¥ trafikken vil gÃ¥ over https mellom tjenestene, men fra ditt perspektiv skal du bruke http og trenger ikke tenke pÃ¥ sertfikater.  For Ã¥ sende en spÃ¸rring pÃ¥ denne mÃ¥ten bruker du en URL i fÃ¸lgende format:  http://&lt;appnavn&gt;.&lt;namespacenavn&gt;:port   Merk at Ã¥ snakke med en annen tjeneste pÃ¥ denne mÃ¥ten krever at du har Ã¥pnet opp for at trafikk kan flyte mellom disse tjenestene. I utgangspunktet blir all trafikk blokkert av sikkerhetshensyn. Ã… Ã¥pne opp gjÃ¸res ved Ã¥ spesifisere spec.accessPolicy.outbound.rules i applikasjonen som skal sende spÃ¸rringen og spec.accessPolicy.inbound.rules i applikasjonen som skal motta spÃ¸rringene.  Dersom du har samme tjeneste i sky og Ã¸nsker Ã¥ presisere at du skal gÃ¥ mot samme cluster mÃ¥ man legge pÃ¥ dette i URL. Hvis ikke blir den â€œround robinedâ€ mellom remote og lokal. Eksempel:  http://&lt;appnavn&gt;.&lt;namespacenavn&gt;.svc.cluster.local:port   ","version":"Next","tagName":"h3"},{"title":"Mesh-internâ€‹","type":1,"pageTitle":"URLer og sertifikat for tjenester pÃ¥ SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-pÃ¥-skip#mesh-intern","content":" Dersom du har behov for Ã¥ sende en spÃ¸rring til en annen applikasjon som ikke ligger pÃ¥ samme cluster, men er en del av samme service mesh (for eksempel fra atkv3-prod til atgcp1-prod) sÃ¥ kan dette rutes pÃ¥ nesten samme mÃ¥te som cluster-intern trafikk.  Merk at man her bruker http og ikke https. Trafikken vil allikevel krypteres av service meshet sÃ¥ trafikken vil gÃ¥ over https mellom tjenestene, men fra ditt perspektiv skal du bruke http og trenger ikke tenke pÃ¥ sertfikater.  For Ã¥ sende trafikk til et annet cluster over service meshet sender du en spÃ¸rring i fÃ¸lgende format:  http://&lt;appnavn&gt;.&lt;namespacenavn&gt;.svc.cluster.&lt;cluster&gt;:port   TODO: Hvordan blir networkpolicies for Skiperator apper pÃ¥ mesh?  ","version":"Next","tagName":"h3"},{"title":"Tjenester eksponert pÃ¥ internettâ€‹","type":1,"pageTitle":"URLer og sertifikat for tjenester pÃ¥ SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-pÃ¥-skip#tjenester-eksponert-pÃ¥-internett","content":" Det er to alternativer for Ã¥ eksponere ting pÃ¥ internett. Bruk kartverket.cloud eller en penere â€œvanity URLâ€.  Merk at skiperator-tjenester som eksponeres pÃ¥ andre domenenavn enn subdomener av kartverket-intern.cloud vil automatisk bli Ã¥pnet for trafikk fra internett, men vil ikke vÃ¦re tilgjengelig fÃ¸r DNS konfigureres.  ","version":"Next","tagName":"h2"},{"title":"kartverket.cloudâ€‹","type":1,"pageTitle":"URLer og sertifikat for tjenester pÃ¥ SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-pÃ¥-skip#kartverketcloud","content":" For tjenester som skal nÃ¥s pÃ¥ et domene under kartverket.cloud hÃ¥ndteres alt automatisk, inkludert utstedelse og fornying av sertfikater. Det ligger et wildcard record i DNS som hÃ¥ndterer innkommende trafikk, og bruker cluster-leddet i URL-en pÃ¥ Load Balanceren til Ã¥ rute denne inn til riktig cluster. Deretter rutes denne til applikasjonen din basert pÃ¥ URL-konfigurasjonen din i Skiperator.  info Eksempel: minapp.atkv3-prod.kartverket.cloud  ","version":"Next","tagName":"h3"},{"title":"Vanity URL-erâ€‹","type":1,"pageTitle":"URLer og sertifikat for tjenester pÃ¥ SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-pÃ¥-skip#vanity-url-er-1","content":" Dersom du Ã¸nsker et annet hostname enn app.&lt;cluster&gt;.kartverket.cloud er dette mulig, men krever noe mer setup. Den nye URL-en mÃ¥ registreres i DNS og skiperator-applikasjonen din mÃ¥ settes opp til Ã¥ lytte pÃ¥ denne. Utstedelse og fornying av sertfikater vil fremdeles hÃ¥ndteres automatisk av Skiperator.  For Ã¥ sette opp DNS mÃ¥ du gjÃ¸re fÃ¸lgende: FÃ¸rst bestem hvilke URL du vil ha, deretter sett opp et CNAME for denne URL-en til &lt;cluster&gt;.kartverket.cloud . Dersom du Ã¸nsker et CNAME som ligger under kartverket.cloud (for eksempel minapp.kartverket.cloud) kan dette gjÃ¸res av SKIP, for alle andre domener ta kontakt med eier av domenet via bestilling i pureservice. NÃ¥r dette er gjort vil alle spÃ¸rringer som gÃ¥r mot URL-en du har bestemt ende opp host lastbalansereren foran clusteret, og sendes videre inn til Kubernetes.  Neste steg er at Kubernetes sender spÃ¸rringen videre til din applikasjon. Da mÃ¥ du registere URL-en i Skiperator som vanlig under ingresses .  Dersom du Ã¸nsker Ã¥ ha en URL pÃ¥ toppnivÃ¥ (annentjeneste.no) er ikke CNAME stÃ¸ttet i DNS. Her mÃ¥ man bruke an A record, og her kan man i sÃ¥ fall fÃ¥ IP-adresser med Ã¥ gjÃ¸re et DNS-oppslag pÃ¥ &lt;cluster&gt;.kartverket.cloud .  ","version":"Next","tagName":"h3"},{"title":"HTTPS by defaultâ€‹","type":1,"pageTitle":"URLer og sertifikat for tjenester pÃ¥ SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-pÃ¥-skip#https-by-default","content":" NÃ¥r man eksponerer en applikasjon fÃ¥r man ogsÃ¥ HTTPS automatisk satt opp og eksponert. I dette tilfellet kan man fort spÃ¸rre seg om man burde redirecte HTTP til HTTPS for at alle brukerene skal nyte godt av dette, og svaret pÃ¥ det er i nesten alle tilfeller ja.  For Ã¥ sette opp en slik redirect er det enkleste Ã¥ fÃ¥ applikasjonen som serverer ressurser til klienten (nettleseren) Ã¥ sende en https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Strict-Transport-Security header (HSTS). NÃ¥r en nettleser laster en nettside og oppdager en HSTS header vil den legge denne nettsiden i sin interne cache med et flagg som sier at denne nettsiden alltid skal lastes med HTTPS. Lengden pÃ¥ denne cachen kan settes i flagget, og i de fleste tilfeller vil denne settes ganske hÃ¸yt.  Den eneste tiden hvor dette kan bli problematisk er om det plutselig skjer en endring som gjÃ¸r at nettsiden ikke lenger serveres pÃ¥ HTTPS. For Ã¥ forhindre downgrade attacks vil nettleseren serveres en feilmelding om at nettsiden kun kan Ã¥pnes pÃ¥ HTTPS og det vil ikke vÃ¦re mulig Ã¥ gÃ¥ forbi denne for Ã¥ nÃ¥ HTTP-siden. Men i de aller fleste tilfeller er ikke dette noe Ã¥ bekymre seg over. ","version":"Next","tagName":"h2"},{"title":"ðŸ”­ Observability","type":0,"sectionRef":"#","url":"/docs/observability","content":"","keywords":"","version":"Next"},{"title":"Handy resourcesâ€‹","type":1,"pageTitle":"ðŸ”­ Observability","url":"/docs/observability#handy-resources","content":" Intro to o11y: What is Observability?Bloggen til Charity Majors ","version":"Next","tagName":"h2"},{"title":"Objektlagring med Scality S3","type":0,"sectionRef":"#","url":"/docs/lagring/objektlagring-scality-s3","content":"Objektlagring med Scality S3 I Kartverket har vi en lokalt S3-kompatibel lagringslÃ¸sning ved navn Scality. Denne er mulig Ã¥ fÃ¥ tilgang til, og er godt egnet i tilfellet at dere trengre Ã¥ lagre filer fra en container. Ã… fÃ¥ tilgang til denne krever fÃ¸lgende: SKIP oppretter bruker og lagringsbÃ¸tter for dere i scality-lÃ¸sningen Admin interface Dere fÃ¥r access key og secret","keywords":"","version":"Next"},{"title":"Grafana and GCP","type":0,"sectionRef":"#","url":"/docs/observability/grafana-and-GCP","content":"","keywords":"","version":"Next"},{"title":"Google Cloud Monitoringâ€‹","type":1,"pageTitle":"Grafana and GCP","url":"/docs/observability/grafana-and-GCP#google-cloud-monitoring","content":" It is possible to get metrics from a Google Cloud project by the use of the Grafana data source â€œGoogle Cloud Monitoringâ€.    Through the use of this data source, you will be able to see all metrics that are exposed through different Google Cloud services, such as CloudSQL, BigQuery, CloudKMS, Logging etc. This can then be added to your dashboards and alarms.  ","version":"Next","tagName":"h2"},{"title":"Setting up the data sourceâ€‹","type":1,"pageTitle":"Grafana and GCP","url":"/docs/observability/grafana-and-GCP#setting-up-the-data-source","content":" While the data source is present, it will not scrape all projects in the Kartverket organisation in GCP by default. As of writing this (13 Oct 2023), SKIP does not facilitate this setup in any particular way, but you are free to do it the â€œSKIP wayâ€.  To add your GCP project to the list of projects, simply add the GCP role monitoring.viewer to the Google Service Account grafana-scraper@kubernetes-0dca.iam.gserviceaccount.com. It should look like the below image.    Remember that if you do not have access to editing IAM for your projects by default, you can always elevate your access using JIT Access .  Note that the setup for this may change in the future as this feature is somewhat unexplored as of writing this documentation. ","version":"Next","tagName":"h3"},{"title":"Distributed tracing with Tempo","type":0,"sectionRef":"#","url":"/docs/observability/distributed-tracing-with-tempo","content":"","keywords":"","version":"Next"},{"title":"What is distributed tracing?â€‹","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#what-is-distributed-tracing","content":" In complex (and distributed) systems there are at any time many ongoing parallel processes. Some of these are interlinked or trigger each other. In order to find out which operations that originate from the same request, it is common in many systems to have a so-called Trace ID. With modern distributed tracing this is standardized, and in addition sub-operations (spans) per Trace ID are also supported. When you use a standardized setup to trace applications you also gain access to a large and exciting toolbox.  Further reading:  OpenTelemetryZipkin (interesting from a historical perspective)A general guide to getting started with distributed tracing  ","version":"Next","tagName":"h2"},{"title":"What does SKIP offer?â€‹","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#what-does-skip-offer","content":" As part of our implementation of the LGTM stack, SKIP has chosen to offer Grafana Tempo as as service. This is a component that is fully integrated with the rest of this modern observability stack, and shares the same user interface and authentication as Grafana, Mimir and Loki.  ","version":"Next","tagName":"h2"},{"title":"How do I get started?â€‹","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#how-do-i-get-started","content":" ","version":"Next","tagName":"h2"},{"title":"Instrumentationâ€‹","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#instrumentation","content":" warning A known limitation in the way we have collected trace data is that we up until recently have had no way of excluding certain traces automatically. This means that all Prometheus scrapes (metrics collection) and automatic health checks will also be collected. Now that issue #4628has been implemented, this can finally be rectified. Follow SKIP-1250 for updates to when this is implemented in our setup.  In order to generate, propagate and send traces the application must be instrumented.  Instrumentation can be achieved in several ways, of which 2 are relevant to us: manual and automatic instrumentation.  Manual instrumentation requires the use of a library which knows how a given integration behaves, and which enables it to connect to hooks in that integrations in order to generate new traces and/or spans if those do not already exist.  The other (and recommended) method is to use an automated approach. In the case of Java applications (the only type that has been tested as of now), you will need to bundle a java agent in your Docker image, as well as set up some extra configuration when the application is run (for example through Skiperator).  info Itâ€™s worth mentioning that the Spring ecosystem offers a form of automatic instrumentation via Micrometer Tracing and OpenTelemetry OTLP exporters. Per october 2023 this is still under development and not considered a mature enough solution to utilize in our systems.  ","version":"Next","tagName":"h3"},{"title":"Example Dockerfileâ€‹","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#example-dockerfile","content":" FROM alpine:3.18.3@sha256:c5c5fda71656f28e49ac9c5416b3643eaa6a108a8093151d6d1afc9463be8e33 AS builder ARG OTEL_AGENT_VERSION=1.29.0 # 1. Last ned pÃ¥krevd java-agent RUN apk add --no-cache curl \\ &amp;&amp; mkdir /agents \\ &amp;&amp; curl -L https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/download/v${OTEL_AGENT_VERSION}/opentelemetry-javaagent.jar &gt; /agents/opentelemetry.jar ADD build/distributions/gbok-run*.tar /gbok FROM eclipse-temurin:11-jdk-alpine COPY cert/kartverket_root_ca.crt /usr/local/share/ca-certificates/kartverket_root_ca.crt ENV USER_ID=150 ENV USER_NAME=apprunner RUN apk add --no-cache tzdata \\ &amp;&amp; addgroup -g ${USER_ID} ${USER_NAME} \\ &amp;&amp; adduser -u ${USER_ID} -G ${USER_NAME} -D ${USER_NAME} \\ &amp;&amp; keytool -import -v -noprompt -trustcacerts -alias kartverketrootca -file /usr/local/share/ca-certificates/kartverket_root_ca.crt -keystore $JAVA_HOME/lib/security/cacerts -storepass changeit ENV TZ=Europe/Oslo COPY --from=builder --chown=${USER_ID}:${USER_ID} /gbok /gbok # 2. Kopier inn nedlastet agent COPY --from=builder --chown=${USER_ID}:${USER_ID} /agents /agents USER ${USER_NAME} EXPOSE 8081 ENTRYPOINT [&quot;sh&quot;, &quot;-c&quot;, &quot;/gbok/gbok-run*/bin/gbok-run&quot;]   ","version":"Next","tagName":"h3"},{"title":"Runtime configurationâ€‹","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#runtime-configuration","content":" In order to use the Java agent, it needs to be configured and loaded. Through testing with Grunnboken, we have arrived at the first version of configuration which can be seen here .  When this configuration is done, it is then passed to JAVA_TOOL_OPTIONS like this .  There is currently no inbuilt mechanism in ArgoKit to achieve this. We are open for PRs on this topic if anyone would like to contribute.  ","version":"Next","tagName":"h3"},{"title":"View tracesâ€‹","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#view-traces","content":" Traces can be viewed through our Grafana instance at monitoring.kartverket.dev . From here, choose Explore in the menu and then the correct Tempo data source corresponding to the environment you wish to view traces for.  After that, you have the choice of using the Search (graphical build tool for queries) or TraceQL (manual query specification) tools.  Above: The â€œSearchâ€ tab is active, and fields have been filled through the use of dropdowns.  Above: The â€œTraceQLâ€ tab lets you specify a user-defined query. Here is shown a query for â€œgbok2-serverâ€ traces, filtering out health checks ","version":"Next","tagName":"h3"},{"title":"Logs with Loki","type":0,"sectionRef":"#","url":"/docs/observability/logs-with-Loki","content":"Logs with Loki SKIPâ€™s LGTM stack is set up to automatically collect logs from all applications running in our Kubernetes clusters. There is nothing in particular you as a developer need to configure or set in order to achieve this, apart from ensuring that your application logs to stdout . These are picked up by the Grafana Agent through the PodLogs custom resource, which specifies which namespaces to collect logs for (all of them in this case) and a set of relabeling rules to ensure that we have a common set of labels for use in searching, dashboards and alerting. Logs are collected and stored in Loki, which is backed by an on-premise S3-compatible Scality storage bucket system, one for each cluster. Each Loki instance is defined as a data source in Grafana, which provides the tools for search queries, dashboards and alerting. For an overview of the Explore section as it pertains to Loki, see https://grafana.com/docs/grafana/latest/explore/logs-integration/ . This and other pages outline the features and how to use it efficiently in relatively good detail, so we shall not attempt to reproduce such a guide here, only to point out a few things as they apply to our own setup. By necessity, the default label set is rather limited compared to what some of you might wish. This is because a large selection of labels can be extremely detrimental to performance - see https://grafana.com/docs/loki/latest/get-started/labels/bp-labels/ for an explanation. Hence, it is recommended to use filter expressions instead. You can filter on log lines containing/not containing a given text, regex expression and a host of other possibilities. The search function is also equipped with a JSON parser which makes it easier to filter on the fields you want. You can choose between two modes of searching: typing a query manually, or building a query through Grafanaâ€™s graphical query builder. As long as the query you have built or typed is valid, you can seamlessly switch between the two modes. Above: Using JSON parser to extract fields and filtering on method â€œPOSTâ€","keywords":"","version":"Next"},{"title":"Grafana cheat sheet","type":0,"sectionRef":"#","url":"/docs/observability/grafana-cheat-sheet","content":"","keywords":"","version":"Next"},{"title":"Useful Mimir queriesâ€‹","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#useful-mimir-queries","content":" ","version":"Next","tagName":"h2"},{"title":"Top 20 of metrics with high cardinalityâ€‹","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#top-20-of-metrics-with-high-cardinality","content":" https://monitoring.kartverket.dev/goto/cc_GwW1SR?orgId=1  # Set time range to &quot;Last 5 minutes&quot; topk(20, count by (__name__)({__name__=~&quot;.+&quot;}))   ","version":"Next","tagName":"h3"},{"title":"Top 10 namespaces with overallocated cpu resourcesâ€‹","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#top-10-namespaces-with-overallocated-cpu-resources","content":" https://monitoring.kartverket.dev/goto/6V2jQZJIg?orgId=1  topk(10, sum by (namespace) (kube_pod_container_resource_requests{job=&quot;integrations/kubernetes/kube-state-metrics&quot;, resource=&quot;cpu&quot;}) - sum by (namespace) (rate(container_cpu_usage_seconds_total{}[$__rate_interval])))   ","version":"Next","tagName":"h3"},{"title":"Sum of overallocated cpu for containers by namespaceâ€‹","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#sum-of-overallocated-cpu-for-containers-by-namespace","content":" https://monitoring.kartverket.dev/goto/xF2DlW1SR?orgId=1  sum by (container) (kube_pod_container_resource_requests{job=&quot;integrations/kubernetes/kube-state-metrics&quot;, resource=&quot;cpu&quot;, namespace=~&quot;matrikkel.*&quot;}) - sum by (container) (rate(container_cpu_usage_seconds_total{namespace=~&quot;matrikkel.*&quot;}[$__rate_interval]))   ","version":"Next","tagName":"h3"},{"title":"Daily amount of requests by destination app and response codeâ€‹","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#daily-amount-of-requests-by-destination-app-and-response-code","content":" sum by (destination_app, response_code) ( increase(istio_requests_total{namespace=&quot;&lt;namespace name&gt;&quot;, response_code=~&quot;.*&quot;, source_app=&quot;istio-ingress-external&quot;}[1d]) )   Useful Loki queries ","version":"Next","tagName":"h3"},{"title":"Alerting with Grafana","type":0,"sectionRef":"#","url":"/docs/observability/alerting-with-grafana","content":"","keywords":"","version":"Next"},{"title":"Creating alertsâ€‹","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#creating-alerts","content":" The first step to start adding alerts to your application is to onboard that app to SKIP. Grafana is only used for SKIP, the rest of Kartverket uses Zabbix. Once you have been onboarded and deployed your app to SKIP you can request access to the grafana-alerts repo.  The grafana-alerts repo is designed to be a repo that contains the alerts of all teams and handles deployment of alerts to Grafana. You will get a file which contains the configuration of your alerts in a Terraform format. For example, the file could look like this:  resource &quot;grafana_folder&quot; &quot;MYTEAMNAME_folder&quot; { for_each = local.envs title = &quot;Alerts MYTEAMNAME ${each.key}&quot; } module &quot;MYTEAMNAME_alerts_kubernetes&quot; { source = &quot;../modules/grafana_alert_group&quot; for_each = local.envs name = &quot;kube-state-metrics&quot; env = each.value runbook_base_url = # URL to document describing each alert folder_uid = grafana_folder.MYTEAMNAME_folder[each.key].uid team = { name = &quot;MYTEAMNAME&quot; } alerts = { KubernetesPodNotHealthy = { summary = &quot;Kubernetes Pod not healthy (instance {{ $labels.instance }})&quot; description = &quot;Pod has been in a non-ready state for longer than 15 minutes.\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}&quot; severity = &quot;critical&quot; for = &quot;15m&quot; expr = &lt;&lt;EOT sum by (namespace, pod) (kube_pod_status_phase{phase=~&quot;Pending|Unknown|Failed&quot;, namespace=~&quot;nrl-.*&quot;}) EOT }, # ... more alerts } }   In the above file we create an alert that monitors the health of a pod in all nrl namespaces. Pay attention to the expr field, which is the Prometheus query language PromQL. If you want to learn more about PromQL look at the documentation as well as some examples from the Prometheus documentation and the examples at awesome prometheus alerts .  This is a file that you will be given CODEOWNER access to. This means that you and your team will be able to update this file and review your own changes without involving SKIP. Your team is expected to keep them at a level that verifies the running state of the application.  Updating this file in the GitHub repo will automatically deploy the changes to Grafana.  ","version":"Next","tagName":"h2"},{"title":"Grafana Oncall Alertsâ€‹","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#grafana-oncall-alerts","content":" In addition to Grafana alerts, we have installed a plugin to Grafana called Oncall. This plugin gives us the possibility of adding schedules/shifts and custom alerting behaviour. It also gives your team an overview and a system to handle alerts.    ","version":"Next","tagName":"h2"},{"title":"Integrationâ€‹","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#integration","content":" In order to start using Oncall you need an oncall integration to Grafana. This integration will show up as a contact point in Grafana which can be used in notification policies to route alerts to your integration.  From the integration you can add routes and escalation chains which decides how the integration will notify the team. The standard setup is to send all alerts to a slack channel, and also to a team member on schedule or shared inbox.      In the grafana-alerts repository we have created an oncall_integration module, which you can use to create your teams integration.  ","version":"Next","tagName":"h3"},{"title":"Routesâ€‹","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#routes","content":" In an integration you always have a default route, but you can also have a specified route. A route will decide which escalation chain the integration should use when it receives an alert. For example if you have a critical app that requires 24/7 alerting, you can create a route that checks for certain labels, and if found, it will route the alert to the â€œappdriftâ€ escalation chain.  ","version":"Next","tagName":"h3"},{"title":"Schedulesâ€‹","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#schedules","content":" An Oncall Schedule is a collection of â€œShiftsâ€. In short this means that you can assign a person to a shift, and that person will receive all alerts sent to the Oncall integration for the duration of their shift. In the grafana-alerts repository you can use the oncall_team integration to create both a schedule and escalation chain.    ","version":"Next","tagName":"h3"},{"title":"Escalation Chainsâ€‹","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#escalation-chains","content":" Escalation chains are instructions to Oncall on how to notify you when the connected integration receives an alert. The standard setup here is to contact the assigned person in the set way in Oncall.  The escalation chain below will contact the person which has an assigned shift in Schedule, in the way they have set in Oncall. Usually email or slack mentions.    In Oncall â†’ Users â†’ edit user, you can decide how you want the escalation chain to contact you.    ","version":"Next","tagName":"h3"},{"title":"Terraformâ€‹","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#terraform","content":" A typical Grafana Oncall setup for a team will look like this:  module &quot;team_oncall&quot; { source = &quot;../modules/oncall_team&quot; team_name = &quot;team&quot; use_schedule = true } module &quot;team_integration&quot; { source = &quot;../modules/oncall_integration&quot; integration_name = &quot;team&quot; slack_channel_name = &quot;grafana-oncall&quot; //Not required, replace with your own vaktlag_enabled = false default_escalation_chain_id = module.team_oncall.team_escalation_chain_id }   note The slack channel must already exist in grafana. If you want to use predefined users instead of a schedule, then the users must already exist in Oncall.  ","version":"Next","tagName":"h3"},{"title":"Notification policiesâ€‹","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#notification-policies","content":" You also have to configure notification policies to use your integration. Terraform doesnâ€™t activate the contact point of the integration yet, so this has to be done manually before adding this to terraform(do this by navigating to your integration and activate the contact point). Add code here.  Example:  policy { contact_point = &quot;watchdog&quot; group_by = [&quot;cluster&quot;, &quot;alertname&quot;] matcher { label = &quot;team&quot; match = &quot;=&quot; value = &quot;Vaktlag&quot; } }   ","version":"Next","tagName":"h3"},{"title":"24/4 alertingâ€‹","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#244-alerting","content":" Once you have configured a set of alerts, you might want them to be monitored 24/7. Kartverket provides a solution for this in the form of â€œVaktlagetâ€. Vaktlaget is a team consisting of various people in IT-drift that have a special agreement that allows them to be notified and follow up when an alert fires outside of normal working hours.  The first step for getting your alerts onboarded onto vaktlaget is to maintain a set of alerts that only fire when there is a serious outage. Keep in mind that an alert that fires will potentially wake people in the middle of the night, so it is paramount that this set of alerts donâ€™t contain non-critical or â€œflakyâ€ alerts. These alerts should be given a severity of â€œcriticalâ€ to make them distinct from other alerts.  Once you have done this you need to contact vaktlaget to discuss the alerts you wish to onboard. They will comment on what is important enough to be onboarded and you will end up with a set of alerts that is a neat balance between ensuring the stability of our systems and preserving the mental health of the people on the alert schedule.  After youâ€™ve discussed with vaktlaget you can contact SKIP in #gen-skip to have your alert integration be switched over. When this is done, all alerts labeled with env=prod and severity=critical will be sent to vaktlaget using the following schedule:  The alerts will be sent to your slack channel all dayThe alerts will be sent to appdrift as email, SMS and phone call between 7 and 22The alerts will be sent to infrastrukturdrift as email, SMS and phone call between 22 and 7  You can also create a pull request in grafana-alerts with the vaktlag escalation chain added to your integration:  module &quot;skip&quot; { source = &quot;../modules/oncall_integration&quot; integration_name = &quot;skip&quot; slack_channel_name = &quot;grafana-oncall&quot; //Not required, replace with your own vaktlag_enabled = true vaktlag_escalation_chain_id = module.vaktlag.appdrift_escalation_chain_id default_escalation_chain_id = module.skip.team_escalation_chain_id }   When you later add more alerts to the critical level you also need to discuss with vaktlaget so they can sign off on the new alerts before they are added. ","version":"Next","tagName":"h2"},{"title":"Metrics with Grafana","type":0,"sectionRef":"#","url":"/docs/observability/metrics-with-Grafana","content":"","keywords":"","version":"Next"},{"title":"Backgroundâ€‹","type":1,"pageTitle":"Metrics with Grafana","url":"/docs/observability/metrics-with-Grafana#background","content":" So far we have been able to see metrics through Google Cloud (to be phased out) and Instana (phased out due to not fulfilling its promised potential), but from now on Grafana is the single source of truth in regards to metrics. This is because SKIP has chosen to implement the LGTM stack (Loki, Grafana, Tempo, Mimir), of which Grafana is pretty much an industry standard.  ","version":"Next","tagName":"h2"},{"title":"Getting startedâ€‹","type":1,"pageTitle":"Metrics with Grafana","url":"/docs/observability/metrics-with-Grafana#getting-started","content":" ","version":"Next","tagName":"h2"},{"title":"Adjusting applicationsâ€‹","type":1,"pageTitle":"Metrics with Grafana","url":"/docs/observability/metrics-with-Grafana#adjusting-applications","content":" To be able to expose metrics from your own application, the following steps are required:  Expose Prometheus-format metrics on a known path, for example /metrics . We prefer this to be done on its own dedicated port for health checks, metrics and other administrative purposes. If this is not possible with your current setup, please contact us and we will work with you to find a solution.Change the Skiperator manifest by adding an extra port and enable scraping of Prometheus metrics.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: super-app namespace: team-foo-main spec: image: &quot;kartverket/example&quot; port: 8080 # Definer egen port additionalPorts: - name: management port: 8181 protocol: TCP # Skru pÃ¥ innsamling av metrikker fra den nye porten prometheus: port: management path: &quot;/actuator/prometheus&quot;   ","version":"Next","tagName":"h3"},{"title":"Viewing metricsâ€‹","type":1,"pageTitle":"Metrics with Grafana","url":"/docs/observability/metrics-with-Grafana#viewing-metrics","content":" In order to view metrics and dashboards, visit our Grafana instance at monitoring.kartverket.dev .  Here we offer pre-provisioned dashboards that can be viewed to gain insight into how an environment is performing, as well as more detailed metrics about single applications. In addition, there is a dashboard named JVM (Micrometer) which is a generic dashboard that offers basic information about Java applications, instrumented through Micrometer.  The Explore menu offers the ability to play around with PromQL-formatted queries and view single metrics.  ","version":"Next","tagName":"h3"},{"title":"Provisioning new dashboardsâ€‹","type":1,"pageTitle":"Metrics with Grafana","url":"/docs/observability/metrics-with-Grafana#provisioning-new-dashboards","content":" There are two main ways to provision dashboards in our Grafana instance.  The first way is to set it up through â€œclickopsâ€, that is, using Grafanaâ€™s own Dashboards editor in order to design and save a dashboard suited to your needs.  Building your own dashboard is a topic that is way too out of scope for this documentation, and there are many good guides and tutorials on this out there, so we shall merely include a selection of links to some of the more relevant ones here.  note We strive to continually improve our documentation, so if you have experience with building dashboards and can provide some helpful hints and tricks (or even know of other good guides out there) we would greatly appreciate any contributions to this guide.  https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/create-dashboard/ - simple introduction to building your first dashboard.https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/best-practices/ - it is always worthwile to follow best practices as defined by Grafana themselves.The Four Golden Signals | Google SRE - worth reading as it mentions the most important metrics to focus on when building a monitoring solution for your system.  The second way is to add a pre-made dashboard as a JSON file (either self-made or from Grafanaâ€™s official Dashboards site ) in the skip-dashboards repository. By following the GitOps principle, you get quicker access to your own dashboards in a critical scenario in case an environment needs to be rebuilt. ","version":"Next","tagName":"h3"},{"title":"Recording and alerting rules","type":0,"sectionRef":"#","url":"/docs/observability/recording-and-alerting-rules","content":"","keywords":"","version":"Next"},{"title":"Recording rules in Mimirâ€‹","type":1,"pageTitle":"Recording and alerting rules","url":"/docs/observability/recording-and-alerting-rules#recording-rules-in-mimir","content":" SKIP supports https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/ through Mimir and Grafana Agent. Just define a PrometheusRule object in your applicationâ€™s namespace, and Grafana Agent will pick it up and add it to Mimir.  For more information, see the Prometheus documentation above, or visit https://grafana.com/docs/grafana/latest/alerting/alerting-rules/create-mimir-loki-managed-recording-rule/  ","version":"Next","tagName":"h2"},{"title":"Recording rules in Lokiâ€‹","type":1,"pageTitle":"Recording and alerting rules","url":"/docs/observability/recording-and-alerting-rules#recording-rules-in-loki","content":" TODO ","version":"Next","tagName":"h2"},{"title":"Sikkerhet","type":0,"sectionRef":"#","url":"/docs/security","content":"Sikkerhet SKIP er bygget etter prinsippet om innebygget sikkerhet, slik at det blir lett Ã¥ gjÃ¸re rett. StandardoppfÃ¸rselen skal i utgangspunktet vÃ¦re sikker, med mulighet for produktteamene Ã¥ overstyre der det gir mening for deres applikasjon. Dette er kort fortalt hvordan SKIP balanserer behovet for sikkerhet med autonomi. Et eksempel pÃ¥ dette er prinsippet om Zero Trust i nettverkslaget. All trafikk pÃ¥ Kubernetes er i utgangspunktet stengt, en pod kan ikke snakke med en hvilken som helst annen. Kun om begge tjenestene Ã¥pner for at de kan snakke med hverandre kan trafikken flyte mellom dem. Dette gjÃ¸r produktteamene selv ved Ã¥ sette accessPolicty i sitt Skiperator-manifest. All trafikk mellom podder i Kubernetes er kryptert med mTLS helt automatisk. Det eneste man trenger Ã¥ gjÃ¸re er Ã¥ sende spÃ¸rringer til en annen pod, sÃ¥ krypterer Service Meshet koblingen automatisk. Dersom man trenger Ã¥ eksponere applikasjonen sin til omverdenen kan man konfigurere et endepunkt som applikasjonen skal eksponeres pÃ¥. NÃ¥r dette er konfigurert fÃ¥r man utstedt et gyldig sertifikat som gjÃ¸r at all trafikk krypteres med HTTPS helt automatisk. Dette sertifikatet fornyes ogsÃ¥ automatisk. Dette og mye mer fÃ¸rer til at det er lett Ã¥ gjÃ¸re rett pÃ¥ SKIP.","keywords":"","version":"Next"},{"title":"Real User Monitoring with Faro","type":0,"sectionRef":"#","url":"/docs/observability/real-user-monitoring-with-Faro","content":"","keywords":"","version":"Next"},{"title":"Getting startedâ€‹","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#getting-started","content":" Setting up Faro requires two steps which are explained below:  Installing the SDKConfiguring the SDK  It will also be useful to start by reading the Faro quick start guide . See also the README of the Faro GitHub page for more links to relevant documentation.  ","version":"Next","tagName":"h2"},{"title":"Installing the SDKâ€‹","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#installing-the-sdk","content":" If you use React this is done by running one of the following commands:  # If you use npm npm i -S @grafana/faro-web-sdk # If you use Yarn yarn add @grafana/faro-web-sdk   ","version":"Next","tagName":"h3"},{"title":"Configuring the SDKâ€‹","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#configuring-the-sdk","content":" Import and configure the following options in your appâ€™s entrypoint (main.js or similar).  import { initializeFaro } from &quot;@grafana/faro-react&quot;; initializeFaro({ app: { name: &quot;my_app_name&quot;, environment: getCurrentEnvironment(), }, url: &quot;https://faro.atgcp1-prod.kartverket.cloud/collect&quot;, });   ","version":"Next","tagName":"h3"},{"title":"List of valid options for appâ€‹","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#list-of-valid-options-for-app","content":" \tType\tDescription\tRequired?name\tstring\tThe name of the application as it will appear on dashboards in Grafana\tYes environment\tâ€œlocalhostâ€ | â€œdevâ€ | â€œtestâ€ | â€œprodâ€\tThe environment the frontend is currently running in. This is used to filter data in Grafana dashboards\tYes  ","version":"Next","tagName":"h3"},{"title":"Configuring the SDK with React Router integrationâ€‹","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#configuring-the-sdk-with-react-router-integration","content":" Grafana Faro supports integration with React Router. This gives you events for page navigation and re-renders. See the Faro docs for more information on this.  ","version":"Next","tagName":"h3"},{"title":"Showing the dataâ€‹","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#showing-the-data","content":" When the metrics have started to be gathered, they will be visible in a dedicated Grafana Faro dashboard. This dashboard can be found here .  It is also possible to search for data in the explore view . Useful labels to search for are:  faro_app_namekindenv  ","version":"Next","tagName":"h2"},{"title":"Privacy concernsâ€‹","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#privacy-concerns","content":" note It is up to you and your team to consider the how to use Faro with personal information as outlined in your IP and DPIA  When we send data to Faro, it is mostly metrics that donâ€™t contain any PII . It is possible to include PII like name, IP or anything that is accessible from JavaScript in the SDK, but this is not done by default and requres calling the setUser function on the SDK.  A session ID is sent in to enable de-duplicating events like navigation between pages and ranking top users. This is a randomly generated string and is stored in the userâ€™s browser SessionStorage. Note that even though this is not a cookie, this means a â€œcookie bannerâ€ is required as per the EUâ€™s ePrivacy directive .  As SessionInstrumentation is included by default in the web instrumentation of the JavaScript SDK, disabling it requires invoking the SDK with instrumentations set and omitting the SessionInstrumentation function.  Data is stored on SKIPâ€™s atgcp1-prod cluster, which stores data in Google Cloud Storage europe-north1 region. This region is located in Finland, and is thus within EU. This means no data leaves the EUâ€™s borders which means the storage of the data is compliant with GDPR.  ","version":"Next","tagName":"h2"},{"title":"Rate limitingâ€‹","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#rate-limiting","content":" A rate limit for requests is implemented and is currently set to 50 requests per second. This is shared between all users of Faro, so itâ€™s possible that we eventually reach the limit. Contact SKIP if you start getting queries rejected with HTTP 429 Too Many Requests .  The rate limiting algorighm is a token bucket algorithm, where a bucket has a maximum capacity for up to burst_size requests and refills at a rate of rate per second.  Each HTTP request drains the capacity of the bucket by one. Once the bucket is empty, HTTP requests are rejected with an HTTP 429 Too Many Requests status code until the bucket has more available capacity.  ","version":"Next","tagName":"h2"},{"title":"Tracingâ€‹","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#tracing","content":" Faro supports tracing of HTTP requests, but this is not currently implemented in the collector on SKIP. Contact SKIP if you want this! ","version":"Next","tagName":"h2"},{"title":"ðŸ¥” Troubleshooting","type":0,"sectionRef":"#","url":"/docs/troubleshooting","content":"ðŸ¥” Troubleshooting","keywords":"","version":"Next"},{"title":"Teknologien bak SKIP","type":0,"sectionRef":"#","url":"/docs/tech","content":"Teknologien bak SKIP SKIP er basert pÃ¥ en moderne teknologiplattform bygget rundt Google Cloud, GitOps-prinsipper og innebygget sikkerhet. Med Kubernetes i hjertet av plattformen automatiserer SKIP orkestreringen av containeriserte applikasjoner og sikrer sÃ¸mlÃ¸s distribusjon, oppdatering og skalerbarhet. Google Clouds rike Ã¸kosystem gir tilgang til en rekke tjenester for Ã¥ administrere og beskytte data og applikasjoner pÃ¥ en god mÃ¥te. Argo CD gir gjennomgÃ¥ende innsyn og oppdateringshÃ¥ndtering for Kubernetes-applikasjoner, mens Skiperator gir finjustert kontroll over applikasjonsstyring og konfigurasjon. Med Grafana kan du overvÃ¥ke ytelsen og helsen til dine tjenester i sanntid, slik at du kan identifisere og hÃ¥ndtere eventuelle problemer raskt og effektivt. Utstrakt bruk av GitHub gjÃ¸r det lett Ã¥ dele kode med andre, bÃ¥de internt og eksternt. ByggelÃ¸yper i form av GitHub Actions er fleksible og raske Ã¥ komme i gang med. I tillegg har man tilgang til GitHub Advanced Security for tilgang til blant annet sikkerhetsscanning og sÃ¥rbarhetsrapporter. Med SKIP er det lett for utviklere Ã¥ utforske mulighetene innen moderne skyinfrastruktur. Vi pÃ¥ SKIP-teamet hjelper deg med Ã¥ raskt implementere, administrere og optimalisere skybaserte tjenester, og holde tritt med det stadig skiftende teknologiske landskapet. Velkommen ombord til SKIP!","keywords":"","version":"Next"},{"title":"Migrering fra atkv1 til atkv3","type":0,"sectionRef":"#","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3","content":"","keywords":"","version":"Next"},{"title":"Deployment av applikasjoner til nye clustereâ€‹","type":1,"pageTitle":"Migrering fra atkv1 til atkv3","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3#deployment-av-applikasjoner-til-nye-clustere","content":" Vi har begynt Ã¥ fÃ¥ mange cluster etterhvert, og har derfor valgt Ã¥ gjÃ¸re om litt pÃ¥ mappestrukturen i apps repoet. I dag har vi en struktur som, litt forenklet, ser slik ut:    Vi kommer til Ã¥ havne pÃ¥ en struktur hvor hvert cluster man Ã¸nsker Ã¥ deployere til er representert med en egen mappe under env:    Legg merke til at de gamle mappene kan eksistere sammen med de nye, og det skjer ikke noe med det som ligger i atkv1-dev/prod fÃ¸r mappene fjernes.  ","version":"Next","tagName":"h2"},{"title":"Ã…pninger til tjenester utenfor skipâ€‹","type":1,"pageTitle":"Migrering fra atkv1 til atkv3","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3#Ã¥pninger-til-tjenester-utenfor-skip","content":" For Ã¥ kunne nÃ¥ tjenester som er internt pÃ¥ kartverket, men utenfor skip, eller i et annet cluster mÃ¥ det ogsÃ¥ bestilles Ã¥pninger til disse pÃ¥ nytt, siden atkv3 clusteret er del av en annen brannmursone.  Har man i dag tilgang til et NFS share, eller en database mÃ¥ dette testes, og eventuelle Ã¥pninger bestilles fÃ¸r det kan fungere.  ","version":"Next","tagName":"h2"},{"title":"DNSâ€‹","type":1,"pageTitle":"Migrering fra atkv1 til atkv3","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3#dns","content":" Dev clusteret stÃ¸tter kun adresser under domene atkv3-dev.kartverket-intern.cloud for interne og atkv3-dev.kartverket.cloud for eksterne. Produksjonsclusteret stÃ¸tter atkv3-prod.kartverket-intern.cloud og atkv3-prod.kartverket.cloud. Men det vil i produksjonsclusteret ogsÃ¥ vÃ¦re mulig Ã¥ definere sine egne domener eksternt. Egene vanity URL-er som f.eks nrl.kartverket.no som allerede eksisterer pÃ¥ atkv1 i dag, pekes i dag til lb01.kartverket.no mÃ¥ da pekes over til ny lastbalanserer (atkv3-prod.kartverket.cloud) for Ã¥ kunne nÃ¥s pÃ¥ atkv3.  ","version":"Next","tagName":"h2"},{"title":"Test clusteret finnes ikke lenger!â€‹","type":1,"pageTitle":"Migrering fra atkv1 til atkv3","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3#test-clusteret-finnes-ikke-lenger","content":" Test clusteret finnes ikke lenger, alternativet her er Ã¥ bruke namespace i enten dev eller prod cluster. Har man test-tjenester som skal vÃ¦re tilgjengelig for brukere utenfor kartverket, bÃ¸r dette legges til produksjons clusteret, og dev clusteret stÃ¸tter kun, som nevnt, adresser i domenet atkv3-dev.kartverket.cloud for ekstern tilgang, og atkv3-dev.kartverket-intern.cloud for interne.  ","version":"Next","tagName":"h2"},{"title":"Endringer pÃ¥ ArgoCDâ€‹","type":1,"pageTitle":"Migrering fra atkv1 til atkv3","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3#endringer-pÃ¥-argocd","content":" For Ã¥ kunne administrere flere cluster, med fÃ¦rre ArgoCD instanser har vi valgt Ã¥ konsolidere alle for dev og prod til en instans for prod, og en for dev. Disse legges i sky, sammen med den nye grafana instansen. Dette gjÃ¸r at vi fÃ¥r en prefix for alle argo applikasjoner med clusteret, feks atkv3-KulApplikasjon eller atgcp1-KulApplikasjon, som vil kunne ligge pÃ¥ samme argoinstans, men pÃ¥ to forskjellige clustere. ArgoCD kan nÃ¥s HER.  ","version":"Next","tagName":"h2"},{"title":"Endringer pÃ¥ Grafanaâ€‹","type":1,"pageTitle":"Migrering fra atkv1 til atkv3","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3#endringer-pÃ¥-grafana","content":" Egress fra sky koster penger, mens ingress er gratis. SÃ¥ for Ã¥ slippe Ã¥ hente ut data fra sky cluster til on-prem for Ã¥ vise det i grafana, velger vi Ã¥ vise data onprem via sky. Det gjÃ¸r at logger og metrikker som vises i den nye grafana visualisers i sky, men lagres fremdeles on-prem.  note Hva betyr dette for deg? Logger lagres fremdeles pÃ¥ Kartverket men gÃ¥r via Google Cloud idet man leser dem. Dere mÃ¥ ha et forhold til loggenes innhold og bekrefte at dere forstÃ¥r at de gÃ¥r via Google Cloud og aksepterer det fÃ¸r dere starter migrering til atkv3.  info Ny URL: https://monitoring.kartverket.cloud  ","version":"Next","tagName":"h2"},{"title":"Trafikk-flyt ArgoCD og Grafanaâ€‹","type":1,"pageTitle":"Migrering fra atkv1 til atkv3","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3#trafikk-flyt-argocd-og-grafana","content":"  ","version":"Next","tagName":"h3"},{"title":"Troubleshooting on SKIP","type":0,"sectionRef":"#","url":"/docs/troubleshooting/troubleshooting-on-skip","content":"Troubleshooting on SKIP info This page is under construction and may be updated without warning Troubleshooting an application on SKIP can be daunting. This documentation aims to give readers a rough idea of where to start and what to look for when troubleshooting. It is intended for use both by SKIP team members as well as product team members, and we will take care to specify troubleshooting steps that might need additional access that is only available to SKIP team members or administrators. Relevant links Skiperator code and documentation CLI Cheatsheet for SKIP (may require additional privileges) General checklist when troubleshooting Network/Istio related issues: Network policies - default-deny and others (if applicable).AccessPolicies both outbound and inbound.ServiceEntries+++","keywords":"","version":"Next"}],"options":{"languages":["en","no"],"id":"default"}}