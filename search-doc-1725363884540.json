{"searchDocs":[{"title":"Hybrid Kubernetes in production pt. 1","type":0,"sectionRef":"#","url":"/blog/hybrid-kubernetes-in-production-part-1","content":"","keywords":"","version":null},{"title":"So why a hybrid cloud?‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#so-why-a-hybrid-cloud","content":"   Were you to take the time machine back a few years, you would see Kartverket as a traditional enterprise with a lot of knowledge and experience in running on-premise workloads. This knowledge served us well, but also slightly held us back in terms of our imagination. We knew that there had to be a better way, but our enterprise was simply not mature enough to adopt a pure cloud strategy. The fear of the unknown cloud weighed heavily on many people, and therefore few people wanted to take the risk of moving to the cloud.  This is something we've worked on for a long time, and still are. After a long time of working with the stakeholders in the organization, we eventually built a cloud strategy, which in simple terms stated that we would prefer SaaS-products over hosting things ourselves, and that we would gradually move our workloads to the cloud.  This cloud strategy however, which cleared up a lot of blockers, came too late for us on SKIP. At that point we had already done most of the work on our on-premise platform, building on the assumptions the organization held at the time, which was that we met our needs through existing infrastructure and that using public cloud had disqualifying cost and compliance implications. For SKIP it was therefore full steam ahead, building the on-prem part first, then adding the hybrid and cloud part later.  It's not like we would have ended up with a pure cloud setup in any case, though. If you're at all familiar with large enterprises, you will know that they are often very complex. This is also true for Kartverket, where we have a lot of existing systems that are not easy to move to the cloud. We also have a lot of systems that are not suitable for the cloud, mostly because they are designed to run in a way that would not be cost effective in the cloud. In addition we have absolutely massive datasets (petabyte-scale) that would be very expensive to move to the cloud.  Because of these limitations, a pure cloud strategy is not considered to be a good fit for us.  A hybrid cloud, however, can give us the scalability and flexibility of the cloud, while still allowing us to run some of our systems on-prem, with the experience being more or less seamless for the developers.  ","version":null,"tagName":"h2"},{"title":"Why we chose Anthos‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#why-we-chose-anthos","content":" After some disastrous issues with our previous hybrid cloud PoC (that's a whole story in itself) we decided to to look at what alternatives existed on the market. We considered various options, but eventually decided to run a PoC on Anthos. This was based on a series of conditions at the time, to name a few:  We had a decent pool of knowledge in GCP compared to AWS and Azure at the timeSome very well established platform teams in the public sector were also using GCP, which meant it would be easier to share work and learningsAnthos and GCP seemed to offer a good developer experience, which for us as a platform team is of paramount importanceA provider like Google is well established in the cloud space (especially Kubernetes), and would have a fully featured, stable and user friendly product  SKIP ran the Anthos PoC over a few months, initially as an on-prem offering only. Drawing on the knowledge of internal network and infrastructure engineers, this took us all the way from provisioning clusters and networking, to iterating on tools and docs and finally onboarding an internal product team on the platform. Once we felt we had learned what we could from the PoC, we gathered thoughts from the product team, infrastructure team and of course the SKIP platform team.  The results were unanimous. All the participants lauded the GCP user interfaces that allowed visibility into running workloads, as well as the new self-service features that came with it. Infrastructure engineers complimented the installation scripts and documentation, which would make it easier to keep the clusters up to date.  Based on the total package we therefore decided to move ahead with Anthos. To infinity and beyond! üöÄ  ","version":null,"tagName":"h2"},{"title":"What is Anthos anyway?‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#what-is-anthos-anyway","content":"   Anthos is Google's solution to multicloud. It's a product portfolio where the main product is GKE (Google Kubernetes Engine) on-premise. Using GKE on-prem you can run Kubernetes clusters on-premise and manage them from the same control plane in Google Cloud, as if they were proper cloud clusters.  In fact, Anthos is truly multi-cloud. That means you can deploy Anthos clusters to GKE and on-prem, but also AWS and Azure. On other cloud platforms it uses the provider's Kubernetes distribution likeAKS, but you can still manage it from GKE alongside your other clusters.  In addition to GKE, the toolbox includes:  ","version":null,"tagName":"h2"},{"title":"Anthos Service Mesh (ASM)‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#anthos-service-mesh-asm","content":" A networking solution based on Istio. This is sort of the backbone of the hybrid features of Anthos, as provided you've configured a hybrid mesh it allows applications deployed to the cloud to communicate with on-premise workloads automatically and without manual steps like opening firewalls.  All traffic that flows between microservices on the mesh is also automatically encrypted with mTLS.  ","version":null,"tagName":"h3"},{"title":"Anthos Config Managment (ACM)‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#anthos-config-managment-acm","content":" A way to sync git repos into a running cluster. Think GitOps here. Build a repo containing all your Kubernetes manifests and sync them into your cluster, making cluster maintenance easier.  ACM also includes a policy controller based on Open Policy Agent Gatekeeper (OPA) which allows platform developers to build guardrails into developers' workflows using policies like &quot;don't allow containers to run as root&quot;.  ","version":null,"tagName":"h3"},{"title":"Anthos Connect Gateway‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#anthos-connect-gateway","content":" The connect gateway allows developers to log on to the cluster using gcloudand kubectl commands, despite the cluster potentially being behind a firewall. From a user experience standpoint this is quite useful, as devs will be logged in to GCP using two factor authentication, and the same strong authentication allows you to access kubernetes on-premise.  Connect Gateway also integrates with GCP groups, enabling RBAC in Kubernetes to be assigned to groups instead of manually administered lists of users.  Currently the connect gateway only supports stateless requests, for examplekubectl get pods or kubectl logs (including -f). It does not supportport-forward, exec or run, which can be a bit annoying.  ","version":null,"tagName":"h3"},{"title":"Summary‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#summary","content":" As you can see, the above tools gives us a lot of benefits.  Combined with the power of Google Cloud and Terraform, they give us a good combination of flexibility through cloud servicesEase the maintenance by using the tools that Anthos and Terraform supply usEases the compliance and modernization burden by allowing a gradual or partial migration to cloud, allowing parts to remain on-premise while still retaining most of the modern tooling of the cloud  That's it for now! üôÇ We'll be back with more details on how we run Anthos as well as the pros and cons we've seen so far in the coming weeks. Stay tuned!  Disclaimer - Google, GKE and Anthos are trademarks of Google LLC and this website is not endorsed by or affiliated with Google in any way. ","version":null,"tagName":"h2"},{"title":"20 teams on SKIP: What we've learned along the way","type":0,"sectionRef":"#","url":"/blog/20-teams-on-skip","content":"","keywords":"","version":null},{"title":"Principles matter‚Äã","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#principles-matter","content":" When you set out to create something new, you have the privilege of setting some standards that encourage best practices. While this is possible to do for an existing system, in practice it will mean a lot of work to get to the point where you're able to enforce these standards. It's much easier to start with a clean slate.  For our platform, we decided on a set of principles that we wanted to follow. Some of these are:  Stateless: Our clusters are stateless, which means that we can easily replace them if something goes wrong. All configuration is held in a GitOps repository and all state is held in external systems like managed databases, object storage, etc. This significantly reduces operational complexity and recovery time. If a cluster fails we can easily replace or revert it by applying the configuration from the GitOps repository without worrying about losing state, or doing time-consuming data recovery operations.Ownership: For each application, there is a clear owner. This owner is responsible for the application and maintains and supports it. This way we're able to avoid the &quot;tragedy of the commons&quot;, where no one is responsible for an application. If an app has unclear or short-term ownership, you simply don't get to use the platform. We're not an orphanage.Financing: You use the platform? You also need to pay for its continued support and development. We're working towards a chargeback model where your department is billed for the resources they use as a way to ensure that the platform is sustainable. Until this is ready, we expose the costs of the resources used by each team and then negotiate with the departments on how to cover these costs, but this is time-consuming work.Secure by default: We enforce security best practices by default. Examples of this are zero trust networking with Network Policies, where no app can talk to another without explicitly allowing this. Some applications will need to opt out of some of these defaults, and they can do so by altering their configuration. But the defaults are secure, which is especially useful for teams that are new to Kubernetes.  All teams that are onboarded on SKIP are given an introduction to these principles and are expected to follow them. This means that being able to use the modern platform is contingent on the teams being able to prioritize modernizing their applications and working in sustainable ways, which helps push for positive change.  ","version":null,"tagName":"h2"},{"title":"Encourage collaboration‚Äã","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#encourage-collaboration","content":" It's easy for a product team to ask the platform team for help when they're stuck. We're always happy to help, but we also have a heavy workload of exciting things we're working on. Therefore it's much better when platform users can help each other, as this facilitates collaboration and learning. This is why we highly encourage teams to help each other out - to build a community around the platform.  In practice this is done through a single Slack channel where all teams that are using the platform are invited. This is a great place to ask questions, share experiences, and learn from each other - and it's a place where all new features and changes are announced. We used to have many different channels for different teams, but we found that this was not as effective for building a community as a channel where everyone can help each other out.  And a final tip: As a platform developer, sometimes it's better to wait a little while before responding to questions in these channels to allow the community to help each other out before you jump in and help.  ","version":null,"tagName":"h2"},{"title":"Make time for innovation‚Äã","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#make-time-for-innovation","content":" It's easy to get bogged down in the day-to-day work of keeping the platform running. This is why it's important to set aside time for innovation, this is something we take very seriously.  On SKIP we have dedicated innovation days where we work on new features, improvements, and other things that we think will make the platform better. This is an extraordinarily successful initiative, and we've seen many great features come out of these days. It's also a great way to build team morale and to build a culture of learning and innovation.  In practice we have two days in a row of dedicated innovation work every other month. We used to have one day every month, but we found that this was not enough time to really get into the flow of things so we started doing two days every 2 months, which worked better. We also have a rule that you can't work on anything that's on the roadmap, as this is work that we're already going to do. This is a great way to get new ideas and to work on things that might not otherwise get done.    There's a little bit of structure around these days, but not too much.  First, it is understood by everyone that these days are for things that are &quot;useful for Kartverket&quot;. This means that you can't work on your own pet project, but it's vague enough that you can work on pretty much anything that you think will be useful for the organization.  Then, a week before the innovation day we will have a &quot;pitching session&quot;, where everyone who has an idea can pitch it to the rest of the team. This is a great way to get feedback on your idea and to get others to join you in working on it.  Finally, we have a &quot;show and tell&quot; session at the end of the last day where everyone shows what they've been working on. This way we can share our experiences and discuss if this work can be improved and put into production. We encourage everyone to show something, even if it's not finished or you did video lessons, as this creates discussion and further ideas.    There's plenty of examples of features that are results of work done on these days. On-premise Web Application Firewall with Wasm, Grafana features, open source tools like Skiperator andSkyline as well as this very website!  No one has time to prioritize innovation, and we're no different. But we prioritize it anyway, because we know that it's important to keep improving and to keep learning.  ","version":null,"tagName":"h2"},{"title":"Communication is key‚Äã","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#communication-is-key","content":"   Unfortunately a lot of infrastructure teams don't prioritize communication very well. This is a mistake. Communication is key to building a successful platform.  Your users exist in the context of all the platform features that you have shipped and the changes you will ship in the future. Not informing them and keeping them up to date with what's going on is a surefire way to lose their trust and to make them unhappy.  It starts with simply informing users of the new features that ship. This can be done through a Slack channel, a newsletter, a blog or a town hall meeting. We use a combination of all of these, but the most important thing is that you inform your users of what's coming. An added benefit of this is helps push adoption of new features and excitement around the platform by showcasing innovation.  The next step is informing users on what will ship and when. This will help users plan their work and to know what to expect, but it also helps users feel involved when they see their requests being planned. This can be done through a roadmap, a technical forum, or a blog. We use a combination of all of these, but the easiest way to do this is to have a roadmap that you keep up to date on a regular basis.  Now for the hard part: When things go wrong, you need to communicate this as well. Product teams will want to know when their applications are affected by outages or other issues, and they will want to know what you're doing to fix it. This can be done through a status page, a Slack channel, or postmortems. Again, we use a mix of these so that we can reach as many users as possible at the right time.  Do these things and you will have happy users that feel informed.  ","version":null,"tagName":"h2"},{"title":"Branding is important‚Äã","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#branding-is-important","content":"   Do you think Spotify would be as successful if it was called &quot;Music Player&quot;? Do you think Apple would be as successful if it was called &quot;Computer Company&quot;? Of course not. Branding is important. It builds a sense of identity and community around your platform.  This is especially important for a platform team, as you're not just building a product, you're building a community. You want your users to feel like they're part of something bigger, and you want them to feel excited to use the platform.  When you're starting out, you want to drive adoption. Here a brand really helps as it's easier to talk about a good brand in a positive way. It's also easier to get leadership buy-in when you have a strong brand.  This holds true when you're more established as well. When you grow larger than your ability to talk to everyone, a brand helps you communicate your values and intent to your users, which will drive organic growth from teams that want to work with you.  A minimum viable brand is a logo, a name, and a color scheme. This is something you should think deeply about, as it's something that will stick with you for a long time. After this you can think about a website, merchandise like stickers and t-shirts, and a mascot. These things are not necessary, but they can help build a sense of identity and community around your platform.  ","version":null,"tagName":"h2"},{"title":"Using the cloud is a long journey‚Äã","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#using-the-cloud-is-a-long-journey","content":" As a platform team, it's our responsibility to push for modern, user-friendly and secure solutions. This generally means using public cloud solutions like Google Cloud Platform. But for most organizations, pushing this narrative incurs significant friction and to some extent fear due to legal and cost concerns. This is understandable, as the known is always more comfortable than the unknown, and it's a view that's hard to change.  This is why it's important to take a long-term view on this. You're not going to move everything to the cloud overnight, and you're not going to convince everyone to get on board with this idea overnight. It's a long journey, and you need to be patient and persistent.  We've spent years pushing for the cloud, and we're still not there. You're going to have to participate in many (many!) meetings, and you're going to have to fight for every little thing over and over again. But it's necessary. Once everyone has a clear understanding of the risks and how to mitigate them, you will be able to formulate a document guiding the organization's teams on how to get to the cloud from a compliance point of view.  If you asked me for any recommendations on how to get to the cloud as easily as possible, it would be to first get leadership buy-in across the organization. This is important, as it will make any large initiative like cloud migration easier. After this and a competent platform team is in place, you can start pushing for the cloud technologies and eventually cloud migration. Here you need to talk directly with the legal team, not via other people. Have representatives of the platform team sit down with the lawyers and talk through the risks and how to mitigate them. This is the only way you can combine the technical and legal aspects of this work. Working in silos and not talking to each other is a surefire way to fail.  ","version":null,"tagName":"h2"},{"title":"Autonomy and platform as a product‚Äã","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#autonomy-and-platform-as-a-product","content":" Your platform is a product, and so you need to work as a product team. This means continuously improving your product, listening to your users, and building the features that they need.  Research-based literature like &quot;Team Topologies&quot; establishes the importance of autonomous teams in modern organizations. Traditional top-down organizations are just not going to be able to have as close of a relationship with their stakeholders as a team that is able to proactively understand the needs of their users and make their own decisions that push continuous improvement of their products. This is why it's important, even for infrastructure teams, to be able to own their roadmap and make decisions on what to build when.  As a team you're obviously limited to the amount of resources you have and not able to do everything, so understanding the needs of your stakeholders and prioritizing them is essential. You need to do research to know the needs of your users; sometimes requests don't align well with the actual needs. Just because someone asks loudly for something, doesn't mean it's the right fit for your platform. Saying yes to everything does not result in a good product. Dare to challenge assumptions and ask why.  ","version":null,"tagName":"h2"},{"title":"Abstractions save time‚Äã","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#abstractions-save-time","content":" It should go without saying that a platform team's job is to make tools that make product teams' jobs easier. But it really can't be said enough. The better the tooling you provide, the less you have to do support. This is a win-win for everyone.  When building tools, think about how you can abstract away complexity. This can be done in many ways, but we've had great success building an operator that abstracts away the complexity of managing applications on Kubernetes. The operator is called Skiperator and makes deploying applications on Kubernetes as easy as writing a configuration manifest.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: namespace: sample name: sample-two spec: image: nginxinc/nginx-unprivileged port: 80 replicas: 2 ingresses: - foo.com - bar.com   The key takeaway here is that abstractions like Skiperator are designed to speak the language of the user. There is no mention of NetworkPolicies or Istio VirtualServices in the configuration, as these are things that the user generally doesn't have any knowledge of. Instead, the user can specify things like &quot;I want to expose this service to the internet&quot; or &quot;I want to run this job every day at midnight&quot;. This simplifies the user experience of Kubernetes, which is a complex system, and makes it easier for users to get started.  Work smarter not harder.  ","version":null,"tagName":"h2"},{"title":"Build forward- and backwards compatibility‚Äã","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#build-forward--and-backwards-compatibility","content":" We've had multiple experiences where we've weighed our options and decided to make a breaking change. Just recently we asked our users to migrate their apps from one cluster to another in order to improve the architecture of the platform. Multiple options were considered, but in the end the scale of the changes meant that upgrading the clusters in-place would not be practical, so we commissioned new clusters with the new architecture and asked users to migrate their apps.  In our case, we had a simple way to migrate, only requiring moving a config file from one directory to another to make the change. But even so, this was a time consuming process for our users, and a laborious process for us to support. This is because even though the change was simple, it was still a change that required testing and validation, and it was a change that was not necessarily the highest priority for the teams that were asked to make it. So even though the change was simple, it took months.  If you ask your users to make changes to their applications, you're asking a team that is already busy to do more work. Any changes you ask them to make will take time, as it would not necessarily be the highest priority for them. Therefore avoiding breaking changes should be a primary goal, so wherever possible building in forward and backwards compatibility by inferring as much as possible from the existing configuration is a good thing.  When building operators, don't change or remove fields that are in use. Use default values for new fields, and use lists of objects instead of raw values like lists of strings as they are easier to extend.  ","version":null,"tagName":"h2"},{"title":"Documentation is key‚Äã","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#documentation-is-key","content":" One thing we keep hearing from our users is the need for more and better documentation. This is understandable. When you're using a platform, you don't want to have to ask for help all the time - you want to be able to discover platform features and implement them yourself with the support of good documentation.  The point here is that as a platform team you need to prioritize documentation. A task is not done until it has been documented. This way announcing new features will always include a link to the documentation where users can dive deeper into the feature and how to use it, like the example below.    The bigger challenge here is preventing documentation from going stale. It's too easy to forget about updating documentation to reflect changes in the code. Here we can share a few tips from our experience:  First, the obvious way to keep docs up to date is to allocate time to update them. One way we do this is that a few times a year we will do a documentation grooming session where we huddle together and review documentation, rewriting it when we find out of date information.  A more interesting way to keep docs up to date is changing how you respond to questions. Instead of answering questions immediately, we should be asking ourselves: &quot;How can we make sure that this question never gets asked again?&quot;. In our case we spend some time to write documentation or improve existing documentation and reply with a link to the documentation page. This is a triple win, as you will now have more updated documentation, save time in the future by being able to refer to the improved docs instead of writing a lengthy response and the user will now know where to look for answers.  ","version":null,"tagName":"h2"},{"title":"Learn from others‚Äã","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#learn-from-others","content":" When building a platform you'll quickly learn that you don't have all the answers. You might discuss how to implement a feature with your team, but you might not have the experience to know what works well in this context. When you get into this situation, an outside perspective can be crucial to avoid making costly mistakes.  One great advantage of working in the public sector is that we can ask other public sector platform teams for advice and learn from their experiences. We can also share our experiences with others, which is usually interesting. Invest some time in building these relationships of mutual benefit.    I also want to give special credit to Hans Kristian Flaatten and the Public PaaS network here. Having a shared forum to discuss platform issues is a strong asset and helps the Norwegian public sector get ahead and stay competitive.  Even if you work in the private sector, you can still learn from other organizations. Honestly, if you want to learn from someone's experiences it never hurts to ask. Teams generally want to help each other out, and it's usually possible to make a trade of some sort. I suggest to offer to give a talk on your experiences and ask if they can do the same. It's a win-win for both parties.  ","version":null,"tagName":"h2"},{"title":"Conclusion‚Äã","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#conclusion","content":" You may think building a platform is mostly technology, and we've written a lot about technology in previous blog posts. But it's important to remember that building a platform is also about building a community, and communities have expectations and needs that go beyond technology. This is a strength, and not a weakness, as if you're able to inspire and motivate your users you will be able to build a platform that is sustainable and that drives positive change in your organization.  Best of luck in your endeavors! ","version":null,"tagName":"h2"},{"title":"Hybrid Kubernetes in production pt. 2","type":0,"sectionRef":"#","url":"/blog/hybrid-kubernetes-in-production-part-2","content":"","keywords":"","version":null},{"title":"Installation and upgrades‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#installation-and-upgrades","content":"   We have been early adopters of Anthos, so when doing the install we did not have options for controlplane architecture. We wanted to use existing underlying VMware infrastructure, so the nodes in our clusters are VMs, provisioned by scripts provided by Google. Our cluster is installed withkubeceptioncontrolplane architechture, this no longer the only, or recommended way. The recommended model is Controlplane V2, where the controlplane nodes for the user cluster are in the user cluster itself.  In the kubeception model, Kubernetes clusters are nested inside other Kubernetes clusters. Specifically, the control plane of the user clusters runs in an admin-cluster. For each on-premise cluster created, a new set of nodes and a namespace are created in the admin cluster.  To install and make changes to the admin cluster, an admin workstation is required, which must be located in the same network as the admin cluster. All configurations are done using a CLI tool called gkectl. This tool handles most cluster administration tasks, and the cluster specific configuration is provided in YAML files.  Our cluster setup is more or less static, and most cluster administration tasks involve upgrading or scaling existing clusters. The SKIP team has a cluster referred to as ‚Äúsandbox‚Äù, which is always the first recipient of potentially breaking changes. After testing in sandbox, we'll deploy changes to both development and test environments, and if nothing breaks, we roll out the changes to our production environment. This is mostly done outside work-hours, although we have not experienced downtime during cluster upgrades. Here is the general workflow for upgrading:  Upgrade your admin workstation to the target version of your upgrade.From your admin workstation, upgrade your user clusters.After all of the user clusters have been upgraded, you can upgrade your admin cluster from the admin workstation.  We have tried using Terraform where possible to simplify the setup. This can not be done in the same way for clusters using the kubeception model. When we migrate to Controlplane V2 however, clusters can be managed via GCP, and we can finally start using terraform for our on-premise cluster config in the same way as for our GKE clusters, and GCP configuration in general.  ","version":null,"tagName":"h2"},{"title":"GCP integration‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#gcp-integration","content":" When working with an on-premise Anthos cluster, some of the nice-to-have features of a standard GKE cluster have been lost. However, recently Anthos on VMware clusters have gradually received more and more features compared to GKE clusters.  ","version":null,"tagName":"h2"},{"title":"IAM and Groups‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#iam-and-groups","content":" Since we were early adaptors of Anthos, we had to endure not being able to delegate clusterroles to IAM groups, and had to add single users to clusterrole/rolebindings in Kubernetes. This was not a huge problem for us, since we were working with a very limited number of teams and devs, but it was apparent that this was not going to scale well. Luckily we got support for groups before it was a problem, and our config files went from containing way too many names and email addresses, to only containing groups.  Our Google Workspace receives groups and users from our Microsoft Active Directory. Groups are initially created either in Entra ID, or on our local Domain Controllers, and at set intervals changes are pushed to Google Workspace.Role-based access control (RBAC) based on membership in these groups was needed. We wanted to manage this through Terraform, and created a repo with where we store and configure our entire IAM configuration. Since we have had growing adoption of Kubernetes and public cloud in our organization, more teams, projects and apps have been onboarded to SKIP, and this IAM repo has grown. We've tried to simplify the structure more than once, but since this is a problem not affecting dev teams, we have chosen to prioritize other tasks.  ","version":null,"tagName":"h3"},{"title":"Workloads‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#workloads","content":" All clusters created in in Anthos can be viewed from the GCP console, and theConnect gatewaymakes it possible to do management from the console (or via kubectl) as well. The GCP console can be used to get information about, or manage the state of the cluster, workloads and resources present. This is a web GUI, part of the GCP console, and not as snappy as cli-tools, but still usable, and intuitive to use.  This view shows workloads running in the argocd namespace. All workloads displayed here can be clicked, and explored further.  When accessing the cluster via the Connect gateway there are some limits. The Connect gateway does not handle persistent connections, and this makes it impossible to do exec, port-forward, proxy or attach. This is not a problem for a production environment, where containers should never be used in this way. But for a dev, or sandbox environment, this is a bit of a pain-point.  This issue should be partially fixed in Kubernetes 1.29 and should be completely resolved in Kubernetes 1.30.  ","version":null,"tagName":"h3"},{"title":"Service Mesh‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#service-mesh","content":" A Service Mesh in Kubernetes is an infrastructure layer that manages communication between services. We are using Anthos Service Mesh (ASM), which is based on Istio and nicely integrated with the GCP console. It's easy to get an overview of services, the connection between them, and what services are connected to either our internal or external gateways. This can be displayed in a Topology view, or if you click on a service, you'll get a more detailed drilldown.  A snippet of services running in our sandbox cluster.  When we deploy services to our cluster we create almost all Kubernetes and service-mesh resources with our custom operator;Skiperator. This operator configures the resources to fit our setup, and applies &quot;best practices&quot; the easy way. This has been one of the great success stories in SKIP, and Skiperator is in continuous development.  ","version":null,"tagName":"h3"},{"title":"Deployment‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#deployment","content":" Deployment is a very interesting subject when it comes to Anthos. As a platform team, it is our job to make sure that deployment is as quick and convenient as possible for the product teams. This ambition has led us to iterate on our processes, which has finally led us to a solution that both we and the developers enjoy using.  ","version":null,"tagName":"h2"},{"title":"Iteration 1 - Terraform‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#iteration-1---terraform","content":" When we first started out with Anthos, we had a very manual process for deploying applications. A service account was provisioned in GCP, which allowed the developers to impersonate a service account in Kubernetes, which in turn allowed them to deploy apps using Terraform. This approach worked, but had a decent amount of rough edges, and also would fail in ways that was hard to debug.  With this approach the developers would have to manage their own Terraform files, which most of the time was not within their area of expertise. And while SKIP was able to build modules and tools to make this easier, it was still a complex system that was hard to understand. Observability and discoverability was also an issue.  Because of this we would consistently get feedback that this way of deploying was too complicated and slow, in addition handling Terraform state was a pain. As a platform team we're committed to our teams' well being, so we took this seriously and looked at alternatives. This was around the time we adopted Anthos, so thus Anthos Config Managment was a natural choice.  ","version":null,"tagName":"h3"},{"title":"Iteration 2 - Anthos Config Managment (ACM)‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#iteration-2---anthos-config-managment-acm","content":"   ACM is a set of tools that allows you to declaratively manage your Kubernetes resources. Here we're mostly going to talk about Config Sync, which is aGitOps system for Kubernetes.  In a GitOps system, a team will have a Git repository that contains all the Kubernetes resources that they want to deploy. This repository is then synced to the Kubernetes cluster, and the resources are applied.  This can be likened to a pull-based system, where the GitOps tool (Config sync) watches the repo for changes and pulls them into the cluster. This is in contrast to a push-based system, where a script pushes the changes to a cluster. It is therefore a dedicated system for deployment to Kubernetes, and following the UNIX philosophywhich focuses on doing that one thing well.  Using this type of a workflow solves a lot of the issues around the Terraform based deployment that we had in the previous iteration. No longer do developers need to set up a complicated integration with GCP service accounts and impersonation, committing a file to a Git repo will trigger a deployment. The Git repo and the manifests in them also works as a state of truth for the cluster, instead of having to reverse engineer what was deployed based on terraform diffs and state.    It started well, however we soon ran into issues. The system would often take a long time to reconcile the sync, and during the sync we would not have any visibility into what was happening. This was not a deal breaker, but at the same time this was not a particularly good developer experience.  We also ran into issues with implementing a level of self-service that we were satisfied with. We wanted to give the developers the ability to provision their own namespaces, but due to the multi-tenant nature of our clusters we also had to make sure that teams were not able to write to each others' namespaces. This was not a feature we were able to implement, but luckily our next iteration had this built in, and we'll get back to that.  The final nail was the user interface. We simply expected more from a deployment system than what ACM was able to provide. The only view into the deployment was a long list of resources, which to a developer that is not an expert in Kubernetes, was not intuitive enough.  ","version":null,"tagName":"h3"},{"title":"Final iteration - Argo CD‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#final-iteration---argo-cd","content":"   This finally brought us to our current iteration. We had heard about Argo CD before, but initially we were hesitant to add another system to our stack. After ACM had introduced us to GitOps and we looked deeper into Argo CD, it was obvious to us that Argo was more mature and would give our developers a better user experience.  The killer feature here is the UI. Argo CD has an intuitive and user-friendly UI that gives the developers a good overview of what is deployed. Whenever anything fails, it's immediately obvious which resource is failing, and Argo allows you to drill down into the resource to see the details of the failure, logs for deployments, Kubernetes events, etc.    The above photo illustrates this well. Here you can see a project with a number of Skiperator applications. The green checkmarks indicate that the application is synced and the green heart indicates that the application is healthy. A developer can see the underlying &quot;owned&quot; resources that Skiperator creates (such as a deployment, service, etc), and get a look &quot;behind the curtain&quot; to see what is actually deployed. This helps debugging and gives the developers a better insight into what is happening during a deployment.  In terms of multi tenancy, Argo CD has a concept of projects. A project is a set of namespaces that a team has access to, and a team can only use Argo to sync to namespaces that are part of their project. The namespace allowlist can also include wildcards, which sounds small but this solved our self-service issue! With our apps-repo architecture, we would give a team a &quot;prefix&quot; (for example seeiendom-), and that team would then be able to deploy to and create any namespace that started with that prefix. If they tried to deploy to another team's namespace they would be stopped, as they would not have access to that prefix.  The prefix feature allows product teams to create a new directory in their apps repo, which will then be synced to the cluster and deployed as a new namespace. This is a very simple and intuitive workflow for creating short-lived deployments, for example for pull requests, and it has been very well received by the developers.  The apps-repo architecture will be a blog post itself at some point, so I won't go too much into it.  And finally, if you're wondering what disaster recovery of an entire cluster looks like with Argo CD, I leave you with the following video at the end.    ","version":null,"tagName":"h3"},{"title":"Hybrid Mesh‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#hybrid-mesh","content":" A hybrid mesh service mesh configuration is a setup that allows for service networking across different environments. For Kartverket this includes a hybrid cloud environment. The setup involves several steps, including setting up cross-cluster credentials, installing the east-west gateway, enabling endpoint discovery, and configuring certificate authorities. All clusters in a hybrid mesh are registered to the same fleet host project, and istiod in each cluster must be able to communicate with the Kube-API on the opposing clusters.  ASM is as previously mentioned based on Istio, and after some internal discussion we decided to experiment with running vanilla upstream Istio in our GKE clusters running in GCP. Pairing it with ASM in our on-premise clusters worked as expected (after a bit of config), and we are now running upstream Istio in GKE, with ASM on-prem in a multi-cluster setup. We also looked into using managed ASM in our GKE cluster, this was hard for us however, due to it requiring firewall openings on-prem for sources we could not predict.    We have chosen the Multi-Primary on different networksafter reviewing our network topology and configuration. We connect our on-premise network, with the GCP VPC through a VPN connection (using host and service projects). To have a production ready environment, the VPN connection must be configured with redundancy.  We're working towards getting this architecture into production, as this will enable us to seamlessly use GKE clusters in GCP together with our on-premise clusters. The elasticity of cloud infrastructure can be utilized where needed, and we can handle communication between services on different clusters much more smoothly. This has been a bit of a journey to configure, but as a learning experience it has been valuable. Being able to address services seamlessly and communicate with mTLS enabled by default across sites, zones and clusters without developers having to think about it feels a bit like magic.  ","version":null,"tagName":"h2"},{"title":"Monitoring‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#monitoring","content":" ","version":null,"tagName":"h2"},{"title":"Google Cloud Monitoring‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#google-cloud-monitoring","content":"   GKE Enterprise includes an agent that collects metrics from the cluster and sends them to Google Cloud. This is a great feature which makes it relatively easy to get started with metrics and monitoring. However, we have decided not to use the agent, and instead use Grafana and LGTM for metrics and monitoring.  This is mainly due to a couple of challenges:  The amount of metrics that are collected out of the box and sent to GCP contributes a significant part of our total spend. It's not that we have a lot of clusters, but the amount of metrics that are collected out of the box is very high, and Anthos' default setup didn't give us the control we needed to be able to manage it in a good way.  Note that this was before Managed Service for Prometheus was released with more fine grained control over what metrics are collected. It is now the recommended default, which should make metrics collection easier to manage.  Second, while Google Cloud Monitoring has a few nice dashboards ready for Anthos, it feels inconsistent which dashboards work on-premise and which only work in cloud as they are not labeled as such. This is not a big issue, but it's a bit annoying. The bigger issue is that all the dashboards feel sluggish and slow to load. Several of us have used Grafana before, so we're used to a snappy and responsive UI. In our opinion, Google Cloud Monitoring feels clunky in comparison.  So the cost and the user experience were the main reasons we decided to look at alternatives to Google Cloud Monitoring. We ended up using Grafana and LGTM, which we'll talk about next.  ","version":null,"tagName":"h3"},{"title":"Grafana with the LGTM stack‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#grafana-with-the-lgtm-stack","content":"   When we realized that our needs were not entirely met by Google Cloud Monitoring, we started a project to develop a monitoring stack that would meet our needs. Since Grafana is open source and has a large community, we decided to use that as our frontend. Our backend is the LGTM stack, which is a set of open source tools that are designed to work well together for ingesting, storing and querying logs, traces and metrics.  What we noticed immediately was that the product teams were much more engaged with this stack than they were with Google Cloud Monitoring. Previously they would not really look at the dashboards, but now they are using them and even creating their own. This is a huge win for us, as we want the teams to be engaged with the monitoring and observability of their services.  It definitely helps that most developers on the product teams are familiar with Grafana, which makes it easier for them to get started as the learning curve is not as steep.  There was a discussion about what the backend should be, if we should useGrafana Cloud or host it ourselves. There would be a lot of benefits of using the cloud, as we would not have to maintain the stack or worry about performance or storage. There was, however, a concern about cost and whether or not log files could be shipped to a cloud provider. In the end we decided to host it ourselves, mostly because we didn't have control over what quantities of data we're processing. Now that we have a better understanding of our usage we can use that to calculate our spend, so we're not ruling out migrating to Grafana Cloud in the future.  The collection (scraping) of data is done by Grafana Agent, which is an &quot;all-in-one&quot; agent that collects metrics, logs and traces. This means a few less moving parts for the stack, as we don't have to run both Prometheus,Fluent Bit and someOpenTelemetry compatible agent for traces. It's a relatively new project, but it's already relative stable and has a lot of features. It uses a funky format for configuration called river, which is based on Hashicorp's HCL. The config enables forming pipelines to process data before it's forwarded to Loki, Tempo or Mimir. It's a bit different, but it works well and is easy to understand and configure to our needs.    Using a system like Grafana also enables us to build an integrated experience that also includes alerting. Using Grafana alerting and OnCall, we configure alerts that are sent to the correct team based on the service that is failing. This helps the teams get a better overview of what is happening in their services, and also helps us as a platform team to not have to be involved in every alert that is triggered.  Overall we're very happy with the LGTM stack, even though it's a fair bit of work to maintain the stack (especially with Istio and other security measures). We're also happy with Grafana, and we're looking forward to seeing what the future holds for monitoring and observability in Kubernetes.  ","version":null,"tagName":"h3"},{"title":"Summary‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#summary","content":" To summarize: We like Anthos, and we think it's a great platform for running hybrid Kubernetes. As a platform team we look at each feature on a case-by-case basis, with the goal of giving our developers the best possible experience instead of naively trying to use as much as possible of the platform. Because of this we've decided to use Anthos for Kubernetes and service mesh, but not for config sync and monitoring. This has given us a great platform that we're confident will serve us well for years to come.  Stay tuned for the third and final part of this series, where we'll talk about the benefits we've seen from Anthos, and what we would have done differently if we were to start over.  Disclaimer - Google, GKE and Anthos are trademarks of Google LLC and this website is not endorsed by or affiliated with Google in any way. ","version":null,"tagName":"h2"},{"title":"Crisis Management Exercises","type":0,"sectionRef":"#","url":"/blog/crisis-management-exercises","content":"","keywords":"","version":null},{"title":"Exercise 1: Malicious actor‚Äã","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#exercise-1-malicious-actor","content":"   The first exercise scenario revolved around a malicious actor gaining privileged access to our production Kubernetes cluster, simulated in this case by our internal sandbox cluster. Admittedly, it was somewhat difficult to set up a realistic scenario without outright disabling some of our security tools, so in the end we simulated a hostile takeover of the user account belonging to the person responsible for planning and running the exercise.  The first sign that something was amiss was an alert from our Sysdig Secure toolset, a Falco-based agent software which continually monitors our cluster for signs of abnormal activity according to a predefined ruleset and provides a SaaS portal for further analysis and management of threats. (We will cover more of our security features and mechanisms and how we try to build a modern kubernetes based application platform with built-in security and zero trust in a future blog post.) After initial examination, we found that the incident was of such a nature that we engaged our crisis management plan in order to investigate, contain and mitigate the incident. We simulated communication with the organization-level crisis management team, having regular meetings in order to keep them informed of progress. Systematic examination of logs and audit logs soon turned up suspicious activity confined to one specific platform developer account, and the decision was made to immediately suspend (simulated in this case) the account, removing all access to organizational systems and in effect locking it out. Simultaneously, the malicious software was removed once enough evidence was secured in order to further analyze the actions and impact of it. The exercise was announced as ended once we suspended the compromised user account and removed the malicious application while retaining and analyzing enough logs, forensic captures and other traces of activity.  ","version":null,"tagName":"h2"},{"title":"Exercise 2: \"Everything is on fire\"‚Äã","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#exercise-2-everything-is-on-fire","content":"   The second exercise scenario was somewhat more involved, taking place over two days. The incident itself was as follows: A software update or rogue script caused catastrophic hardware failure in production infrastructure, necessitating creation of a new Kubernetes cluster from scratch. Once the cluster itself and all underlying infrastructure had been created and configured, it would then be up to our platform team to deploy all necessary IAM configuration, service accounts, RBAC and supporting systems (Istio, ArgoCD ++) needed to deploy workloads and restore normal operations. The exercise itself focused on this second phase of restoration, as the infrastructure configuration and cluster creation itself is done by another team, with little involvement by our platform team members.  The failure itself was simulated by having our infrastructure team wipe our sandbox environment and present us with a clean-slate Kubernetes cluster. We called an all-hands meeting and set to work restoring services right away. Right at the onset, we recognized that this was a golden opportunity both to ensure that our documentation was up-to-date, consistent and easy to follow, as well as give our three newest team members some much-needed experience and insight into setting up our services from scratch. We therefore decided that the newest team members would be the ones to actually execute all the actions outlined in our documentation, while the rest of us followed along and made notes, updated documentation and otherwise provided guidance throughout the process.  The first run-through of the recovery process took around 2-3 hours before everything was in working order. Keep in mind that we took the time to update our documentation and explain everything we did while we were working, so in a real-life scenario this would have been even quicker. Once the IAM, RBAC, Istio and ArgoCD was up and running, it was merely a matter of using ArgoCD to synchronize and deploy all relevant workloads. Afterwards, we had a meeting to discuss the process and what experiences we gained from it. Based on the feedback from this meeting, we made further adjustments and updates to our documentation in order to make it even easier to follow on a step-by-step basis, focusing on removing any ambiguity and put any &quot;tribal&quot; knowledge among our platform developers into writing. This ensured that we are way less dependent on the knowledge and skillset of specific people, enabling any team member to contribute to recovery efforts by simply following the documentation.  The newest team members greatly enjoyed being responsible for the recovery effort itself, and expressed a wish to run through the scenario again in order to refine their skills and further improve the documentation. Therefore, we decided to set aside most of day 2 to do just that. We had the infrastructure team tear down and setup the cluster again, and let the newest team members loose on it - this time on their own without guidance - an additional two times. The last run-through of the exercise took between 30 and 60 minutes, a significant improvement from the initial attempt.  All in all, we considered the exercise to be a great success, with many important lessons learned and a substantial improvement in the quality of our documentation and crisis management plans.  ","version":null,"tagName":"h2"},{"title":"What did we learn?‚Äã","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#what-did-we-learn","content":"   ","version":null,"tagName":"h2"},{"title":"Lesson 1: You are only as good as your documentation‚Äã","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-1-you-are-only-as-good-as-your-documentation","content":" Documentation is vitally important during a crisis, and should be detailed enough that any team member may follow it on a step-by-step basis and be able to restore normal service, even with minimal knowledge and during a stressful situation. This ensures that you avoid being dependent upon key personnel that might or might not be available during a crisis scenario, and also ensures that you retain vital institutional knowledge even when team members move on to different tasks or even new jobs.  ","version":null,"tagName":"h3"},{"title":"Lesson 2: Logging, logging, logging! Oh, and monitoring too!‚Äã","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-2-logging-logging-logging-oh-and-monitoring-too","content":" Having the ability to search through logs of all parts of our system greatly simplifies any incident management, whether the incident revolved around malicious actors or other factors. But logs by themselves are not sufficient - you need some sort of monitoring and alerting system in order to alert on and react to abnormal situations/behaviour in your systems. Ideally, you should be able to react on these alerts instead of messages from users - or worse, customers - that something is wrong.  ","version":null,"tagName":"h3"},{"title":"Lesson 3: Test your plans!‚Äã","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-3-test-your-plans","content":" Merely having plans, routines and documentation is insufficient. Unless they have been thoroughly tested and their quality assured through crisis exercises in realistic scenarios and conditions, they should be treated as flawed and unreliable until the opposite is proven. Running crisis management exercises is a great way to expose flaws, insufficiencies and outdated documentation, and careful note-taking and postmortems should be the norm throughout the exercise in order to easily identify and update weak spots in your plans and documentation. As systems and circumstances change, so should plans and documentation too in order to reflect the new order of the day.  ","version":null,"tagName":"h3"},{"title":"Lesson 4: Communicate!‚Äã","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-4-communicate","content":" Openness and communication is critical during both exercises and real-world crisis scenarios. Plans should always involve key points of communication - who needs to be informed, whose responsibility it is to keep said people informed, and the frequency, scope and format of information to disseminate. This also applies to communication afterwards. Anyone in your organization should be able to understand what happened, how it was solved and what lessons were learned from it. In Kartverket, we solve this by writing postmortems about incidents, summing up the incident itself and what we learned from it. We favour Blameless Postmortems, enabling us to quickly and thoroughly analayze and document all aspects of an incident without focusing on individual mistakes and avoid passing blame. This contributes to a culture of openness, learning and improvement. Hoarding information and disseminating it only on a &quot;need-to-know&quot; basis only breeds distrust and contempt, as does a culture that focuses on blaming and punishing people for mistakes instead of learning from them. A further bonus when communicating the happenings and results of your crisis management exercises is the potential to inspire others - when people see the great results and lessons you yourselves have gained from such exercises, they might want to try it with their own systems and teams.  ","version":null,"tagName":"h3"},{"title":"Lesson 5: Let the \"newbies\" handle it‚Äã","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-5-let-the-newbies-handle-it","content":" Putting our newest team members in charge of the recovery operations was a great learning experience for them, as well as enabling us to quickly find flaws and shortcomings in our documentation and crisis management plans. It is also a great confidence booster, because if they succeed, they'll gain valuable insight and positive experiences with setting up all those scary critical systems from scratch - and if they don't succeed, well, that's not their fault, it was because the documentation and training was insufficent to enable them to handle the situation!  ","version":null,"tagName":"h3"},{"title":"Lesson 6: Crisis exercises as team building‚Äã","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-6-crisis-exercises-as-team-building","content":" Crisis exercises are fun and contribute to better teamwork! They bring everyone together in order to achieve a common goal - get things up and running again as quickly as possible. Combine it with &quot;pair programming&quot; - that is, if possible make sure at least two people are working on any given task together - this helps facilitate cooperation and communication, and provides an extra set of eyes to help catch any manual errors or deviations from the plan.  ","version":null,"tagName":"h3"},{"title":"Thank you for reading!‚Äã","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#thank-you-for-reading","content":" We appreciate you taking the time to read through this blog post. We have learned quite a lot (and had lots of fun) through our approach to crisis management exercises. We hope our experiences and thoughts regarding this subject has been interesting, and that they may inspire others to start doing crisis management exercises as well. ","version":null,"tagName":"h2"},{"title":"Hybrid Kubernetes in production pt. 3","type":0,"sectionRef":"#","url":"/blog/hybrid-kubernetes-in-production-part-3","content":"","keywords":"","version":null},{"title":"Do you really need hybrid?‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#do-you-really-need-hybrid","content":" When we started out, there was an assumption that it was simply impossible to use the cloud. This came from all sides of the organization, so this was taken as a given. SKIP was therefore started as a project to build an on-premise Kubernetes platform to service our needs as a transition to cloud native development principles.  As we moved along, a lot of these assumptions got challenged. We found that most of these assumptions were based on misunderstandings or a lack of a deeper understanding of cloud technologies and the surrounding legal aspects. This led to a fear of the unknown, and subsequent inaction. In the end it turned out that quite a lot of our workloads could indeed run in the public cloud, given some minor adjustments.  Had we started out with the knowledge we have now, we would probably have started with a public cloud provider, and then moved to hybrid when and if we saw a need for it. Using a cloud provider's managed Kubernetes offering is significantly easier than running your own, and you can get started much faster, with less risk.  Given our organization, we would probably have ended up with hybrid anyway, but that complexity could potentially have been moved down the timeline to a point where the platform was more mature.  Starting with hybrid is a massive undertaking, and you should have a good reason for doing so. Do you need hybrid, or do you just need to mature your organization? If you do, reduce the scope of the initial work to get to a workable platform, and preferably start in the cloud, adding hybrid features later. If you're not sure, you probably don't need hybrid.  ","version":null,"tagName":"h2"},{"title":"Hybrid gives your organization flexibility‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#hybrid-gives-your-organization-flexibility","content":"   Now that we've built a platform that seamlessly runs workloads in both public cloud and on-premise, we have a lot of flexibility in where we run our workloads and how we manage them. Our experience is that this makes it easier for the organization to mature legacy workloads.  All our greenfield projects are written with cloud native principles in mind, which makes it trivial to run them in the cloud. Legacy workloads, however, are not so lucky. They are often written with a lot of assumptions about the underlying infrastructure and are not cognizant of the resources they use. This means they are a poor fit to lift and shift to the cloud, as they will often be expensive and inefficient.  With a hybrid platform, we can use our on-premise offering as a spring board for modernization. Product teams will start by shifting their app to our on-premise Kubernetes platform, and then gradually modernize it to be cloud native. This method gives a few immediate benefits from the lift and shift like better observability, developer experience and security features but also gives fewer of the drawbacks, as the on-premise cloud is closer to the existing dependencies than a public cloud. Once this is done, smaller chunks kan be rewritten as microservices and moved to the cloud, communicating with the monolith seamlessly over the hybrid network. This is sometimes referred to as the strangler application.  This method significantly reduces the scope of refactoring, as one can focus on gradually rewriting smaller modules instead of rewriting the entire application.  ","version":null,"tagName":"h2"},{"title":"Service mesh is hard, but maybe a necessary evil to make hybrid less painful‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#service-mesh-is-hard-but-maybe-a-necessary-evil-to-make-hybrid-less-painful","content":"   Oh my word how we have struggled with service mesh.  Starting from nothing with a goal of providing a secure-by-default zero-trust network layer with observability and traffic control is quite an undertaking, especially when you pair that with setting up a new kubernetes-based infrastructure from scratch. Istio is famously complex, and we've had our fair share of that.  So how do we feel about Istio? There are various opinions in the team, but if we average them all out, we're content. It's quite complex and can be hard to debug, but it does the job. As we've matured and gotten more experience with Istio, we've also started to see more benefits, like extensions for handling OAuth2and the traffic control features for gradual rollouts which we used for canary-testing the migration of some of our larger applications to SKIP. Not all of these features, like EnvoyFilters, are supported by Anthos Service Mesh (ASM), which is why we're exploring using upstream Istio instead of ASM.  One thing we quickly learned is to not let the product teams configure the service mesh directly using service mesh resources. This is a recipe for disaster. We tried this in the beginning, and first of all it's a huge complexity burden for the product teams. We also started getting a lot of weird issues when product teams would configure the mesh in ways that broke their encapsulation. Since the service mesh is a cluster-wide feature, if one team makes an invalid configuration, it can break other teams' workloads. Kubernetes namespaces be damned. We've therefore moved to a model where the platform team provides an abstraction throughSkiperator which configures the service mesh on their behalf.  Finally, I think it's prudent to ask yourself wether or not you actually need a service mesh. If you're running a small cluster with a few services, you'll probably be fine with using the built-in Kubernetes features like Ingress and Network Policies. The observability features are nice, but you can get most of them with a combination of instrumentation and Grafana.  If you need service mesh then limit the scope until you get comfortable with the mesh, for example start with just mTLS and observability, and then add zero trust networking features later.  Also keep in mind there is a lot of competition in the service mesh space, and there are some interesting alternatives to Istio, likeLinkerd and the up-and-coming Cilium Service Mesh.  ","version":null,"tagName":"h2"},{"title":"Anthos helps you as a platform team getting started with best practices.. Even if you plan to move to open source components later‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#anthos-helps-you-as-a-platform-team-getting-started-with-best-practices-even-if-you-plan-to-move-to-open-source-components-later","content":"   When our platform team started out a few years ago, we picked some of the brightest cloud engineers from within the organization and combined them with some consultants to work on the platform. Most of these engineers had some experience working with Kubernetes and cloud, but not building something of this scale from scratch. The first months would therefore be a learning experience for most of the team.  I think a lot of teams will be in a similar situation, and this is where a managed service like Anthos can be a huge help. Anthos is built with best practices in mind, so a lot of the architecture decisions were built-in to the installer. Choosing a managed offering, even when running on-prem has therefore helped us deliver value to the product teams much quicker than if we had to build everything from scratch.  What's important to point out is that choosing something that is managed does not rule out using open source components later. We started out using all the parts that Anthos gave us, including service mesh, logging, monitoring and configuration management. Managed services do come with some tradeoffs, however, as you lose some of the finer control of the platform. As the team has matured and gained experience, we've started to replace some of these components with open source alternatives, which has helped us save money and gain more control over our platform. This has the downside of having to maintain these components ourselves, but with more experience in the team, this is a tradeoff we feel is worth it.  Even though we're increasingly using more open source components, we don't regret using a paid managed offering in the beginning. It helped us get started and make the right decisions early on, and we're now in a position where we can capitalize on that great start.  ","version":null,"tagName":"h2"},{"title":"Keep in mind autoscaling when choosing licensing models‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#keep-in-mind-autoscaling-when-choosing-licensing-models","content":"   This may be an obvious point to some of the more experienced platform engineers out there, but it was still something that we had to learn. When we started out, we appreciated the simplicity of SaaS products that billed per node, as it made it easy to predict costs. We could simply look at the number of nodes we had running and multiply that with the price per node to get a relatively accurate estimate of what this offering would cost. This would turn out to be a double edged sword, however.  It is safe to assume that one of the reasons people choose Kubernetes is the ability to scale workloads easily. This could be scaling up to handle more traffic, or scaling down to save money. This is a great feature, but as the number of workloads grow, the provisioned nodes will start to become insufficient and new nodes will be provisioned. With Kubernetes and Anthos on VMware this can be done automatically, which is a fantastic feature.  The problem arises when you scale out more nodes and have a static license that bills per node. We've made the mistake of getting contracts with two (now just one) SaaS providers where we order a set of nodes, let's say 10, and when workloads scale up, we end up with more than 10 nodes. This means we're not running that SaaS-service's agents on the new nodes, which can be anything from inconvenient to critical, depending on the service. In the end we've had to restrict our node scaling to avoid this issue, which goes against the whole ethos of Kubernetes. We're also provisioning bigger nodes than we need to avoid scaling out, which can be suboptimal.  We're now working with the vendors to get a more flexible license that bills per node on demand, but this is something to keep in mind when choosing a SaaS offering. Try to factor in the future scaling needs of your platform when purchasing SaaS services.  ","version":null,"tagName":"h2"},{"title":"Summary‚Äã","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#summary","content":" To summarize: We've learned a lot on our journey to building a hybrid Kubernetes platform. Over the last few years we've iterated on our platform and learned lots of great lessons. It's been a huge help and privilege to have the support of our organization, especially in terms of us being allowed to fail and learn from our mistakes. The Norwegian saying &quot;it's never too late to turn around&quot; comes to mind, as we've changed course several times on our journey, sometimes to the annoyance of our product teams who depend on a stable platform - but in the end we've ended up with a better product - a platform we can be proud of and that our product teams love using.  Thanks for reading this series on Anthos and hybrid Kubernetes. We hope you've learned something from our experiences, and that our hard earned lessons can help you on your journey to building a hybrid Kubernetes platform.  Disclaimer - Google, GKE and Anthos are trademarks of Google LLC and this website is not endorsed by or affiliated with Google in any way. ","version":null,"tagName":"h2"},{"title":"SKIP on Plattformpodden!","type":0,"sectionRef":"#","url":"/blog/skip-on-plattformpodden","content":"Very recently, SKIP was featured on thePlattformpodden podcast! Vegar and Eline were invited to talk about SKIP, how it came to be and what it's like to work on it. Give it a listen! https://plattformpodden.no/episode/6","keywords":"","version":null},{"title":"SKIP has a tech blog!","type":0,"sectionRef":"#","url":"/blog/welcome","content":"SKIP is starting a tech blog! üöÄ Or call it a newsletter if you're tired of blogs ü§™ Our first entry is already out, and it's about why we chose Anthos for hybrid cloud. We're working on more entries into that series and other exciting topics, so stay tuned! SKIP is Statens Kartverks Infrastruktur Plattform, or in English, the Infrastructure Platform of the Norwegian Mapping Authority. We're the platform team at Kartverket. We tame Kubernetes and the Cloud. With SKIP, developers in Kartverket are empowered to run, not walk, using a comprehensive toolbox of modern cloud technology. Using SKIP, developers can deploy applications to Kubernetes in a matter of minutes, while still being able to use the tools they know and love. Like what you see? We're a small team, but we're growing fast. We're also hiring, so if you're interested in working with us, check out our open positions.","keywords":"","version":null},{"title":"Intro til SKIP","type":0,"sectionRef":"#","url":"/docs","content":"","keywords":"","version":"Next"},{"title":"Velkommen til SKIP! üéâ‚Äã","type":1,"pageTitle":"Intro til SKIP","url":"/docs#velkommen-til-skip-","content":" SKIP st√•r for Statens Kartverks Infrastrukturplattform. SKIP uttales som det norske ordet skip, et st√∏rre sj√∏g√•ende fart√∏y.  SKIP-teamet jobber med en utviklingsplattform hvor kjernekomponentene er Kubernetes, Google Cloud, Argo CD og GitHub.  Hensikten er √• ha en helhetlig plattform for moderne utvikling hvor utviklere enkelt skal kunne lage, teste og kj√∏re containerbaserte applikasjoner basert p√• Cloud Native-prinsipper p√• en enkel og sikker m√•te.  Under finner du en presentasjon som gir en introduksjon til SKIP. Presentasjonen er laget av Eline Henriksen, som er en av utviklerne bak SKIP. Trykk p√• pilene for √• g√• gjennom presentasjonen.   ","version":"Next","tagName":"h2"},{"title":"üöÄ Argo CD","type":0,"sectionRef":"#","url":"/docs/argo-cd","content":"","keywords":"","version":"Next"},{"title":"Lenker til Argo‚Äã","type":1,"pageTitle":"üöÄ Argo CD","url":"/docs/argo-cd#lenker-til-argo","content":" Du m√• v√¶re p√• Kartverkets nettverk eller VPN for √• kunne n√• disse lenkene.  Dev (argo-dev.kartverket.dev)Prod (argo-prod.kartverket.dev)  ","version":"Next","tagName":"h2"},{"title":"GitOps‚Äã","type":1,"pageTitle":"üöÄ Argo CD","url":"/docs/argo-cd#gitops","content":" Argo CD er et GitOps-verkt√∏y, det vil si at kilden til sannhet ligger i git og synkes inn i clusteret derfra. GitOps er beskrevet i bildet over og er en ‚ÄúPull-basert‚Äù deployment-flyt kontra den tradisjonelle ‚ÄúPush-baserte‚Äù deployment-flyten. En operator kj√∏rer i clusteret og overv√•ker kontinuerlig ett eller flere git-repoet og synker yaml-filer inn i clusteret. P√• den m√•ten kan produktteam forholde seg til noe s√• enkelt som filer i en mappe i git, og n√•r disse filene endres gj√∏res en deploy helt automatisk.  GitOps vil gi mange fordeler, men det blir et paradigmeskifte for mange. Istedenfor √• tenke ‚ÄúPush‚Äù-basert deploy ved √• kj√∏re et skript for √• deploye vil man legge inn √∏nsket state i en fil og s√• vil systemet jobbe for √• bringe clusteret i synk med √∏nsket state. Denne overgangen kan ogs√• sammenlignes litt med imperativ vs. deklarativ programmering, som jQuery vs. React. For de fleste som har jobbet med Kubernetes vil det f√∏les veldig kjent, siden Kubernetes i praksis er en stor reconciliation loop som kontinuerlig driver clusteret mot √∏nsket state.  Det er mange fordeler med et slikt deployment-system. N√•r deployment og CI er to distinktive komponenter i systemet blir deployment-systemet mye mer spisset inn mot sin rolle og vil kunne perfeksjonere den, den s√•kalte ‚ÄúDo one thing and do it well‚Äù-tankegangen.  I de neste sidene skal vi beskrive hvordan Argo CD fungerer og hvordan dere kan bruke det til √• deploye til SKIP. ","version":"Next","tagName":"h2"},{"title":"Scaling with Argo CD: Introducing the Apps Repo Architecture","type":0,"sectionRef":"#","url":"/blog/introducing-apps-repositories","content":"","keywords":"","version":null},{"title":"Multi-tenancy in Argo CD‚Äã","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#multi-tenancy-in-argo-cd","content":" So you've deployed Argo CD on your multi-tenant cluster and given your teams access to the user interface. Let's imagine we now have tens of teams and hundreds of applications in the Argo UI. When we start scaling out to more than a handful of users we get into some issues with scale. Examples of these issues can be:  How do you organize your apps and projects?How do you make sure no two teams accidentally (or maliciously) use the same namespace?How can we make sure teams clean up unused deployment resources?How do you seamlessly deploy to multiple clusters?  As a platform team we often find ourselves thinking that everyone loves infrastructure and Kubernetes as much as we do. This is not the case! Most people have not had the joy of having their childhood ruined by installing Linux on their school laptops and configuring WLAN drivers using ndiswrapper. Believe it or not, most people just want tools to get out of their way and let them do their job, be that programming, testing or anything else. Not every team is going to be experts in Kubernetes and Argo. So should we expect all teams to know what a deletion finalizer is? What about the intricacies of serverside apply vs. clientside apply?  It's our responsibility as a platform team to make the user experience of deploying to Kubernetes as user friendly as possible. After implementing an architecture built with UX in mind we've had the joy of seeing people who are extremely skeptical of Kubernetes and the cloud be won over by how easy it is to get your workloads running on Kubernetes. This is thanks to the consistent user experience and built-in best practices of the apps-repo architecture. But we're getting ahead of ourselves, first we need to talk about a few abstractions that make this possible.  ","version":null,"tagName":"h2"},{"title":"What are ApplicationSets?‚Äã","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#what-are-applicationsets","content":" In Argo CD there's an advanced feature that allows for automating creation of Argo CD Applications calledApplicationSets. Using an ApplicationSet we can essentially make a template that generates Argo CD applications based on files or folders in a Git repository, sort of like a ReplicaSet for Pods. Using ApplicationSets we can build in features and assumptions and provide the teams with a user experience that essentially boils down to &quot;add a file to a repo and it gets deployed to the cluster&quot;. The purest form of GitOps. No messing around with Argo CD applications and projects.  A core Argo CD component called the ApplicationSet controller will detect anyApplicationSet resources deployed to the cluster and read them. After this, it will periodically scan the a repo configured in the ApplicationSet resource and generate Application resources, which in turn scan a repo for manifest files and sync them to the cluster. So in other words: ApplicationSet -&gt;Application -&gt; Deployments  For this to work you need a Git repo containing manifest files. You could have the teams put these manifest files into their source code repositories, but this is not considered best practice. Usually you would put your manifests into a separate repo so that changes to the manifests don't conflict with changes in the source code. At Kartverket we call this manifest repo an apps repo.  ","version":null,"tagName":"h2"},{"title":"Introducing apps repositories‚Äã","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#introducing-apps-repositories","content":"   The apps repo is where the product teams put their manifests. It has a consistent structure and is designed to be read by an Argo CD ApplicationSet. It also has a lot of nifty features that enable self-service which we'll get back to.  First, let's have a look at the structure of an apps repo.  teamname-apps/ env/ clustername/ namespace/ example.yaml   In the simplest of terms, this tree describes where to deploy a given manifest. By using a directory tree it makes setting up an ApplicationSet for this repo trivial.  Consider this example ApplicationSet:  apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: exampleteam-apps namespace: argocd spec: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD goTemplate: true goTemplateOptions: - missingkey=error template: metadata: name: '{{.path.basename}}' spec: destination: namespace: '{{ index .path.segments 2 }}' name: '{{ index .path.segments 1 }}' project: exampleteam source: path: '{{.path.path}}' repoURL: 'https://github.com/kartverket/exampleteam-apps.git' targetRevision: HEAD syncPolicy: syncOptions: - CreateNamespace=true automated: prune: true allowEmpty: true selfHeal: true   With this ApplicationSet any directory within env/*/* will be picked up by the ApplicationSet controller and a new Argo CD Application will be created based on the template in the template object. This enables a product team to create any number of applications for their products.    An example use for this is a product team wanting a namespace for each of their products. Instead of having to order a new namespace from the platform team when they create a new product, they can simply create it themselves by adding a new directory with the same name as the namespace they want. A new Kubernetes namespace will be automatically created thanks to theCreateNamespace=true sync option.  Ephemeral namespaces, aka. preview namespaces, is another usecase. Say a team wants to review a change before merging it to main. They could review the change in the Pull Request, but this removes us from the end user's perspective and is not suitable for non-technical people. With a preview environment the team will automatically create a new directory in the apps repo when a PR is created, and thus get a complete deployment with the change in question. This enables end-to-end testing in a browser, and also allows non-technical people to do QA before a change is merged. When it is merged another workflow can automatically delete the directory, which cleans up and deletes the preview environment.  Our convention is that namespaces are formatted with productname-branch. This allows teams to have multiple deploys per product, and also multiple products per team. So when a new PR is created all a team needs to do to automate the creation of a new directory using CI tools like GitHub actions to create a new commit in the apps-repo. This also enables the flexibility to create it as a PR in the apps-repo, but for ephemeral namespaces, this is usually not necessary.  For example:  footeam-apps/ env/ foo-cluster/ foo-main/ app.yaml foo-feature-123/ app.yaml   ","version":null,"tagName":"h2"},{"title":"Automating and avoiding duplication‚Äã","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#automating-and-avoiding-duplication","content":" Depending on the complexity of the apps repo, the amount of products and branches and a subjective &quot;ickyness&quot; with duplicating files (can you spell DRY?), you have several options on how to automate creating new namespaces.  Simple repos will probably be fine with directories containing simple yaml-files that are synced to the cluster. Newer product teams especially appreciate the simplicity of this approach. To optimize for this you may consider using atemplate directory at the base containing some example files that are copied into the sub-directories. A pseudo-coded GitHub action that uses afrontend.yaml template from the templates directory could look like the following:  jobs: build: # Build a container image and push it deploy: strategy: matrix: env: ['dev', 'test', 'prod'] steps: # .. Checkout repo &amp; other setup .. - name: Deploy to ${{ matrix.version }} run: | namespace=&quot;myapp-${{ github.ref_name }}&quot; path=&quot;./env/atkv3-${{ matrix.env }}/$namespace&quot; mkdir -p $path cp -r templates/frontend.yaml $path/frontend.yaml kubectl patch --local \\ -f $path/frontend.yaml \\ -p '{&quot;spec&quot;:{&quot;image&quot;:&quot;${{needs.build.outputs.container_image_tag}}&quot;}}' \\ -o yaml git config --global user.email &quot;github-actions@github.com&quot; git config --global user.name &quot;GitHub Actions&quot; git commit -am &quot;Deploy ${{ matrix.env }} version ${{ github.ref_name }}&quot; git push   This works for most simple apps. Our experience, however, is that as a team matures and gets more experienced with Kubernetes and Argo CD, they add more complexity and want more control. At this point most teams will migrate to usingjsonnet to enable referencing and extending a reusable library shared between multiple components. SKIP also provides some common manifests via ArgoKit, a jsonnet library.  Kustomize is also a common choice, widely used by SKIP for our own infrastructure, but not really widespread with other teams.  Despite Argo supporting Helm we mostly avoid using it to create reusable templates due to the complexity of templating YAML. Jsonnet is superior in this regard.  Fixing indentation errors in YAML templates in a Helm chart pic.twitter.com/Dv2JUkCdiM ‚Äî memenetes (@memenetes) December 8, 2022  ","version":null,"tagName":"h2"},{"title":"Security considerations‚Äã","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#security-considerations","content":" You may be wondering: &quot;This seems great and all, but what about the security implications of allowing teams to create and edit namespaces in a multi-tenant cluster? That seems really dangerous!&quot;.  First of all, I love you for thinking about security. We need more people like you. Second, Argo CD has some great features we can leverage to make this work without removing the self-service nature of the apps repo architecture.  ","version":null,"tagName":"h2"},{"title":"Prefixes‚Äã","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#prefixes","content":" In order to make this work we need to give each team a set of prefixes. A prefix will usually be the name of a product that a product team has responsibility for maintaining. The only important part is that it is unique and that no other teams have been allocated the same prefix. At Kartverket this is done by the platform team as part of the team onboarding process.  The prefix is used as part of all namespaces that are created by the teams. In the example namespace product-feature-123, product is the prefix. By giving each team a set of prefixes it helps them separate products into easily identifiable namespaces and it ensures that a product team does not accidentally use another team's namespace.  Since each product team has an apps repo with the ability to name their directories as they wish, how can we enforce this? This is where Argo CD's Projects come into play.  Argo CD Projectsprovide a logical grouping of applications, which is useful when Argo CD is used by multiple teams. It also contains a field that allows allowlisting which clusters and namespaces are usable by a project.  Add the following to a Project to only allow this project to create and sync to namespaces prefixed with myprefix-.  metadata: name: exampleteam spec: destinations: - namespace: 'myprefix-*' server: '*'   If you scroll back up to the ApplicationSet example above, you will see that it only creates applications with the project exampleteam. This will automatically wire any applications created to the destination rules we've defined in this project and therefore deny any attempts by a team to use prefixes that they have not been allocated.  The crucial part here is that ApplicationSets and Projects are provisioned by the platform team, and therefore build in these security features. These resources must not be accessible to the teams, or an attacker can simply add exclusions.  ","version":null,"tagName":"h3"},{"title":"Namespace resources‚Äã","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#namespace-resources","content":" Another way this could be abused is if a team is able to create Namespace resources in their apps repository. This should be denied using Argo and/or cluster policies.  If a team is able to create namespace resources (or other cluster scoped resources) in their namespace an attacker can use this to break their namespace &quot;encapsulation&quot;. Imagine for example if one could use their apps repo to sync a namespace resource named kube-system into their env/foo-cluster/foo-maindirectory. Argo CD would allow this, as the manifests are read into an Argo CD application. Then the attacker could delete the namespace and take down the cluster.    It's useful in this multi-tenancy scenario to think of namespaces as resources owned by the platform team and namespace-scoped resources as owned by the product teams. This is considered a best practice, and was reiterated at KubeCon Europe 2024 by Marco De Benedictis. Allowing product teams to edit namespaces can open up a ton of attack vectors, like disabling Pod Security Admissioncontrollers, allowing an attacker to create privileged containers which can compromise the host node.  Friends don't let friends edit namespaces!  ","version":null,"tagName":"h3"},{"title":"Self service customization‚Äã","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#self-service-customization","content":" So we set up an ApplicationSet that configures best practices and secure defaults for product teams! Great! But now that team with experienced cloud engineers really wants to customize their Argo configuration. Maybe they want to configure that one app has auto sync on, but another app has it turned off. Maybe they want to disable self-healing for a short period to manually edit in the cluster. In any case, how can we let teams change this configuration self-service when applications are provisioned by theApplicationSet resource?  We could let the teams edit the ApplicationSet. In our case this would mean the teams need to learn about the ApplicationSet abstraction, gotemplate and SKIP's internal GitOps repo structure. This is overkill when a team usually just wants to flip a flag between true or false for a directory. There could also be security implications with allowing teams to edit ApplicationSet resources that could break encapsulation, which we want to avoid.  Another option would be to contact the platform team and tell us to change some config for them. This is not in line with our thinking, as we want the teams to be able to work autonomously for most operations like this. It would also mean we were given a lot of menial tasks which would mean we have less time to do other more meaningful things or become a bottleneck for the teams.  A third option is setting the ApplicationSet sync policy to create-only. This would confifure the ApplicationSet controller to create Application resources, but prevent any further modification, such as deletion, or modification of Application fields. This would allow a team to edit the application in the UI after creation, for example disabling auto sync. This last option is user friendly, but in violation of GitOps principles where config lives in git and not in a database. If you run Argo stateless like we do this would also mean the changes disappear when the pod restarts.  Because none of these options seemed to be the best, we created a better solution. By using a combination of generators and the new template patchfeature in Argo CD 2.8 we can look through every directory in the apps repo for a configuration file called config.json.  Let's look at an example config.json file. This example file is commited in the apps repo to the env/foo-cluster/foo-main directory.  { &quot;tool&quot;: &quot;kustomize&quot;, &quot;autoSync&quot;: false }   This file is not required, but if this file is found the values configured there overrides a set of default values in the ApplicationSet template. These flags are then used to determine how the resulting Application will behave. This means the team is able to change the values they care about per directory of their apps repo  footeam-apps/ env/ foo-cluster/ foo-main/ config.json app.yaml foo-feature-123/ config.json app.yaml foo-feature-with-default-config/ app.yaml   Additionaly, since the platform team is in control of the template we can eliminate the ability to maliciously change the template by parsing the inputs in a secure way.  ","version":null,"tagName":"h2"},{"title":"Example ApplicationSet‚Äã","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#example-applicationset","content":" Let's look at how we can write an ApplicationSet that allows us to useconfig.json files.  First, we need to configure the ApplicationSet to look through all directories, and at the same time use a config.json file if it is found. This is perhaps the least intuitive part of this new ApplicationSet, so let's walk through it step by step.  First we create a merge generator, which will merge two generators. The key thing here is that it only merges if the key matches in both generators, so this allows us to first find all directories (the default), then directories that contain config.json files (the override).   generators: - merge: generators: - # default - # override mergeKeys: - key   Now we're going to add the generator from before into the default. The only difference is we're doing this using a matrix generator. Doing this combines the parameters generated by the two child generators, which gives us the values from the git generator like before, but also a set of default values we can use in our template later if the config.json file is not provided.  We're also using a value from the git generator to assign a key that will uniquely identify this directory for the merge generator later.   generators: - merge: generators: - matrix: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory - # override mergeKeys: - key   Now we use a variant of the git generator to find all config.json files in the same repo and extract the values from it. Again we're using the key field to uniquely identify this directory so that it will be merged with the correct directory in the merge generator.  We're repeating the default values here as well, since not all fields are required and we don't want them to be overwritten as null in the resulting merge.   generators: - merge: generators: - matrix: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory - matrix: generators: - git: files: - path: env/*/*/config.json repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory mergeKeys: - key   That's it for the generator! Now we can use these variables in thetemplatePatch field (and other fields). In this case we want to set syncPolicy options, so we need to use the templatePatch, as gotemplates don't work for objects.  We're also adding a special case where for directory sources (the default) we exclude config.json files, as we don't want to sync the config file with Argo. This allows us to extend it later to add options for other tools like Kustomize or Helm.  Keep in mind that we don't want users to inject maliciously formed patches, so we cast booleans to booleans.   templatePatch: | spec: source: directory: {{- if eq .tool &quot;directory&quot; }} exclude: config.json {{- end }} {{- if .autoSync }} syncPolicy: automated: allowEmpty: {{ .allowEmpty | toJson }} prune: {{ .prune | toJson }} selfHeal: {{ .selfHeal | toJson }} {{- end }}   ","version":null,"tagName":"h3"},{"title":"Complete ApplicationSet‚Äã","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#complete-applicationset","content":" Here is a complete ApplicationSet containing all the features we've discussed so far.  apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: exampleteam-apps namespace: argocd spec: generators: - merge: generators: - matrix: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory - matrix: generators: - git: files: - path: env/*/*/config.json repoURL: https://github.com/kartverket/exampleteam-apps.git revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory mergeKeys: - key goTemplate: true goTemplateOptions: - missingkey=error template: metadata: name: '{{.path.basenameNormalized}}' spec: destination: namespace: '{{ index .path.segments 2 }}' name: '{{ index .path.segments 1 }}' project: exampleteam source: path: '{{.path.path}}' repoURL: 'https://github.com/kartverket/exampleteam-apps.git' targetRevision: HEAD syncPolicy: managedNamespaceMetadata: labels: app.kubernetes.io/managed-by: argocd pod-security.kubernetes.io/audit: restricted team: exampleteam syncOptions: - CreateNamespace=true - ServerSideApply=true - PrunePropagationPolicy=background templatePatch: | spec: source: directory: {{- if eq .tool &quot;directory&quot; }} exclude: config.json {{- end }} {{- if .autoSync }} syncPolicy: automated: allowEmpty: {{ .allowEmpty | toJson }} prune: {{ .prune | toJson }} selfHeal: {{ .selfHeal | toJson }} {{- end }}   ","version":null,"tagName":"h2"},{"title":"Results‚Äã","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#results","content":" With Argo CD and the apps repo architecture, we've seen some real improvements in our deploy system. Teams find it to be incredibly intuitive to just update a file in Git and have it be instantly reflected in Argo CD and Kubernetes, especially when combined with Argo CD auto-sync.  Onboarding new teams is quick and easy, since just putting files into a Git repo is something most developers are already familiar with. We just show them the structure of the apps repo and they're good to go. A team can go from not having any experience with Kubernetes to deploying their first application in a matter of minutes.  Migrating from one cluster to another is also a breeze. Just move manifests from one directory under env to another, and the ApplicationSet will take care of the rest. This is especially useful for teams that want to start developing with new cloud native principles on-premises, modernizing the application and eventually moving to the cloud.  I feel the key part of this architecture is the config.json file. It allows a degree of customization that is not possible with the default ApplicationSettemplate and was to us the last missing piece. It allows teams to change configuration without needing to know about the ApplicationSet abstraction, and it allows the platform team to enforce security and best practices.  ","version":null,"tagName":"h2"},{"title":"Tradeoffs‚Äã","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#tradeoffs","content":" But of course, there are some drawbacks. Like always, it's tradeoffs all the way down.  Since a product team uses an apps repo to organize their apps, moving apps from one team to another will require migrating files from one repo to another. This will require some manual work to prevent Argo deleting the entire namespace when the directory is removed from the old repo. Usually this is not a big issue, and moving projects between teams happens very rarely, but it's something to keep in mind.  There is also a risk that a team could accidentally delete a namespace by removing a directory in the apps repo. We have mitigated this by disabling auto-sync for most mission critical applications in production.  And finally, projects that don't have clear ownership or shared ownership can be tricky to place into a repo. You could make an apps repo for a &quot;pseudo-team&quot; consisting of the teams that need access, but generally we find that it's better that all products have a clear singular main owner. This also preventsdiffusion of responsibility.  ","version":null,"tagName":"h3"},{"title":"Thank you for reading!‚Äã","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#thank-you-for-reading","content":" We hope you found this article helpful and informative. Getting intoApplicationSets can be a bit tricky, so we hope we managed to convey the most important parts in a clear and understandable way. Thanks for reading!  We recently created a Mastodon account @kv_plattform! If you want to contact us or discuss this article, feel free to reach out to us there. ","version":null,"tagName":"h2"},{"title":"Hente hemmeligheter fra hemmelighetshvelv","type":0,"sectionRef":"#","url":"/docs/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv","content":"","keywords":"","version":"Next"},{"title":"Hvordan bruke External Secrets‚Äã","type":1,"pageTitle":"Hente hemmeligheter fra hemmelighetshvelv","url":"/docs/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv#hvordan-bruke-external-secrets","content":" ESO lytter i clusteret etter ExternalSecret - og SecretStore -manifester. I det √∏yeblikket disse blir plukket opp blir de lest som konfigurasjon for ESO og en synk mot hvelvet starter som vil ende opp med √• opprette en Kubernetes Secret. Kubernetes Secreten vil ogs√• synkroniseres regelmessig slik at man kan f.eks. rullere hemmeligheter ved √• endre dem i hvelvet.  ","version":"Next","tagName":"h2"},{"title":"SecretStore‚Äã","type":1,"pageTitle":"Hente hemmeligheter fra hemmelighetshvelv","url":"/docs/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv#secretstore","content":" SecretStore-manifestet definerer et hvelv, slik som Vault eller GSM, og m√• settes opp f√∏rst. Denne konfigurasjonen vil ogs√• inneholde hvordan ESO skal autentisere seg og kan gjenbrukes av flere ExternalSecret-manifester. Disse settes typisk opp av et produktteam for deres namespace for √• definere hvor de har lagret sine hemmeligheter.  Se GCPSMProvider for alle gyldige verdier.  apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: gsm spec: provider: gcpsm: projectID: &lt;YOUR_PROJECT_ID&gt;   For at det skal v√¶re lov √• hente ut secrets m√• i tillegg f√∏lgende gj√∏res:  Man m√• g√• inn p√• secreten som skal eksponeres til ESO og gi rollen roles/secretmanager.secretAccessor til servicekontoen: Dev - eso-secret-accessor@skip-dev-7d22.iam.gserviceaccount.comTest - eso-secret-accessor@skip-test-b6e5.iam.gserviceaccount.comProd - eso-secret-accessor@skip-prod-bda1.iam.gserviceaccount.com Namespacene dere oppretter m√• allowlistes for √• kunne hente ut fra prosjektene deres, kontakt SKIP s√• setter vi skip.kartverket.no/gcpProject p√• prosjektene deres og synkroniserer Argo p√• nytt  ","version":"Next","tagName":"h3"},{"title":"ExternalSecret‚Äã","type":1,"pageTitle":"Hente hemmeligheter fra hemmelighetshvelv","url":"/docs/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv#externalsecret","content":" N√•r man har definert et hemmelighetshvelv med SecretStore kan man definere hvilke hemmeligheter som skal hentes ut. Dette gj√∏res med ExternalSecret-manifestet. ExternalSecret-manifestet vil referere til et SecretStore for √• definere backenden og bruker autentiseringen derfra. ESO vil bruke dette manifestet til √• hente ut de definerte feltene fra den gitte hemmeligheten og putte dem inn i en Kubernetes Secret i det formatet som blir spesifisert. Det betyr at man kan mappe om verdier fra et felt til et annet, for eksempel om man skal uppercase navnene n√•r man bruke dem som milj√∏variabler.  I eksempelet under vises hvordan man synker inn enkeltverdier til Kubernetes. Det er ogs√• mulig √• synke alle n√∏klene i en secret som dokumentert i All keys, One secret .  Det er ogs√• mulig √• bruke templates som dokumentert i Advanced Templating .  Se ExternalSecret for alle gyldige verdier.  apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: dbpass spec: # A list of the remote secrets to sync data: - remoteRef: # The name of the secret in the GCP project key: db-pass # Will be written into the Kubernetes secret under this key secretKey: DB_PASSWORD # Refresh the secret every hour refreshInterval: 1h # Uses the gsm secret backend secretStoreRef: kind: SecretStore name: gsm # Creates a kubernetes secret named dbpass target: name: dbpass   Se ogs√• Get all keys from one GSM secret  ","version":"Next","tagName":"h3"},{"title":"Mounting av hemmelighet‚Äã","type":1,"pageTitle":"Hente hemmeligheter fra hemmelighetshvelv","url":"/docs/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv#mounting-av-hemmelighet","content":" N√•r ESO har synkronisert inn hemmeligheten og opprettet en Kubernetes Secret er det ofte slik at man √∏nsker √• bruke dette i en Pod. Vanligvis gjennom √• mounte dette som milj√∏variabler eller som en fil p√• filsystemet, eksempelvis for sertfikater. Bruker man Skiperator er dette veldig rett frem.  Se ogs√• Using Secrets as files from a Pod og Using Secrets as environment variables , men merk at spec er annerledes med Skiperator.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: teamname-frontend spec: # Each key will be set as an env var with its value as the value envFrom: - secret: dbpass # Each key will be created as a file with the key as filename and value as content filesFrom: - secret: dbpass mountPath: /var/run/secret   ","version":"Next","tagName":"h3"},{"title":"Hva hindrer andre √• hente min hemmelighet?‚Äã","type":1,"pageTitle":"Hente hemmeligheter fra hemmelighetshvelv","url":"/docs/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv#hva-hindrer-andre-√•-hente-min-hemmelighet","content":" Med External Secrets gis en sentral servicekonto tilgang til √• hente ut hemmelighetene i GSM. Man skulle derfor tro at det var mulig for andre som bruker den samme servicekontoen √• hente ut hemmeligheten. Det er ikke tilfellet og er l√∏st med andre policies i clusteret.  Ditt team oppretter en SecretStore, og det finnes policies i clusteret som s√∏rger for at kun prosjekter som dere eier kan knyttes opp her. SecretStore-en er det som brukes for √• hente fra GCP. Dermed er det kun prosjektet som ligger her som kan hentes fra, og kun ditt team som kan hente fra ditt prosjekt. ","version":"Next","tagName":"h3"},{"title":"ArgoCD Notifications","type":0,"sectionRef":"#","url":"/docs/argo-cd/argocd-notifications","content":"","keywords":"","version":"Next"},{"title":"Slack‚Äã","type":1,"pageTitle":"ArgoCD Notifications","url":"/docs/argo-cd/argocd-notifications#slack","content":" For Slack er det satt opp en notifikasjonskanal for hvert team p√• m√∏nster &lt;teamnavn&gt;-argocd-alerts, f.eks. #nrl-argocd-alerts. Disse kanalene er videre satt opp med integration mot Slack-appen ‚ÄúArgoCD Notifications‚Äù som tar imot meldinger fra ArgoCD og dytter de inn i korrekt kanal.  (NB: Hvis du ikke finner en slik kanal for teamet ditt, kontakt en administrator for Kartverkets Slack og be om √• f√• opprettet en kanal med korrekt navnem√∏nster og integrasjon mot ‚ÄúArgoCD Notifications‚Äù).    ","version":"Next","tagName":"h3"},{"title":"Github‚Äã","type":1,"pageTitle":"ArgoCD Notifications","url":"/docs/argo-cd/argocd-notifications#github","content":" For Github er det satt opp en app kalt ‚ÄúKV ArgoCD Notifications‚Äù som har mulighet til √• skrive til Github workflow statuser til de forskjellige apps-repoene. Kontakt en av Kartverkets Github-administratorer dersom flere apps-repoer skal legges til her.  Eksempler p√• notifikasjoner:  ","version":"Next","tagName":"h3"},{"title":"Standardnotifikasjoner‚Äã","type":1,"pageTitle":"ArgoCD Notifications","url":"/docs/argo-cd/argocd-notifications#standardnotifikasjoner","content":" F√∏lgende triggers er lagt til som standard for alle apps-repoer:  Trigger\tKommunikasjonskanal\tN√•r trigges denne?notifications.argoproj.io/subscribe.on-sync-failed.slack\tSlack\tSynkronisering av applikasjon feilet notifications.argoproj.io/subscribe.on-sync-failed.github\tGithub\tSynkronisering av applikasjon feilet notifications.argoproj.io/subscribe.on-sync-succeeded.github\tGithub\tSynkronisering av applikasjon gikk bra notifications.argoproj.io/subscribe.on-sync-running.github\tGithub\tSynkronisering av applikasjon kj√∏rer notifications.argoproj.io/subscribe.on-health-degraded.github\tGithub\tHelsesjekk av applikasjonen returnerer et ‚Äúdegraded‚Äù-resultat notifications.argoproj.io/subscribe.on-sync-status-unknown.github\tGithub\tUkjent synkroniseringsstatus notifications.argoproj.io/subscribe.on-deployed.github\tGithub\tNy versjon av applikasjonen deployet til milj√∏ notifications.argoproj.io/subscribe.on-outofsync-one-day.slack\tSlack\tApplikasjonen har status OutOfSync i minst en dag (det har blitt sjekket inn endringer i apps-repoet som ikke har blitt deployet) notifications.argoproj.io/subscribe.on-outofsync-one-week.slack\tSlack\tApplikasjonen har status OutOfSync i minst en uke (det har blitt sjekket inn endringer i apps-repoet som ikke har blitt deployet)  ","version":"Next","tagName":"h3"},{"title":"Ekstra triggers‚Äã","type":1,"pageTitle":"ArgoCD Notifications","url":"/docs/argo-cd/argocd-notifications#ekstra-triggers","content":" I tillegg er det mulig √• spesifisere andre triggers (s√• lenge disse er lagt inn i ArgoCD) per team i objektet triggerSubscriptions i https://github.com/kartverket/skip-apps/blob/main/lib/argocd/argocd.libsonnet .  info Husk √• spesifisere om det er slack eller github notifikasjon man √∏nsker ved √• legge til suffikset .slack eller .github p√• slutten av trigger, og husk √• spesifisere kanalnavn ved bruk av slack notifikasjon  { name: 'teamnavn', oidcGroup: 'aabbbcc-123-321-ccbbbaa', allowlistedPrefixes: [{ name: 'teamnavn' }], triggerSubscriptions: { 'notifications.argoproj.io/subscribe.on-sync-succeeded.slack': 'navn-paa-slack-kanal', 'notifications.argoproj.io/subscribe.eksempel-trigger.github': '', # denne er blank siden det ikke er en kanal √• sende til p√• github } },  ","version":"Next","tagName":"h3"},{"title":"Configuring apps repositories with config.json","type":0,"sectionRef":"#","url":"/docs/argo-cd/configuring-apps-repositories-with-configjson","content":"","keywords":"","version":"Next"},{"title":"Supported options‚Äã","type":1,"pageTitle":"Configuring apps repositories with config.json","url":"/docs/argo-cd/configuring-apps-repositories-with-configjson#supported-options","content":" Key\tType\tDescriptiontool (required)\tdirectory / kustomize / helm\tWhich tool should Argo CD use to sync this directory? The ‚ÄúDirectory‚Äù option supports yaml and jsonnet files. See also tools . autoSync\tboolean ( true / false )\tWhen set to true , the directory is automatically synced when changes are detected. The default value is true in dev and false in prod. prune\tboolean ( true / false )\tWhen enabled, Argo CD will automatically remove resouces that are no longer present in Git. Default is true . See prune . Only used when autoSync is true allowEmpty\tboolean ( true / false )\tSafety mechanism. When prune is enabled it deletes resources automatically, but it will not allow empty syncs (delete all) unless allowEmpty also is enabled. Default is false . See allowEmpty . Only used when autoSync is true selfHeal\tboolean ( true / false )\tWhen changes are made on the cluster directly, Argo will not revert them unless selfHeal is provided. Default is true . See self heal . Only used when autoSync is true ","version":"Next","tagName":"h2"},{"title":"Hvordan bruke Argo CD","type":0,"sectionRef":"#","url":"/docs/argo-cd/hvordan-bruke-argocd","content":"","keywords":"","version":"Next"},{"title":"Applikasjoner‚Äã","type":1,"pageTitle":"Hvordan bruke Argo CD","url":"/docs/argo-cd/hvordan-bruke-argocd#applikasjoner","content":" Det f√∏rste man gj√∏r n√•r man skal ta i bruk Argo er √• g√• til nettsiden og logge inn. Lenkene til nettsiden finner man p√• Argo CD og alle kan logge inn med kartverket-brukeren sin hvis man er p√• et team som har fulgt Komme i gang med Argo CD.    Det neste som m√∏ter deg er en oversikt over applikasjonene som Argo leser ut, avbildet over. Dersom man ikke sere noen applikasjoner her, sjekk om dere har fulgt alle stegene i Komme i gang med Argo CD og at dere har manifester som er satt opp til √• bli synket inn fra apps-repoet deres. Disse prosjektene blir automatisk opprettet basert p√• mappestrukturen i apps-repoet deres, s√• det er ingen behov for √• opprette eller rydde opp prosjekter manuelt.  Klikk p√• et av kortene p√• denne siden og dere vil g√• inn i en mer detaljert visning hvor man ser alle ressursene som blir synkronisert.    Dersom man bruker Skiperator og eksponererer en URL via ingresses vil man ogs√• kunne se sm√• ikoner som er lenker og om man klikker p√• dem √•pnes applikasjonen i nettleseren.  Det er ogs√• et sett med filtere p√• venstre side som er lurt √• bli kjent med, spesielt dersom applikasjonene blir store og vanskelige √• se p√• en skjerm uten √• scrolle.  ","version":"Next","tagName":"h2"},{"title":"Sync‚Äã","type":1,"pageTitle":"Hvordan bruke Argo CD","url":"/docs/argo-cd/hvordan-bruke-argocd#sync","content":"   info Merk at i dev synkroniseres applikasjoner automatisk  P√• prosjektsiden ser man alle kubernetes-ressurser som er en del av applikasjonen. Legg merke til de sm√• fargede symbolene p√• hvert kort som sier noe om statusen p√• ressursen. Hvis de er gr√∏nne viser det at den ressursen er ‚Äúhealthy‚Äù. Dersom den er r√∏d er det et tegn p√• at noe er galt med ressursen. Dersom den er gul er den ‚Äúute av synk‚Äù, og da m√• man synkronisere applikasjonen.  Bildet over viser hvordan man kan synkronisere ut endringene til kubernetes-milj√∏et. Sync-knappen i menylinjen lar deg velge hvordan ting skal synkroniseres ut, og man kan til og med gj√∏re en Selective Sync av kun noen av ressursene. Det vanligste og tryggeste er vel √• merke √• synkronisere alt med default-innstillingene.  Dersom en synk ikke har fungert vil man se en feilmelding i menylinjen √∏verst. I det tilfellet kan det v√¶re lurt √• trykke p√• ‚Äúsync status‚Äù-knappen √∏verst for √• f√• en mer detaljert oversikt over hva som har g√•tt galt.  ","version":"Next","tagName":"h2"},{"title":"Rollback‚Äã","type":1,"pageTitle":"Hvordan bruke Argo CD","url":"/docs/argo-cd/hvordan-bruke-argocd#rollback","content":" I noen tilfeller kan man tenke seg at en u√∏nsket endring er kommet ut i kj√∏remilj√∏et. Da vil den raskeste og enkleste m√•ten √• gjenopprette funksjonaliteten for brukerene ofte v√¶re en rollback til en tidligere kjent fungerende versjon.  Rollbacks er det innebygget st√∏tte for i Argo CD som en del av applikasjonsvisningen. Klikk ‚ÄúHistory and rollback‚Äù for √• f√• en liste over alle tidligere synker som er gjort i denne applikasjonen. Dersom man √∏nsker √• rulle tilbake finner man versjonen man √∏nsker i listen og trykker p√• de tre prikkene og velger rollback. ‚ÄúRevisjonene‚Äù i listen peker p√• en commit i git-historikken til apps-repoet.  Ved en rollback gj√∏r Argo CD en synk som vanlig, men mot en tidligere kjent tilstand. Den vil da ikke bruke tilstanden som ligger i git, men tilstanden til en tidligere synk. Etter en rollback vil applikasjonen st√• som ‚Äúout of sync‚Äù, og det er forventet siden den ikke matcher tilstanden i git.  info Husk at container imaget m√• finnes for at det skal v√¶re mulig √• rulle tilbake. Om container imaget er slettet i ghcr.io , for eksempel av en oppryddingsjobb, s√• vil det ikke v√¶re mulig √• starte opp den tidligere versjonen.  ","version":"Next","tagName":"h2"},{"title":"Detaljer og Web Terminal‚Äã","type":1,"pageTitle":"Hvordan bruke Argo CD","url":"/docs/argo-cd/hvordan-bruke-argocd#detaljer-og-web-terminal","content":"   Dersom man klikker p√• en ressurs i prosjektvisningen vil man se flere detaljer om denne ressursen. Man finner blant annet en oversikt over metadata, manfiest-filen som Argo CD skal synke ut, events og logger.  Det er ogs√• mulig √• endre p√• manifestfilen som ligger i clusteret om man g√•r p√• ‚Äúlive manifest‚Äù og trykker ‚Äúedit‚Äù. Dette vil f√∏re til at applikasjonen kommer ut av synk, og i milj√∏er hvor auto-synking er skrudd p√• vil det tilbakestilles med en gang. Men i noen tilfeller kan det v√¶re nyttig.    Legg ogs√• merke til ‚Äúterminal‚Äù-fanen. Denne er kun synlig om man velger en pod. Velger man denne fanen f√•r man en live terminaltilkobling inn til podden som man kan bruke til feils√∏king.  info Web terminal er ikke tilgjengelig i prod  ","version":"Next","tagName":"h2"},{"title":"Hvordan bruke Argo gjennom API‚Äã","type":1,"pageTitle":"Hvordan bruke Argo CD","url":"/docs/argo-cd/hvordan-bruke-argocd#hvordan-bruke-argo-gjennom-api","content":" Visst du √∏nsker √• automatisere oppgaver, for eksempel synk ved ny image versjon s√• kan det v√¶re greit √• ha muligheten til √• gj√∏re dette fra Github. Det f√∏rste du trengre da er nettverkstilgang fra Github, det f√•r du med tailscale.  For √• autentisere mot Argo s√• m√• du generere en JWT, dette kan du gj√∏re i Argo UIet. G√• inn p√• f.eks https://argo-dev.kartverket.dev, trykk p√• settings oppe til venstre ‚Üí Projects ‚Üí ditt prosjekt ‚Üí trykk p√• ‚ÄúRoles‚Äù fanen, og deretter p√• apiuser. Scroll helt ned p√• modalen som kommer opp og trykk Create under JWT Tokens. Det er samme framgangsm√•te i andre milj√∏.    Etter at token er generert kan du testen den med kommandoen:  curl https://argo-dev.kartverket.dev/api/v1/applications/&lt;min-app&gt; -H &quot;Content-Type: application/json&quot; -H &quot;Authorization: Bearer &lt;token&gt;&quot;   Argos API spec kan man finne her: https://argo-dev.kartverket.dev/swagger-ui ","version":"Next","tagName":"h2"},{"title":"Komme i gang med Argo CD","type":0,"sectionRef":"#","url":"/docs/argo-cd/komme-i-gang-med-argocd","content":"","keywords":"","version":"Next"},{"title":"Sjekkliste‚Äã","type":1,"pageTitle":"Komme i gang med Argo CD","url":"/docs/argo-cd/komme-i-gang-med-argocd#sjekkliste","content":" For √• starte med Argo CD m√• du gj√∏re f√∏lgende:  S√∏rg for at teamet ditt oppfyller Hva skal til for √• bruke Kompass?Produktteamet deres m√• ha en team-gruppe i Azure AD Samle en liste med alle teammedlemmerVelg to av teammedlemmene som skal ha h√∏yere tilganger, for eksempel tech lead og team leadSend denne listen til produkteier SKIP som bestiller opprettelse av CLOUD_SK_TEAM-gruppePass p√• at team-gruppen legges inn i Enterprise Applicationen til ArgoCD for alle relevante milj√∏ Det m√• settes opp et apps-repo Les Hva er et apps-repo for √• forst√• hvordan apps-repoer fungererRepoet opprettes fra apps-template malenGitHub teamet deres m√• gis tilgang til apps-repoet som adminSKIP m√• gi Argo CD-appen p√• GitHub tilgang slik at Argo kan pulle apps-repoet, dette gj√∏res gjennom Github IAC repoet Det bestemmes et ‚Äúprefiks‚Äù som dere deployer til Vanligvis er dette navnet p√• applikasjonen som skal deploye til SKIPDere kan administrere alle Kubernetes namespacer som starter med dette prefikset SKIP m√• konfigurere Argo til √• lese og synkronisere fra apps-repoet SKIP gj√∏r en endring i skip-apps repoet N√• skal du kunne logge inn p√• Argo CD og se applikasjonen din! üöÄ Du finner lenker til Argo p√• Argo CDVidere dokumentasjon finnes p√• Hvordan bruke Argo CD ","version":"Next","tagName":"h2"},{"title":"‚õÖ Google Cloud Platform (GCP)","type":0,"sectionRef":"#","url":"/docs/gcp","content":"‚õÖ Google Cloud Platform (GCP) Under denne siden finner du artikler som omhandler oppsett og bruk av Google Cloud Platform.","keywords":"","version":"Next"},{"title":"Dynamisk tilgangskontroll (JIT)","type":0,"sectionRef":"#","url":"/docs/gcp/jit","content":"Dynamisk tilgangskontroll (JIT) Most developers will at some point experience not having the correct permissions to operate on Google Cloud resources. This is intentional and is part of the principle of least privilege . In order to operate on the resources you want to access, you need to elevate your privileges. A system exists to make this operation self-service, and it is called Just-In-Time access. It can be accessed at https://jit.skip.kartverket.no . After logging in with your Kartverket google account, it will take you to the below screen. First step is filling in the ID of the project you wish to get access to. This can be found by searching in the box or by finding the ID from console.cloud.google.com. Second step, select the roles you want. It is often possible to see which role you need from the error message you got when trying to do an operation and getting denied. A common role that is used for administering secrets in Google Secret Manager is secretmanager.admin. Select a suitable duration using the slider and click continue. Note that some sensitive roles are not compatible with longer durations. Now for the final step, enter a reason for the access request. This is mostly for auditing, as generally speaking requests are granted automatically. The reason entered will be possible to see in the logs if we need to investigate a security breach. In less common cases, for example when restricted roles are to be granted, a manual approval is required. In that case the reason will be visible to the person who approves the request. When you click request access, you will be taken to a summary screen which gives you the result of your request. In the example above, my request was granted automatically. You now have access, and that's just in time!","keywords":"","version":"Next"},{"title":"Tilgang til GCP","type":0,"sectionRef":"#","url":"/docs/gcp/access","content":"Tilgang til GCP SKIP benytter Google Cloud Platform som √∏kosystem rundt Kubernetes/Anthos. Det gj√∏r at man kan benytte seg av andre Google-produkter selv om applikasjonen kj√∏rer p√• et on-premise cluster. Man kan ogs√• autentisere seg mot GCP og benytte kubectl gjennom Google sin Connect Gateway for √• aksessere on-premise cluster uten √• v√¶re p√• det interne nettverket/VDI. For √• kunne logge p√• GCP med Kartverket-brukeren m√• brukeren v√¶re medlem i en CLOUD_SK_TEAM AD-gruppe. Vi anbefaler at leads (produkteier, team lead, tech lead) p√• teamet sender inn en ticket til PureService, eksempelvis med f√∏lgende informasjon: Hei! Kan dere legge til f√∏lgende medlemmer i AD-gruppen CLOUD_SK_TEAM_Eiet? Navn NavnesenKari Nordmann Hilsen Navnesen Navnemann CLOUD_SK_TEAM AD-gruppen m√• ogs√• v√¶re synket inn i GCP. Dette kan ta opp til en time, selv etter du har f√•tt bekreftelse p√• at medlemmet har blitt lagt inn i AD-gruppen.","keywords":"","version":"Next"},{"title":"Provisjonere infrastruktur med Crossplane","type":0,"sectionRef":"#","url":"/docs/argo-cd/provisjonere-infrastruktur-med-crossplane","content":"","keywords":"","version":"Next"},{"title":"Hvordan komme i gang‚Äã","type":1,"pageTitle":"Provisjonere infrastruktur med Crossplane","url":"/docs/argo-cd/provisjonere-infrastruktur-med-crossplane#hvordan-komme-i-gang","content":" La oss si vi har en applikasjon som er deployed med Argo CD og vi √∏nsker √• sette opp en database for denne applikasjonen med Cloud SQL. Da vil vi ha en mappestruktur i v√•rt apps-repo som ser slik ut:  dev/ namespace/ app.yaml # Skiperator-manifest for applikasjonen db.yaml # Crossplane-manifester for databasen p√• GCP   Det f√∏rste steget er √• f√• autentisert mot GCP slik at Crossplane f√•r tilgang til √• opprette ressurser i prosjektet deres. Dette gj√∏res ved √• kontakte SKIP og f√• lagt inn mapping for prefikset deres i skip-apps .  Deretter kan man opprette ressurser som er st√∏ttet av SKIP dokumentert lenger ned. Crossplane st√∏tter mye mer, se CRD-er i GCP provideren , men det m√• lages st√∏tte for disse, se ‚ÄúTilgang til ressurser‚Äù.  For √• provisjonere opp ressurser oppretter produktteamet manifester p√• Kubernetes som blir lest av Crossplane. Et eksempel p√• √• opprette lagring (bucket).  apiVersion: skip.kartverket.no/v1alpha1 kind: BucketInstance metadata: name: my-bucket spec: parameters: bucket: name: dsa-test-bucket-123 serviceAccount: name: crossplane-test displayName: Testing Crossplane Integration   Etter dette er lagt ut vil man kunne se status p√• crossplane ressursene som et hvilket som helst annen kubernetes-ressurs.  $ kubectl get bucketinstance   Man kan ogs√• bruke kubectl describe for √• hente ut events p√• disse ressursene. Events sier mer om hva som skjer og er nyttig til feils√∏king.  Mer om feils√∏king finnes p√• https://docs.crossplane.io/knowledge-base/guides/troubleshoot/ .  ","version":"Next","tagName":"h2"},{"title":"St√∏ttede ressurser‚Äã","type":1,"pageTitle":"Provisjonere infrastruktur med Crossplane","url":"/docs/argo-cd/provisjonere-infrastruktur-med-crossplane#st√∏ttede-ressurser","content":" F√∏lgende ressurser er st√∏ttet for √• provisjoneres med Crossplane i dag:  Buckets (Lagring i Google Cloud Storage)GCP Service AccountsBucket Access (Kubernetes SA to Bucket)Workload Identity (Kubernetes SA to GCP SA)  ","version":"Next","tagName":"h2"},{"title":"Oppsett‚Äã","type":1,"pageTitle":"Provisjonere infrastruktur med Crossplane","url":"/docs/argo-cd/provisjonere-infrastruktur-med-crossplane#oppsett","content":" For √• komme i gang med Crossplane m√• du gj√∏re noe setup. Alle produktteam f√•r automatisk opprettet en servicekonto p√• GCP som vil brukes av Crossplane til √• autentisere mot GCP, og for at Crossplane skal f√• brukt denne m√• det ligge en secret i namespacet deres. For √• f√• inn denne kan dere opprette en secret ved hjelp av en ExternalSecret (se Hente hemmeligheter fra hemmelighetshvelv) som kopierer hemmeligheten fra Google Secret Manager inn i Kubernetes. Dette m√• dere sette opp for hvert prefiks i &lt;prefix&gt;-main mappen deres i apps-repoet:  apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: crossplane-secret spec: refreshInterval: 1h secretStoreRef: name: gsm kind: SecretStore target: name: crossplane-secret data: - secretKey: creds remoteRef: conversionStrategy: Default decodingStrategy: None key: crossplane-credentials metadataPolicy: None   SKIP setter automatisk opp en ProviderConfig n√•r man f√•r knyttet sitt prefix i Argo CD mot GCP. Denne forutsetter en secret i -main namespacet deres som heter crossplane-secret . Hvis ikke denne secreten blir plukket opp s√• h√∏r med SKIP om knytningen til GCP mangler.  For √∏vrig m√• vi bruke JSON keys for GCP service kontoer her siden crossplane st√∏tter ikke Workload Identity on-prem.  ","version":"Next","tagName":"h2"},{"title":"Tilgang til ressurser‚Äã","type":1,"pageTitle":"Provisjonere infrastruktur med Crossplane","url":"/docs/argo-cd/provisjonere-infrastruktur-med-crossplane#tilgang-til-ressurser","content":" I utgangspunktet kan ikke produktteamene f√• tilgang til crossplane CRD-er direkte ettersom disse ikke er namespaced-ressurser og produktteamene kun har tilgang til √• opprette ressurser i sitt eget namespace. Dette betyr at SKIP m√• opprette s√•kalte ‚ÄúCompositions‚Äù for hver ting som produktteamene skal kunne opprette gjennom Crossplane.  Dersom du som utvikler p√• et produktteam har et √∏nske om √• f.eks. kunne opprette en database eller provisjonere andre ressurser gjennom Crossplane som ikke allerede er st√∏ttet m√• det bestilles en ny Composition fra SKIP.  For at SKIP skal opprette en ny composition m√• det lages en XRD og en composition .  Se st√∏ttede ressurser over. ","version":"Next","tagName":"h2"},{"title":"Oppsett og bruk av Google Secret Manager","type":0,"sectionRef":"#","url":"/docs/gcp/oppsett-og-bruk-av-secret-manager","content":"","keywords":"","version":"Next"},{"title":"Hvordan komme i gang?‚Äã","type":1,"pageTitle":"Oppsett og bruk av Google Secret Manager","url":"/docs/gcp/oppsett-og-bruk-av-secret-manager#hvordan-komme-i-gang","content":" GSM fungerer ganske likt Vault. Vault har noe mer funksjonalitet for avansert bruk, men vi bruker for det meste som et KV secret store. For √• bruke GSM m√• det opprettes en secret, og denne secreten m√• tilgangsstyres.  ","version":"Next","tagName":"h2"},{"title":"Hvordan opprette Secret‚Äã","type":1,"pageTitle":"Oppsett og bruk av Google Secret Manager","url":"/docs/gcp/oppsett-og-bruk-av-secret-manager#hvordan-opprette-secret","content":"   Velg Security under navigasjonsmenyen (de tre strekene ved Google Cloud i h√∏yre hj√∏rne).Velg Secret Manager i venstre kolonne.Hvis API‚Äôet ikke er skrudd p√•, skru p√• API‚Äôet ved √• trykke ‚ÄúEnable‚ÄùTrykk p√• + CREATE SECRET    Name, og Value kan tenkes p√• som et Key/Value par. Resten av valgene trenger man ikke gj√∏re noe med med mindre man har spesielle behov. Noen felter man kan merke seg er:  Replication Policy: Dette er hvor hemmeligheten lagres. Det kan v√¶re en fordel √• lagre hemmeligheter i flere datacenter for redundans, vi har vanligvis holdt oss i europe-north1.  Encryption: Om det er spesielle behov for √• administrere krypteringsn√∏kkel selv er det ogs√• en mulighet. Dette m√• produktteamene ta ansvar for selv. SKIP teamet administrerer ikke krypteringsn√∏kler.  ","version":"Next","tagName":"h3"},{"title":"Tilgangsstyring‚Äã","type":1,"pageTitle":"Oppsett og bruk av Google Secret Manager","url":"/docs/gcp/oppsett-og-bruk-av-secret-manager#tilgangsstyring","content":" N√•r en secret er opprettet, kan man klikke seg inn p√• den, og velge PERMISSIONS fanen. Man f√•r da opp hvem som har tilgang til denne secreten, og hvilke rettigheter de har.    I de fleste tilfeller vil man bruke External Secret til √• hente ut disse hemmelighetene. Det kan gj√∏res ved √• opprette ExternalSecrets-ressurser i Kubernetes som henter ned hemmeligheten til en Kubernetes Secret. Det st√•r mer om dette inkludert tilgangsstyring p√• Hente hemmeligheter fra hemmelighetshvelv . ","version":"Next","tagName":"h3"},{"title":"Legge til eller fjerne personer fra et team","type":0,"sectionRef":"#","url":"/docs/generelt/add-remove-team-member","content":"","keywords":"","version":"Next"},{"title":"Mitt team √∏nsker tilgang til selvbetjente team-grupper‚Äã","type":1,"pageTitle":"Legge til eller fjerne personer fra et team","url":"/docs/generelt/add-remove-team-member#mitt-team-√∏nsker-tilgang-til-selvbetjente-team-grupper","content":" Dersom du ikke har tatt i bruk de nye gruppene enda, vil det kreves noe jobb for SKIP √• flytte tilganger fra de gamle CLOUD_SK-gruppene til TF - AAD - TEAM-gruppene. Dette er noe SKIP m√• gj√∏re, s√• ta kontakt med oss for √• gjennomf√∏re disse endringene.  Ta kontakt med SKIP i #gen-skip for √• f√• oppgradert til selvbetjente grupper.  F√∏rst m√• det bekreftes at gruppen som skal oppgraderes eksisterer. S√∏k etter den i Entra ID . Hvis den ikke finnes er ikke teamet onboardet p√• SKIP riktig og m√• legges inn i entra-id-config.  N√• m√• gruppene som gis tilgang i skip-core-infrastructure-repoet (tidligere IAM) byttes over fra CLOUD_SK-gruppene til TF - AAD-gruppene. Dette gj√∏res i teams-modulen. Stort sett er det bare √• endre fra en e-post til en annen slik at man ender opp med de nye aad-tf-gruppene. Dette vil v√¶re litt problematisk dersom teamet bruker locals.teams-abstraksjonen, s√• der b√∏r noe skrives om for √• st√∏tte TF - AAD-grupper i tillegg til de gamle ‚ÄúCLOUD_SK‚Äù-gruppene.  Etter IAM er oppdatert og kj√∏rt m√• gruppene som gis tilgang til Argo CD oppdateres. Dette gj√∏res i argocd.libsonnet. Finn UUID-en fra teamet i Entra ID og kopier Object ID inn. Deretter m√• argocd-apps synkes ut i alle milj√∏er.  Etter dette skal teamet v√¶re byttet over til det nye oppsettet. Sp√∏r teamet om de fortsatt har tilgangene de forventer. Merk at det kan ta noe tid f√∏r tilgangene er ordentlig inne, s√• om det ikke funker med en gang, alt ser riktig ut og alt er kj√∏rt kan det l√∏nne seg √• pr√∏ve igjen etter litt tid. ","version":"Next","tagName":"h2"},{"title":"Hva er et apps-repo","type":0,"sectionRef":"#","url":"/docs/argo-cd/hva-er-et-apps-repo","content":"","keywords":"","version":"Next"},{"title":"Mappestruktur‚Äã","type":1,"pageTitle":"Hva er et apps-repo","url":"/docs/argo-cd/hva-er-et-apps-repo#mappestruktur","content":" Du vil se at et apps-repo har en predefinert mappestruktur. Den ser omtrent slik ut:  dev/ # 1 foo-main/ # 2 app.yaml # 3   P√• toppniv√• (1) finner man mapper som gjenspeiler hvilket milj√∏ det skal synkroniseres til. Dette er enten dev, test eller prod.  P√• niv√• 2 finner man navnet p√• namespacet som det skal deployes til. Dette m√• starte med et gitt prefiks, vanligvis produktnavnet (i dette tilfellet heter produktet foo ). Etter prefikset kan man skrive hva man vil, vanligvis navnet p√• branchen i git som er deployed her. Dette kan v√¶re nyttig om man √∏nsker √• deploye en mer stabil main branch deployed i tillegg til √• deploye pull requests som testes live f√∏r de merges.  Niv√• 3, alts√• innholdet av mappen over, er et sett med en eller flere manifestfiler som beskriver applikasjonen. I eksempelet over vil app.yaml inneholde en Skiperator Application manifest som for eksempel kan se slik ut:  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: foo-frontend spec: image: kartverket/example port: 8080     N√•r vi putter hele dette eksemplet sammen vil f√∏lgende skje:  Produktteamet gj√∏r en endring i apps-repoetArgo CD vil etter kort tid lese apps-repoet og finne den endrede app.yaml filenArgo CD ser at den er plassert i dev og foo-main mappene og oppretter foo-main namespacet p√• dev-clusteretArgo CD legger Application definisjonen inn i namespacet p√• KubernetesSkiperator plukker opp endringen i namespacet og bygger ut Kubernetes-definisjonen for en applikasjon som skal kj√∏re kartverket/example imagetKubernetes puller container imaget og starter podder som kj√∏rer applikasjonen  ","version":"Next","tagName":"h2"},{"title":"Gjenbruke konfigurasjon‚Äã","type":1,"pageTitle":"Hva er et apps-repo","url":"/docs/argo-cd/hva-er-et-apps-repo#gjenbruke-konfigurasjon","content":" Man vil ofte f√• gjentagende konfigurasjon n√•r man f√•r flere applikasjoner, namespacer og milj√∏er. Det finnes metoder i Argo CD for √• gj√∏re konfigurasjonen gjenbrukbar, og du vil finne dokumentasjon om disse p√• Argo CD Tools .  Flere produktteam har l√∏st gjenbruk ved √• bruke http://jsonnet.org/ som er st√∏ttet ut av boksen med Argo. Man kan se et eksempel av dette p√• eiet-apps . SKIP jobber med et bibliotek med gjenbrukbare jsonnet-objekter .  Vi p√• SKIP anbefaler at dere starter med √• sjekke inn vanlige YAML-filer mens dere l√¶rer dere systemet. N√•r dere blir komfortable med Argo kan dere se p√• alternativene som er beskrevet over, da blir ikke l√¶ringskurven brattere enn n√∏dvendig.  ","version":"Next","tagName":"h2"},{"title":"Kildekode-repoer‚Äã","type":1,"pageTitle":"Hva er et apps-repo","url":"/docs/argo-cd/hva-er-et-apps-repo#kildekode-repoer","content":" Apps-repoer skal ikke inneholde kildekode. Apps-repoer har kun metadata om applikasjonen i form av manifest-filer. Dette kan man ogs√• lese om i Best Practices for Argo CD.  Dette gj√∏r at man f√•r et tydelig skille mellom kildekoderepoer og apps-repoer. Kildekoderepoer har ansvaret for √• lagre kode, bygge artefakter og container-imager. Apps-repoer beskriver den √∏nskede staten til applikasjonen p√• clusteret og Argo jobber mot √• bringe clusteret i synk med denne staten. Dette gj√∏r det ogs√• enkelt √• forholde seg til apps-repoene som en ‚Äúsingle source of truth‚Äù til applikasjonsstaten p√• clusteret.  ","version":"Next","tagName":"h2"},{"title":"Deploye automatisk ved push‚Äã","type":1,"pageTitle":"Hva er et apps-repo","url":"/docs/argo-cd/hva-er-et-apps-repo#deploye-automatisk-ved-push","content":"   Man √∏nsker ofte √• deploye ut nye versjoner av applikasjoner ved push til kildekoderepoer. Hvordan kan man gj√∏re dette med Argo CD?  Ved hvert push til et kildekoderepo kj√∏res et bygg for √• bygge et byggartefakt og bygge et container image. S√• snart dette imaget er pushet til et registry som ghcr.io vil man at dette skal legges ut p√• clusteret, og da m√• man oppdatere manifest-filene i apps-repoet. Man kan oppdatere disse filene manuelt for √• trigge en synk, men det er ogs√• mulig √• gj√∏re dette automatisk som en del av samme pipeline.  Etter imaget er publisert til ghcr.io puller bygget apps-repoet ved √• bruke https://github.com/actions/checkout. Deretter endres filene til √• inneholde referansen til det nye imaget, og disse filene commites lokalt. Hvordan disse filene endres er opp til produktteamet, men et forslag ligger i Automation from CI Pipelines. Til slutt pushes filene til repoet som vil trigge en synk med de oppdaterte manifestene.  Dette kan ogs√• gj√∏res med en PR istedenfor √• pushe rett til apps-repoet om man vil ha en godkjenning f√∏r deploy.  For √• logge inn p√• apps-repoet brukes metoden som beskrives i Tilgang til repoer med tokens fra GitHub Actions.  info Dersom man bruker Argo CD til √• opprette namespacer for alle branches og pull requests er det viktig √• slette branchene n√•r de ikke lenger er i bruk. Det er begrenset med kapasitet p√• clusterene og √• anskaffe hardware, b√•de on-prem og i sky, er ekstremt kostbart. Det holder √• slette filene i apps-repoet for √• rydde opp, noe som kan gj√∏res automatisk ved sletting av branches.  ","version":"Next","tagName":"h2"},{"title":"Eksempel p√• Github Actions‚Äã","type":1,"pageTitle":"Hva er et apps-repo","url":"/docs/argo-cd/hva-er-et-apps-repo#eksempel-p√•-github-actions","content":" name: build-and-deploy on: pull_requests: target: - main workflow_dispatch: push: branches: - main env: prefix: prefix jobs: build: # Her bygges et artefakt og et container image pushes til ghcr.io deploy-argo: needs: build runs-on: ubuntu-latest strategy: matrix: env: ['dev', 'test', 'prod'] steps: - uses: octo-sts/action@6177b4481c00308b3839969c3eca88c96a91775f # v1.0.0 id: octo-sts with: scope: kartverket/example-apps identity: example_name - name: Checkout apps repo uses: actions/checkout@v3 with: repository: kartverket/example-apps token: ${{ steps.octo-sts.outputs.token }} - name: Deploy to ${{ matrix.version }} run: | namespace=&quot;${{ env.prefix }}-${{ github.ref_name }}&quot; mkdir -p ./${{ matrix.version }}/$namespace cp -r templates/frontend.yaml ./${{ matrix.version }}/$namespace/frontend.yaml kubectl patch --local \\ -f ./${{ matrix.version }}/$namespace/frontend.yaml \\ -p '{&quot;spec&quot;:{&quot;image&quot;:&quot;${{needs.build.outputs.new_tag}}&quot;}}' \\ -o yaml git config --global user.email &quot;noreply@kartverket.no&quot; git config --global user.name &quot;GitHub Actions&quot; git commit -am &quot;Deploy ${{ matrix.version }} version ${{github.ref_name}}&quot; git push   name: clean-up-deploy on: delete: env: prefix: prefix jobs: delete-deployment: runs-on: ubuntu-latest strategy: matrix: env: ['dev', 'test', 'prod'] steps: - uses: octo-sts/action@6177b4481c00308b3839969c3eca88c96a91775f # v1.0.0 id: octo-sts with: scope: kartverket/example-apps identity: example_name - name: Checkout apps repo uses: actions/checkout@v3 with: repository: kartverket/example-apps token: ${{ steps.octo-sts.outputs.token }} - name: Delete ${{ matrix.version }} deploy run: | namespace=&quot;${{ env.prefix }}-${{ github.ref_name }}&quot; rm -rfv ./${{ matrix.version }}/$namespace git config --global user.email &quot;noreply@kartverket.no&quot; git config --global user.name &quot;GitHub Actions&quot; git commit -am &quot;Delete ${{ matrix.version }} deploy ${{github.ref_name}}&quot; git push  ","version":"Next","tagName":"h3"},{"title":"Vedlikehold av applikasjoner","type":0,"sectionRef":"#","url":"/docs/generelt/maintenance-of-apps","content":"","keywords":"","version":"Next"},{"title":"Stoppe kj√∏rende applikasjon i ArgoCD‚Äã","type":1,"pageTitle":"Vedlikehold av applikasjoner","url":"/docs/generelt/maintenance-of-apps#stoppe-kj√∏rende-applikasjon-i-argocd","content":" For √• kunne stoppe en kj√∏rende applikasjon som er administrert av ArgoCD m√• man f√∏rst v√¶re sikker p√• at autosync/self heal er deaktivert for produktteamet som eier applikasjonen. Hvis ikke vil bare applikasjonen spinne opp igjen automatisk.  Se denne filen for √• sjekke hva som er status, eventuelt sp√∏r noen p√• SKIP hvis du er usikker. Hvis ikke annet er satt kan du g√• ut i fra at autosync er skrudd p√• i dev og test, men avsl√•tt i prod.  For √• stoppe en applikasjon trykker du p√• menyen til en application-ressurs og velger ‚ÄúStop‚Äù. Dette vil midlertidig sette antall kopier til 0 slik at skiperator skalerer ned applikasjonen. Du vil da kunne se at pods forsvinner fra grensesnittet, og ‚ÄúSync Status‚Äù for applikasjonen vil st√• som ‚ÄúOutOfSync‚ÄùN√•r man er ferdig med vedlikeholdet og √∏nsker √• gjennopprette tidligere konfigurasjon trenger man bare √• trykke ‚ÄúSync‚Äù for at applikasjonen skal spinne opp igjen.  ","version":"Next","tagName":"h2"},{"title":"Stoppe kj√∏rende applikasjon manuelt‚Äã","type":1,"pageTitle":"Vedlikehold av applikasjoner","url":"/docs/generelt/maintenance-of-apps#stoppe-kj√∏rende-applikasjon-manuelt","content":" For √• stoppe en applikasjon som kj√∏rer p√• SKIP m√• man i praksis skalere ned antallet kj√∏rende kopier til 0. Den st√∏rste hindringen ved dette er en policy som vi h√•ndhever i prod-milj√∏et, som heter ‚ÄúK8sReplicaLimits‚Äù. Denne krever at en applikasjon skal ha mellom 2 og 30 kj√∏rende kopier til en hver tid.  For √• manuelt stoppe en skiperator-applikasjon er det to ting man m√• gj√∏re:  Sette en annotation for √• ignorere k8sReplicaLimits policySette antall replicas til 0  Se f√∏lgende eksempel p√• manifest som skalerer til 0  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: some-app annotations: skip.kartverket.no/k8sReplicaLimits: ignore spec: replicas: 0  ","version":"Next","tagName":"h2"},{"title":"üö¢ Generelt","type":0,"sectionRef":"#","url":"/docs/generelt","content":"üö¢ Generelt","keywords":"","version":"Next"},{"title":"Onboarding new product teams onto SKIP","type":0,"sectionRef":"#","url":"/docs/generelt/onboarding-new-teams","content":"","keywords":"","version":"Next"},{"title":"SKIP team tasks‚Äã","type":1,"pageTitle":"Onboarding new product teams onto SKIP","url":"/docs/generelt/onboarding-new-teams#skip-team-tasks","content":" ","version":"Next","tagName":"h2"},{"title":"Before onboarding‚Äã","type":1,"pageTitle":"Onboarding new product teams onto SKIP","url":"/docs/generelt/onboarding-new-teams#before-onboarding","content":" Invite a representative from the product team to the plattformlaug Dedicate a SKIP team member as a point of contact for the migration process (TAM) (Only for the migration process, after this is finished, a regular support flow is started) Invite to a meeting to clarify expectations between SKIP and the product team Invite to review applications Agree on the frequency of on-boarding standups with the product team and invite to these Ensure that a process is started around risk assessment (&quot;ROS-analyse&quot;). This assessment must be ready in time for production Create a channel on slack for collaboration during on-boarding Invite to #gen-skip, #gen-argo and other relevant common channels for using SKIP Invite to GCP and Kubernetes courses if the product team wants it Give an introduction to ArgoCD and best practices for this tool  ","version":"Next","tagName":"h3"},{"title":"During onboarding‚Äã","type":1,"pageTitle":"Onboarding new product teams onto SKIP","url":"/docs/generelt/onboarding-new-teams#during-onboarding","content":" Invite to a kickoff meeting where points of contact, distribution of responsibilities, support, roadmap and any other relevant issues are discussed.GitHub, given that the team has not used this beforeCreate groups by adding them to entra-id-configThe team need to be labeled with security in the admin.google.com . This can only be done through click-ops and only B√•rd and Eline have access, sadly.The team needs to be added to the IAM repositoryWorkflow in the IAM repository needs to be run by a SKIP member with access to do this.The teams are synced from AD into IAMIf the team requires Terraform: Service account for Terraform is set up through gcp-service-accounts and are granted access to its Kubernetes namespace with WIF.Terraform state is migrated/set upThe team and app-repository is set up in accordance with Komme i gang med Argo CD  ","version":"Next","tagName":"h3"},{"title":"Product team tasks‚Äã","type":1,"pageTitle":"Onboarding new product teams onto SKIP","url":"/docs/generelt/onboarding-new-teams#product-team-tasks","content":" The product team is responsible for delegating tasks among themselves.  Inform SKIP who is the team lead so they can administer the AD group Consider which team members need extra Kubernetes/GCP courses If ArgoCD is going to be used: Create new Apps repository in GitHub based on this SKIP template Ensure the application completes an IP and/or DPIA Adapt the application in order to satisfy SKIP's security requirements Read, understand and follow the GitHub security requirements: Sikkerhet p√• GitHub Finish the risk assessment document (ROS-analyse) Prepare information for the SKIP team, including technical expectations and service design/architecture Take responsibility for your own requirements and communicate these clearly and concisely to SKIP Ensure all team members are invited to meetings and Slack groups during the on-boarding process Read and understand the SKIP documentation Make the expected/required go-live date known to SKIP ","version":"Next","tagName":"h2"},{"title":"Sjekkliste f√∏r internett-eksponering","type":0,"sectionRef":"#","url":"/docs/generelt/sjekkliste-f√∏r-internett-eksponering","content":"Sjekkliste f√∏r internett-eksponering info Denne siden er under utarbeidelse og er et samarbeid mellom utvikling og sikkerhet For √• eksponere en applikasjon som kj√∏rer p√• SKIP mot internett m√• man: Opprette en DNS-record som ikke er under statkart.no-domenet, f.eks. applikasjonX.kartverket.no . Det gj√∏res ved √• opprette en ticket i PureService og be om at dette domenet skal peke mot SKIP-lastbalansereren (lb01.kartverket.no)Legge til det nye domenenavnet under ingresses i Skiperator-manifestet eller hostname for Routing-manifestet , slik at applikasjonen registrerer seg mot ekstern ingress gateway F√∏r dette kan gj√∏res m√• man g√• igjennom denne sjekklisten: Gj√∏r dere kjent med Overordnede f√∏ringer og spesielt Ansvarsfordeling fra Sikkerhetsh√•ndboka Opprett metadata om applikasjonen i henhold til Sikkerhet i repoet . Dette gj√∏r at applikasjonen blir knyttet opp i Utviklerportalen (fortsatt under arbeid) Foranalyse m√• v√¶re gjennomf√∏rt(Kommer l√∏ype for det i ServiceNow) Det er gjort IP (Innledende Personvernsvurdering) og eventuelt DPIA. Lag en kopi av malen p√• IP, DPIA og ROS-analyse for [det som vurderes]. IKKE SKRIV INN I MALEN, men kopier sidene. ROS-analyse gjennomf√∏rt og godkjent av risikoeier/systemeier Codeowners definert i koderepo CODEOWNERS Gjennomf√∏rt initiell penetrasjonstesting (hvem og hvordan?) eller manuell avsjekk med SKIP rundt konfigurasjon F√∏lgende headere blir sendt p√• alle kall: HTTP Strict Transport Security , Content Security Policy , X-Frame-Options , X-Content-Type-Options , Referrer Policy , Permissions Policy N√•r appen er eksponert er sikkerhetsheaders testet med https://securityheaders.com og https://observatory.mozilla.org Monitorering og varsling er satt opp i Grafana, og vaktlaget er onboardet disse alarmene Metrics with Grafana Logs with Loki Alerting with Grafana Denne sjekklisten gjelder eksponering av tjenester som skal v√¶re tilgjengelig p√• internett, uavhengig av milj√∏ (dev/prod). Hvis man √∏nsker og har behov for √• eksponere en applikasjon eksternt i dev m√• man i tillegg kontakte SKIP for √• sikre at alle sikkerhetskrav overholdes. Navnekonvensjon for eksternt tilgjengelig domenenavn vil i s√• fall v√¶re &lt;applikasjonX&gt;.atkv3-dev.kartverket.cloud","keywords":"","version":"Next"},{"title":"üóÉÔ∏è GitHub","type":0,"sectionRef":"#","url":"/docs/github","content":"üóÉÔ∏è GitHub Kartverket lagrer kildekode p√• github.com, og gjennom organisasjonen v√•r distribuerer vi tilgang ved √• fordele lisensene vi har kj√∏pt inn. For √• f√• tilgang f√∏lg sjekklisten under. info Ved sp√∏rsm√•l vedr√∏rende tilgang eller behov for st√∏tte, ta kontakt p√• #gen-github p√• slack Kom igang ved √• sjekke ut lenkene under: Tilgang til GitHubAutentisering til GitHub i terminalen (git clone / push med SSH)Opprette nytt repo p√• GitHubGitHub Actions som CI/CDH√•ndtering av sensitiv data som er kommet p√• repositorietBruk av GitHub med JenkinsTilgang til on-prem infrastruktur fra GitHub ActionsTilgang til repoer med tokens fra GitHub Actions","keywords":"","version":"Next"},{"title":"üß∞ GitHub Actions","type":0,"sectionRef":"#","url":"/docs/github-actions","content":"","keywords":"","version":"Next"},{"title":"Generelt‚Äã","type":1,"pageTitle":"üß∞ GitHub Actions","url":"/docs/github-actions#generelt","content":" GitHub actions er GitHubs CI/CD-system. Med dette systemet kan man kj√∏re bygg som er tett integrert med kodebasen og bruke et √∏kosystem av integrasjoner og ferdiglagde actions via GitHub Marketplace .  Dere kommer til √• m√∏te p√• en del forskjellige verkt√∏y n√•r dere skal deploye til SKIP:  SKIP er kj√∏remilj√∏et for containere i Kartverket. Vi regner ikke GitHub som en del av SKIP, men det er en s√• sentral komponent i √• deploye til SKIP-teamet er med √• drifte GitHub-organisasjonen til KartverketGitHub Actions som er CI/CD-milj√∏et for √• kj√∏re jobber som √• bygge containere fra kildekode og kj√∏re terraform plan og applyTerraform som er IaC -verkt√∏yet som lar oss beskrive det √∏nskede milj√∏et i kode og eksekverer kommandoer for √• modifisere milj√∏et slik at det blir slik som beskrevetgithub-workflows som er gjenbrukbare jobber man kan bruke i sine pipelines for √• gj√∏re oppsettet lettere. Denne inneholder hovedsakelig den gjenbrukbare jobben ‚Äúrun-terraform‚Äù. Denne kan benyttes for √• enkelt autentisere seg mot GCP og bruke terraform p√• en sikker m√•te.Google Cloud og Google Anthos som er milj√∏et som kj√∏rer Kubernetes -milj√∏et hvor containerene kj√∏rerskiperator er en operator som gj√∏r det enklere √• sette opp en applikasjon som f√∏lger best practices. Skiperator definerer en Application custom resource som blir fylt ut av produktteamene og deployet med TerraformNacho SKIP signerer container images med en kryptografisk signatur etter de er bygget  GItHub Actions er et CI-systemet som SKIP legger opp til at alle produktteam skal kunne bruke for √• automatisere bygging av Docker-images i tillegg til muligheter for √• opprette infrastruktur i skyen ved hjelp av Terraform p√• en automatisert m√•te.Actions lages ved √• skrive YAML-filer i .github/workflows -mappa i roten av repoet. Man kan ogs√• trykke p√• ‚ÄúActions‚Äù og ‚ÄúNew workflow‚Äù i GitHub og f√• opp dialogen over. Der kan man velge fra et eksisterende bibliotek med eksempler p√• Actions som kan hjelpe med √• komme i gang med en action. For eksempel kan man trykke ‚ÄúView all‚Äù p√• ‚ÄúContinous Integration‚Äù for √• finne eksempler p√• hvordan man bygger med java eller node.js. DIsse er ofte gode utgangspunkt n√•r man skal sette opp et nytt bygg.  Les https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions for en introduksjon til Actions.  Se https://docs.github.com/en/actions/learn-github-actions/workflow-syntax-for-github-actions for referanse av mulige verdier.  ","version":"Next","tagName":"h2"},{"title":"Lagring av images‚Äã","type":1,"pageTitle":"üß∞ GitHub Actions","url":"/docs/github-actions#lagring-av-images","content":" Det anbefalte m√•ten √• publisere images er n√• til GitHub Container Registry ( ghcr.io ). Dette kan gj√∏res enkelt ved hjelp av GitHub Actions.  Se denne artikkelen for mer informasjon om ghcr: https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry .  Eksempler for publisering av container images til GitHub finnes her .  Dersom dere bruker metoden over vil dere merke at dere ikke trenger √• sette tags p√• docker imaget dere bygger. Dette vil settes automatisk basert p√• en ‚Äúsane default‚Äù ut i fra hvilke branch man er p√• og hvilke kontekst bygget gj√∏res i (commit, PR, tag). De resulterende taggene er dokumentert her . Tags kan ogs√• tilpasses om ikke default er passende for prosjektet.  Resultatet blir √• finne p√• GitHub repositoriet til koden og ser slik ut:    ","version":"Next","tagName":"h2"},{"title":"Deployment‚Äã","type":1,"pageTitle":"üß∞ GitHub Actions","url":"/docs/github-actions#deployment","content":" For deployment brukes Argo CD som det dedikert deployment-verkt√∏y. Se Argo CD for mer informasjon om hvordan man tar i bruk dette.  Det vil finnes prosjekter som bruker Terraform, enten fordi de hadde oppstart f√∏r Argo CD eller fordi de har spesielle behov som tilsier at de trenger Terraform. Disse prosjektene kan se p√• Bruk av Terraform for videre dokumentasjon. For nye prosjekter anbefaler vi Argo CD. ","version":"Next","tagName":"h2"},{"title":"Oversikt over tjenester SKIP tilbyr","type":0,"sectionRef":"#","url":"/docs/generelt/oversikt-over-tjenester-SKIP-tilbyr","content":"","keywords":"","version":"Next"},{"title":"Grafana‚Äã","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/generelt/oversikt-over-tjenester-SKIP-tilbyr#grafana","content":" Grafana Loki er et logglagringsverkt√∏y som brukes som datakilde for Grafana.  Grafana Mimir lagrer metrikker fra appliasjoner, og brukes som datakilde for Grafana.  Grafana Tempo lagrer tracing for applikasjoner, og brukes som datakilde for Grafana  Brukes for √• sende ut varslinger basert p√• data i grafana.  ","version":"Next","tagName":"h2"},{"title":"Google Secret Manager‚Äã","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/generelt/oversikt-over-tjenester-SKIP-tilbyr#google-secret-manager","content":" For hemmelighetsh√•ndtering anbefaler vi bruk av Google Secret Manager (GSM). Her har vi solid adgangskontroll og kan enkelt hente hemmeligheter b√•de i build time og run time til applikasjoner vi kj√∏rer i Kubernetes.  I GSM opprettes hemmeligheter per prosjekt, og man kan adgangskontrollere b√•de for et helt prosjekt og for individuelle hemmeligheter. Hemmeligheter kan versjoneres og rulleres automatisk.  Ved hjelp av et system kalt External Secrets er det enkelt √• hente disse hemmelighetene til build time. Se Hente hemmeligheter fra hemmelighetshvelv .  For √• hente hemmeligheter fra GSM under run time, se Autentisering mot GCP fra Applikasjon .  ","version":"Next","tagName":"h2"},{"title":"GitHub‚Äã","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/generelt/oversikt-over-tjenester-SKIP-tilbyr#github","content":" GitHub er en skybasert git-repository-tjeneste som vi bruker til √• lagre kildekoden til Kartverkets prosjekter. Med GitHub f√•r vi ogs√• mye annet ogs√• som kontinuerlig integrasjon, kodescanning.  ","version":"Next","tagName":"h2"},{"title":"Objektlagring‚Äã","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/generelt/oversikt-over-tjenester-SKIP-tilbyr#objektlagring","content":" SKIP tilbyr flere objektlagringstjenester som blant annet gir deg mulighet √• lagre filer i sky eller on-prem.  For √• lagre filer i sky anbefaler vi √• benytte Google cloud storage . Dette er en lagringstjeneste som f√∏lger med Google Cloud Platform. Her kan du f. eks provisjonere b√∏tter via terraform, og laste opp filer til denne b√∏tten via en applikasjon p√• et Kubernetes cluster. Se Autentisering mot GCP fra Applikasjon for √• koble seg til GCP via en applikasjon.  I tillegg til lagring med Google cloud storage s√• har man mulighet til √• benytte Scality on-prem som er et AWS S3-kompatibel l√∏sning.  ","version":"Next","tagName":"h2"},{"title":"Continuous Deployment‚Äã","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/generelt/oversikt-over-tjenester-SKIP-tilbyr#continuous-deployment","content":" ArgoCD er et deklarativt, GitOps-kontinuerlig leveranseverkt√∏y for Kubernetes-applikasjoner. Det automatiserer distribusjon og administrasjon av applikasjoner i Kubernetes ved √• synkronisere den √∏nskede tilstanden som er definert i Git-repositorier med den faktiske cluster konfigurasjonen. ","version":"Next","tagName":"h2"},{"title":"Tilgang til on-prem infrastruktur fra GitHub Actions","type":0,"sectionRef":"#","url":"/docs/github-actions/tilgang-til-on-prem-infrastruktur-fra-github-actions","content":"","keywords":"","version":"Next"},{"title":"Bakgrunn‚Äã","type":1,"pageTitle":"Tilgang til on-prem infrastruktur fra GitHub Actions","url":"/docs/github-actions/tilgang-til-on-prem-infrastruktur-fra-github-actions#bakgrunn","content":" warning Tailscale i denne konteksten er ment som et hjelpemiddel for √• migrere pakker ut til et ekstern pakkeregister, og som et verkt√∏y for √• bli kvitt interne avhengigheter. Anbefales ikke for allmenn bruk.  For √• underst√∏tte produktteamene med √• migrere bort fra intern kode- og artifakthosting, samt avhengigheter p√• interne databaser har SKIP introdusert Tailscale.  Tailscale er en mesh-basert peer-to-peer VPN-l√∏sning, som du kan lese mer om i deres egen dokumentasjon .  ","version":"Next","tagName":"h2"},{"title":"Komme i gang‚Äã","type":1,"pageTitle":"Tilgang til on-prem infrastruktur fra GitHub Actions","url":"/docs/github-actions/tilgang-til-on-prem-infrastruktur-fra-github-actions#komme-i-gang","content":" Kontakt en GitHub-administrator for √• be om tilgang for ditt repository  Hei $NAVN! Teamet mitt trenger tilgang til √• benytte Tailscale p√• repoet https://github.com/kartverket/mittRepo . Jeg trenger at du granter organisasjonshemmelighetene TS_OAUTH_CLIENT_ID og TS_OAUTH_SECRET (+ tilsvarende for Dependabot org-wide) p√• repoet, s√• klarer vi resten selv.  P√• forh√•nd takk üôå  Etter du har f√•tt tilgang til hemmelighetene, legg til f√∏lgende i din GitHub workflow  - name: Tailscale uses: tailscale/github-action@v2 with: oauth-client-id: ${{ secrets.TS_OAUTH_CLIENT_ID }} oauth-secret: ${{ secrets.TS_OAUTH_SECRET }} tags: tag:github-runner   Du kan n√• benytte deg av utvalgte interne tjenester. Lykke til!  Vil du vite hvilke tjenester du f√•r tilgang til eller behov for flere tjenester enn dagens utvalg? Ta kontakt med SKIP p√• Slack. ","version":"Next","tagName":"h2"},{"title":"Tilgang til repoer med tokens fra GitHub Actions","type":0,"sectionRef":"#","url":"/docs/github-actions/tilgang-til-repoer-med-tokens-fra-github-actions","content":"","keywords":"","version":"Next"},{"title":"Secure Token Service (STS)‚Äã","type":1,"pageTitle":"Tilgang til repoer med tokens fra GitHub Actions","url":"/docs/github-actions/tilgang-til-repoer-med-tokens-fra-github-actions#secure-token-service-sts","content":" En Secure Token Service (STS) er en tjeneste som utsteder sikkerhetstokener som kan brukes til autentisering og autorisering i ulike systemer og applikasjoner. I v√•rt tilfelle √∏nsker vi √• utstede kortlevde tokens som kun er gyldige i perioden de brukes som en erstatning for PAT-er. Vi har derfor implementert et verkt√∏y som heter Octo STS for √• levere denne funksjonaliteten.  M√•ten STS fungerer p√• er at man etablerer tillit mellom to repoer. Dette gj√∏res ved √• legge inn en konfigurasjonsfil i repoet du √∏nsker √• ha tilgang til som sier noe om hvem som skal kunne f√• tilgang til repoet. Deretter bruker man en ferdig GitHub action i repot som skal f√• tilgang til √• etablere et kortlevd tiken via STS-tjenesten.  Les denne artikkelen for mer detaljer om Octo STS.  ","version":"Next","tagName":"h2"},{"title":"Etablere tillit‚Äã","type":1,"pageTitle":"Tilgang til repoer med tokens fra GitHub Actions","url":"/docs/github-actions/tilgang-til-repoer-med-tokens-fra-github-actions#etablere-tillit","content":" F√∏rst m√• man etablere tillit ved √• legge inn en config-fil i repoet man skal f√• tilgang til. Dette legges i mappen .github/chainguard/&lt;navn&gt;.sts.yaml . Erstatt &lt;navn&gt; med identiteten som skal ha tilgang og bruk dette navnet i GitHub actionen senere.  Eksempelet under viser hvordan man gir tilgang fra GitHub actions som kj√∏rer p√• repoet kartverket/mittrepo p√• branchen main .  issuer: https://token.actions.githubusercontent.com subject: repo:kartverket/mittrepo:ref:refs/heads/main permissions: contents: write   Dersom du √∏nsker √• bruke et wildcard til √• gi tilgang, for eksempel dersom det deployes ved hjelp av ‚Äúenvironments‚Äù i GitHub slik at dette blir subjektet ditt kan man bruke et subject_pattern . Dette er et regex.  issuer: https://token.actions.githubusercontent.com subject_pattern: repo:kartverket\\/mittrepo:environment:(sandbox|prod) permissions: contents: write   ","version":"Next","tagName":"h3"},{"title":"F√• tilgang‚Äã","type":1,"pageTitle":"Tilgang til repoer med tokens fra GitHub Actions","url":"/docs/github-actions/tilgang-til-repoer-med-tokens-fra-github-actions#f√•-tilgang","content":" N√•r man skal ha tilgang til dette repoet s√• bruker man en GitHub action til √• snakke med STS-tjenesten og f√• en kortlevd token som brukes p√• samme m√•te som en PAT. For en deploy til et apps-repo kan du for eksempel skrive f√∏lgende i din GitHub action:  permissions: id-token: write # Required for Octo STS steps: - uses: octo-sts/action@6177b4481c00308b3839969c3eca88c96a91775f # v1.0.0 id: octo-sts with: scope: kartverket/skip-apps identity: utviklerportal - name: Checkout apps repo uses: actions/checkout@v4 with: repository: kartverket/skip-apps token: ${{ steps.octo-sts.outputs.token }}   N√•r dette blir kj√∏rt vil det bli gjort en sp√∏rring til Octo STS-tjenesten, som deretter sjekker filen vi laget i repoet over og om det har blitt etablert tillit. Dersom dette er tilfellet s√• genereres en token som brukes i dette eksempelet til √• sjekke ut et annet repo.  Se ogs√• https://github.com/octo-sts/action for dokumentasjon p√• GitHub actionen. ","version":"Next","tagName":"h3"},{"title":"Autentisering med Workload Identity Federation","type":0,"sectionRef":"#","url":"/docs/github-actions/autentisering-med-workload-identity-federation","content":"","keywords":"","version":"Next"},{"title":"Oppsett av GitHub Action‚Äã","type":1,"pageTitle":"Autentisering med Workload Identity Federation","url":"/docs/github-actions/autentisering-med-workload-identity-federation#oppsett-av-github-action","content":" N√•r man skal sette opp autentisering mot GCP med Workload Identity Federation er det en fordel √• ha lest gjennom GitHub sin artikkel om https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/configuring-openid-connect-in-google-cloud-platform#updating-your-github-actions-workflow , og spesifikt kapittelet som heter ‚ÄúUpdating your GitHub Actions workflow‚Äù. Her beskriver de de to trinnene man m√• gj√∏re:  Konfigurere tilgang til √• generere ID-tokensBruke https://github.com/google-github-actions/auth actionen til √• autentisere mot GCP  SKIP-teamet vil ha konfigurert en workload identity provider og service account som dere kan putte rett inn i provideren over. Disse er ikke hemmelige men vil variere avhengig av milj√∏ man skal deploye mot, s√• det kan v√¶re hensiktsmessig √• ha de som variabler, som vist lenger nede.  permissions: contents: read id-token: write jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - id: auth name: Authenticate to GCP uses: google-github-actions/auth@v0 with: workload_identity_provider: projects/your-project-number/locations/global/workloadIdentityPools/your-pool/providers/your-provider service_account: your-account@your-project.iam.gserviceaccount.com project_id: kubernetes-dev-94b9   Eventuelt kan du ha en egen setup-env jobb som lager outputs du kan bruke senere, slik at provider, service account og project id er variabler i stedet for hardkodede strings.  Eksempel:  permissions: contents: read id-token: write env: PROJECT_ID: kubernetes-dev-94b9 SERVICE_ACCOUNT: your-account@your-project.iam.gserviceaccount.com WORKLOAD_IDENTITY_PROVIDER: projects/your-project-number/locations/global/workloadIdentityPools/your-pool/providers/your-provider jobs: setup-env: runs-on: ubuntu-latest outputs: project_id: ${{ steps.set-output.outputs.project_id }} service_account: ${{ steps.set-output.outputs.service_account }} workload_identity_provider: ${{ steps.set-output.outputs.workload_identity_provider }} steps: - name: Set outputs id: set-output run: | echo &quot;project_id=$PROJECT_ID&quot; &gt;&gt; $GITHUB_OUTPUT echo &quot;service_account=$SERVICE_ACCOUNT&quot; &gt;&gt; $GITHUB_OUTPUT echo &quot;workload_identity_provider=$WORKLOAD_IDENTITY_PROVIDER&quot; &gt;&gt; $GITHUB_OUTPUT build: needs: [setup_env] runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - id: auth name: Authenticate to GCP uses: google-github-actions/auth@v0 with: workload_identity_provider: ${{ needs.setup-env.outputs.workload_identity_provider }} service_account: ${{ needs.setup-env.outputs.service_account }} project_id: ${{ needs.setup-env.outputs.project_id }} build-again: needs: [setup_env] runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - id: auth name: Authenticate to GCP uses: google-github-actions/auth@v0 with: workload_identity_provider: ${{ needs.setup-env.outputs.workload_identity_provider }} service_account: ${{ needs.setup-env.outputs.service_account }} project_id: ${{ needs.setup-env.outputs.project_id }}  ","version":"Next","tagName":"h2"},{"title":"Kubectl fra GitHub Actions","type":0,"sectionRef":"#","url":"/docs/github-actions/kubectl-fra-github","content":"","keywords":"","version":"Next"},{"title":"Oppsett‚Äã","type":1,"pageTitle":"Kubectl fra GitHub Actions","url":"/docs/github-actions/kubectl-fra-github#oppsett","content":" F√∏r du kan bruke denne actionen m√• du gj√∏re noen endringer i gcp-service-accounts og i ditt teams apps-repo.  ","version":"Next","tagName":"h2"},{"title":"1. Legg til ekstra permissions til deploy service accounten‚Äã","type":1,"pageTitle":"Kubectl fra GitHub Actions","url":"/docs/github-actions/kubectl-fra-github#1-legg-til-ekstra-permissions-til-deploy-service-accounten","content":" run-kubectl tar i bruk Workload Identity Federation som du kan lese mer om her, men den krever ogs√• ekstra tilganger for √• kunne koble til clusteret. I gcp-service-accounts har du sannsynligvis definert opp ditt gcp project for √• kunne bruke det i GitHub Actions, og dermed f√•tt laget en deploy service account og et workload identity pool. Da m√• du bare legge til en ekstra rolle i modul-definisjonen slik:  module &quot;utviklerportal&quot; { source = &quot;./project_team&quot; team_name = &quot;utviklerportal&quot; repositories = [ &quot;kartverket/kartverket.dev&quot;, ] env = var.env project_id = var.utviklerportal_project_id kubernetes_project_id = var.kubernetes_project_id extra_kubernetes_sa_roles = [ &quot;roles/container.clusterViewer&quot;, # &lt;--- Legg til denne linjen i extra_kubernetes_sa_roles ] }   N√• skal deploy kontoen kunne koble seg til clusteret.  ","version":"Next","tagName":"h3"},{"title":"2. Legg til role og rolebinding i ditt apps-repo‚Äã","type":1,"pageTitle":"Kubectl fra GitHub Actions","url":"/docs/github-actions/kubectl-fra-github#2-legg-til-role-og-rolebinding-i-ditt-apps-repo","content":" For at man skal kunne f.eks restarte et deployment, s√• m√• vi legge til en kubernetes rbac rolle som gir kontoen tilgang til dette.  I apps repoet, legg til:  kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: deployment-restart-role rules: - apiGroups: [&quot;apps&quot;] resources: [&quot;deployments&quot;] verbs: [&quot;get&quot;, &quot;patch&quot;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: deploy-sa-rolebinding subjects: - kind: User name: your-project-deploy@your-project-id.iam.gserviceaccount.com roleRef: kind: Role name: deployment-restart-role apiGroup: rbac.authorization.k8s.io   navn p√• service accounten er &quot;modulnavn&quot;-deploy, hvor modulnavn finnes i gcp-service-accounts. du kan ogs√• finne den med gcloud config set project &lt;projectid&gt; &amp;&amp; gcloud iam service-accounts list | grep deploy  ","version":"Next","tagName":"h3"},{"title":"3. Legg til GitHub workflow‚Äã","type":1,"pageTitle":"Kubectl fra GitHub Actions","url":"/docs/github-actions/kubectl-fra-github#3-legg-til-github-workflow","content":" N√• skal alt v√¶re konfigurert og du kan legge til en GitHub workflow som kj√∏rer run-kubectl workflow.  eksempel:  name: Get pods on: push jobs: sandbox: name: get pods uses: kartverket/github-workflows/.github/workflows/run-kubectl.yaml@4.2.2 with: cluster_name: atgcp1-sandbox service_account: test-deploy@test-sandbox-5cx6.iam.gserviceaccount.com kubernetes_project_id: kube-sandbox-6e32 project_number: 833464945837 namespace: default commands: | get pods get pods -l app=nginx   Forklaring:  cluster_name: navnet p√• clusteret du vil koble til, dette kan du finne med gcloud container fleet memberships list. mer herservice_account: navnet p√• service accounten som skal brukes. denne blir opprettet i gcp-service-accounts, og slutter p√• -deploykubernetes_project_id: id til prosjektet som clusteret ligger i, finnes med gcloud projects list | grep kubernetesproject_number: nummeret til prosjektet som service accounten ligger i, dette er produkt prosjektet, finnes med gcloud projects list | grep produktcommands: kubectl kommandoene du vil kj√∏re, uten kubectl foran. husk √• bruk multiline string.namespace: namespace du vil kj√∏re kommandoen ikubectl_version: versjonen av kubectl du vil bruke, default er latest stable. format: v1.30.0 ","version":"Next","tagName":"h3"},{"title":"Bruk av GitHub med Jenkins","type":0,"sectionRef":"#","url":"/docs/github/bruk-av-github-med-jenkins","content":"","keywords":"","version":"Next"},{"title":"üìö Autentisering üìö‚Äã","type":1,"pageTitle":"Bruk av GitHub med Jenkins","url":"/docs/github/bruk-av-github-med-jenkins#-autentisering-","content":" Det er flere m√•ter √• autentisere Jenkins mot GitHub p√•, blant annet; deploy keys, personal access tokens, GitHub App. Vi vil se at GitHub Apps er valget vi g√•r for n√•r vi autentiserer.   Deploy keys er enkle men;  üëç Eies av repoet og Jenkins (priv + pub n√∏kler)üëä Kan kun brukes som ‚ÄúGit‚Äù source p√• Jenkinsüëé Snakker ikke med GitHub sitt API - kun pulle / pushe kode   Personal access tokens **** (PAT) gir mer;  üëç Kan brukes gjennom ‚ÄúGitHub‚Äù plugin p√• Jenkins (source)üëç Snakker med GitHub API‚Äôet - PR/Commit status triggere etc.üëé N√∏kkelen f√∏lger brukeren, selv etter vedkommende bytter team eller slutter (kan slettes fra bruker)üëé Ikke i utgangspunktet gjenbrukbar (beta- fine grained PAT‚Äôer kan tilegnes flere repo pr. n√∏kkel)   GitHub Apps er litt mer √• konfigurere, men er en kombinasjon av de over;  üëç Gjenbrukbare, som flere repoer kan bruke gjennom √©n privat n√∏kkel p√• Jenkins.üëç Eies av ‚ÄúOrganisasjonen‚Äù Kartverket p√• Github, som da ikke er bundet til en GitHub bruker.üëç Kan brukes gjennom ‚ÄúGitHub‚Äù plugin p√• Jenkins (source)üëç Snakker med GitHub API‚Äôet - PR/Commit status triggere etc.üëé Ratelimit (men skal ikke v√¶re et problem)  ","version":"Next","tagName":"h2"},{"title":"üßë‚Äçüöí Brannmurer üßë‚Äçüöí‚Äã","type":1,"pageTitle":"Bruk av GitHub med Jenkins","url":"/docs/github/bruk-av-github-med-jenkins#-brannmurer-","content":" I utgangspunktet s√• skal portene til ditt Jenkins milj√∏ v√¶re √•pnet, slik at Jenkins n√•r ut til GitHub. Men hvis det dette er f√∏rste gang s√• m√• de √•pnes for trafikk mot GitHub. Prim√¶rt er det HTTPs og SSH trafikk som m√• tilgjengeliggj√∏res p√• port 443 og 22. Dette m√• bestilles hos drift.  ","version":"Next","tagName":"h2"},{"title":"ü™ù Webhook ü™ù‚Äã","type":1,"pageTitle":"Bruk av GitHub med Jenkins","url":"/docs/github/bruk-av-github-med-jenkins#-webhook-","content":" Work in progress. Er ikke ferdig testet enda.  For √• f√• status p√• PR/Commits i GitHub s√• m√• GitHub ha en vei inn til Jenkins. Dette gj√∏res p√• et webhook endepunkt typisk seende slik ut https://&lt;jenkins-host&gt;/github-webhook/ . Dette er noe som m√• √•pnes fra drift og spesifiseres inne i GitHub Appen.  ‚öôÔ∏è Legg til hvordan det er med webhook secret.  ","version":"Next","tagName":"h2"},{"title":"üìÅ Oppsett av GitHub App üìÅ‚Äã","type":1,"pageTitle":"Bruk av GitHub med Jenkins","url":"/docs/github/bruk-av-github-med-jenkins#-oppsett-av-github-app-","content":" SKIP kontaktes og de setter opp en App for ditt behov. Er denne som f√∏lges: Using GitHub App authentication .  info Oppsettet av n√∏klen m√• du gj√∏re selv! Og dette F√òR du f√•r brukt Appen, men ETTER at SKIP setter igang med oppsett av app. SKIP sender melding n√•r du m√• gj√∏re dette. nb: skal Appen ha flere/mindre rettigheter enn i oppskriften m√• du spesifisere dette  N√•r SKIP har satt opp Appen, m√• du sette den private n√∏kkelen, som senere skal deles med Jenkins. Dette gj√∏res slik som beskrevet i punktet Generating a private key for authenticating to the GitHub App .  Det er f√∏rst n√•r dette er gjort, at SKIP kan installere Appen p√• organisasjonen. Send en heads-up at du har lagret n√∏kkelen.  Hvis Appen er installert i org. og linket til ditt repo, og n√∏kkelen er satt opp i App og Jenkins s√• skal alt v√¶re p√• plass! üéâ ","version":"Next","tagName":"h2"},{"title":"H√•ndtering av sensitiv data som er kommet p√• repositoriet","type":0,"sectionRef":"#","url":"/docs/github/h√•ndtering-av-sensitiv-data-som-er-kommet-p√•-repositoriet","content":"","keywords":"","version":"Next"},{"title":"üìò Instruksjoner‚Äã","type":1,"pageTitle":"H√•ndtering av sensitiv data som er kommet p√• repositoriet","url":"/docs/github/h√•ndtering-av-sensitiv-data-som-er-kommet-p√•-repositoriet#-instruksjoner","content":" for √• se om secret scanning har avduket noen sensitive data i repositoriet g√• inn p√• repositorierts forside og klikk deg inn p√• security-fanen og deretter trykk deg inn p√• sidemeny-valget ‚ÄúSecret scanning alerts‚Äùtrykk deg inn p√• det varselet for det sensitive dataen du skal l√∏seher f√•r du vite hvilke filer det er snakk om og akkurat hvilken linje det er snakk om.Deretter er det √• f√∏lge denne guiden, Fjerne sensitive data fra repositorier for selve fjerningen av de sensitive dataenen√•r fjerningen er gjor kan man lukke varslet  ","version":"Next","tagName":"h2"},{"title":"üìã Relaterte artikler‚Äã","type":1,"pageTitle":"H√•ndtering av sensitiv data som er kommet p√• repositoriet","url":"/docs/github/h√•ndtering-av-sensitiv-data-som-er-kommet-p√•-repositoriet#-relaterte-artikler","content":" Fjerne sensitive data fra repositorier  Secret Scanning ","version":"Next","tagName":"h2"},{"title":"Opprette nytt repo p√• Github","type":0,"sectionRef":"#","url":"/docs/github/opprette-nytt-repo-p√•-github","content":"","keywords":"","version":"Next"},{"title":"Merknad for produkter som ikke er p√• SKIP‚Äã","type":1,"pageTitle":"Opprette nytt repo p√• Github","url":"/docs/github/opprette-nytt-repo-p√•-github#merknad-for-produkter-som-ikke-er-p√•-skip","content":" Merk at det meste av dette dokumentet ogs√• er gyldig for prosjekter som ikke er p√• SKIP-plattformen - men at det likevel er skrevet for SKIP-teams, s√• sikkerhetsreglene kan sees p√• som gode r√•d dersom du ikke skal bruke SKIP.  For ikke √• snakke om at du dersom du f√∏lger disse sikkerhetsreglene vil f√• en mye enklere jobb hvis du skal flytte prosjektet over til SKIP i fremtiden   ","version":"Next","tagName":"h3"},{"title":"Hvordan opprette et nytt GitHub Repository‚Äã","type":1,"pageTitle":"Opprette nytt repo p√• Github","url":"/docs/github/opprette-nytt-repo-p√•-github#hvordan-opprette-et-nytt-github-repository","content":" Logg inn p√• GitHubOpprett et nytt repository ved √• trykke p√• pluss-ikonet √∏verst til h√∏yre p√• https://github.com og velge ‚ÄúNew repository‚Äù. Dette gjelder uansett om du skal lage et nytt prosjekt eller importere et eksisterende prosjekt, siden du ikke vil kunne bruke ‚ÄúImport‚Äù-funksjonaliteten p√• vanlig m√•te.Dersom du skal importere et eksisterende git-repository, f√∏lg denne tutorialen .Fyll ut skjemaet med riktig informasjon. Huskeregler: Alle prosjekter som ikke skal v√¶re √•pne skal v√¶re Internal . Det er likevel mulig √• invitere eksterne utviklere. Mer informasjon: https://docs.github.com/en/repositories/creating-and-managing-repositories/about-repositories#about-repository-visibilityPass p√• at Owner er satt til kartverket , og ikke din private bruker.Ikke velg en lisens med mindre du faktisk skal lage et open-source prosjekt. √Ö velge en √•pen kildekode-lisens her kan √∏delegge for sikkerhetsverkt√∏yene i Kartverket og i siste instans skape legale problemer for Kartverket. Hvis du er i tvil, ta kontakt med SKIP-teamet. Dokumenter hvilket team som er ansvarlig for repositoriet ved √• opprette en Codeowners fil.Dette er dokumentert her .Som regel er det nok med en linje - slik (bytt ut skip med ditt eget team).Gi teamet ditt rettigheter til repoet. Dette er dokumentert her . Det er vanlig √• sette Tech Lead som eier for repositoriet, men dette bestemmer dere selv.  ","version":"Next","tagName":"h2"},{"title":"Opprett tilganger til Google Cloud for Github Actions‚Äã","type":1,"pageTitle":"Opprette nytt repo p√• Github","url":"/docs/github/opprette-nytt-repo-p√•-github#opprett-tilganger-til-google-cloud-for-github-actions","content":" Dersom du har behov til √• autentisere deg mot GCP kan du legge til at ditt repo GitHub kan autentisere seg mot Google Cloud med en bestemt bruker. Da m√• man sette opp Workload Identity Federation . Dette er noe SKIP ordner for produktteamene p√• en automatisert m√•te ved hjelp av Terraform.  √ònsker du √• legge til et nytt repo kan du opprette en Pull Request for dette repoet: https://github.com/kartverket/gcp-service-accounts  Eksempel p√• liste over GitHub repoer for KomReg: https://github.com/kartverket/gcp-service-accounts/blob/main/modules.tf  module &quot;komreg&quot; { source = &quot;./project_team&quot; team_name = &quot;KomReg&quot; repositories = [ &quot;kartverket/komreg-frontend&quot;, &quot;kartverket/komreg-backend&quot;, &quot;kartverket/komreg-frontend-api&quot;, # Legg til flere repoer i denne listen ] env = var.env project_id = var.komreg_project_id kubernetes_project_id = var.kubernetes_project_id can_manage_log_alerts_and_metrics = true can_manage_sa = true extra_team_sa_roles = [ &quot;roles/resourcemanager.projectIamAdmin&quot;, &quot;roles/secretmanager.admin&quot;, &quot;roles/storage.admin&quot; ] }   N√•r PR‚Äôen merges inn vil det ved et nytt team bli opprettet en deploy-servicekonto, som heter &lt;teamnavn&gt;-deploy@&lt;prosjekt-id&gt;.iam.gserviceaccount.com . Denne servicekontoen tillater at github-repoene i listen har lov til √• etterligne den og dens tilganger.  Mer informasjon om Github Actions: GitHub Actions som CI/CD ","version":"Next","tagName":"h2"},{"title":"Tilgang til GitHub","type":0,"sectionRef":"#","url":"/docs/github/tilgang-til-github","content":"","keywords":"","version":"Next"},{"title":"Bistand og diskusjon rundt GitHub‚Äã","type":1,"pageTitle":"Tilgang til GitHub","url":"/docs/github/tilgang-til-github#bistand-og-diskusjon-rundt-github","content":" Logg p√• slack ved √• laste ned programmet fra http://slack.com og bruk kartverketgroup.slack.com som workspaceTa kontakt med SKIP p√• slack i #gen-github for √• f√• en invitasjon til GitHub-organisasjonen til Kartverket (send github-brukernavnet ditt). Dersom du vet p√• forh√•nd at du jobber som del av et team s√• fortell oss hvilke team dette er s√• f√•r vi lagt deg i tilsvarende team i GitHub   ","version":"Next","tagName":"h2"},{"title":"Bruk av Terraform","type":0,"sectionRef":"#","url":"/docs/github-actions/bruk-av-terraform","content":"","keywords":"","version":"Next"},{"title":"Deploye applikasjoner med Terraform‚Äã","type":1,"pageTitle":"Bruk av Terraform","url":"/docs/github-actions/bruk-av-terraform#deploye-applikasjoner-med-terraform","content":" P√• SKIP har vi laget en enkel m√•te √• deploye applikasjoner ved hjelp av Skiperator . Dette er en operator som setter opp alt av nettverking, sikkerhetsmekanismer, autoskalering, liveness- og readiness probes for deg s√• lenge man fyller ut en kort config-fil kalt en Application Custom Resource (CR). Man finner dokumentasjonen for hvorden denne Application CR-en ser ut p√• skiperator sin GitHub-side, og man kan se et eksempel p√• dette i Terraform-syntaks under.  resource &quot;kubernetes_manifest&quot; &quot;frontend_application&quot; { manifest = { apiVersion = &quot;skiperator.kartverket.no/v1alpha1&quot; kind = &quot;Application&quot; metadata = { name = local.app_name namespace = local.namespace } spec = { image = &quot;ghcr.io/kartverket/${local.app_name}:${var.image_version}&quot; port = 8080 ingresses = [ var.gateway_host ] replicas = { cpuThresholdPercentage = 80 max = 5 min = 3 } env = [ { name = &quot;BACKEND_URL&quot; value = var.backend-url }, ] liveness = { path = &quot;/&quot; port = 8080 } readiness = { path = &quot;/&quot; port = 8080 } resources = { limits = { cpu = &quot;1000m&quot; memory = &quot;1Gi&quot; } requests = { cpu = &quot;100m&quot; memory = &quot;100M&quot; } } accessPolicy = { outbound = { rules = [ { application = &quot;backend&quot; } ] } } } } }   ","version":"Next","tagName":"h2"},{"title":"Hente hemmeligheter med Vault‚Äã","type":1,"pageTitle":"Bruk av Terraform","url":"/docs/github-actions/bruk-av-terraform#hente-hemmeligheter-med-vault","content":" Hvis man trenger √• bruke hemmeligheter deploy-time, for eksempel for √• deploye sertfikater eller passord til Kubernetes som secrets, s√• m√• man hente ut disse med vault_generic_secret . Eksempelet under gj√∏r dette for √• generere en docker pull secret som lar en pulle fra ghcr.io . Innholdet i hemmeligheten blir generert av en JSON template-fil som ikke er en del av eksempelet.  data &quot;vault_generic_secret&quot; &quot;github_token_ghcr_read&quot; { path = &quot;dsa/github_token_ghcr_read&quot; } data &quot;template_file&quot; &quot;docker_config_script&quot; { template = file(&quot;${path.module}/config.json&quot;) vars = { docker-server = data.vault_generic_secret.github_token_ghcr_read.data[&quot;server&quot;] auth = base64encode(&quot;${data.vault_generic_secret.github_token_ghcr_read.data[&quot;username&quot;]}:${data.vault_generic_secret.github_token_ghcr_read.data[&quot;token&quot;]}&quot;) } } resource &quot;kubernetes_secret&quot; &quot;github-auth&quot; { metadata { name = &quot;github-auth&quot; namespace = local.namespace } data = { &quot;.dockerconfigjson&quot; = data.template_file.docker_config_script.rendered } type = &quot;kubernetes.io/dockerconfigjson&quot; }   ","version":"Next","tagName":"h2"},{"title":"Lagre passord til Vault‚Äã","type":1,"pageTitle":"Bruk av Terraform","url":"/docs/github-actions/bruk-av-terraform#lagre-passord-til-vault","content":" Noen ganger √∏nsker man √• skrive til vault, for eksempel n√•r man genrerer passord. Eksempelet under gj√∏r dette.  resource &quot;random_password&quot; &quot;generated-password&quot; { length = 29 special = true lower = true upper = true number = true } resource &quot;vault_generic_secret&quot; &quot;password-for-vault-storage&quot; { path = &quot;skip/skipet&quot; data_json = &lt;&lt;EOT { &quot;username&quot;: &quot;${skip-bruker}&quot;, &quot;password&quot;: &quot;${random_password.generated-password.result}&quot;, &quot;connection_string&quot;: &quot;jdbc:postgresql://${kubernetes_service.skip-db-service.metadata.0.name}:${local.skip-db-port}/${local.skip-db-database-name}&quot; } EOT }   ","version":"Next","tagName":"h2"},{"title":"Lagring av state‚Äã","type":1,"pageTitle":"Bruk av Terraform","url":"/docs/github-actions/bruk-av-terraform#lagring-av-state","content":" Terraform bruker state for √• kontrollere og sammenlikne den n√•v√¶rende konfigurasjonen mot det som kj√∏rer, staten m√• lagres lokalt eller ekstern. P√• SKIP bruker vi Google Cloud Storage til √• lagre state, og oppsettet for dette kan man se under.  terraform { backend &quot;gcs&quot; { bucket = &quot;terraform_state_foobar_1e8e&quot; prefix = &quot;foobar-frontend&quot; } }   For at backenden over skal kunne n√• denne bucketen m√• service-kontoen den kj√∏rer som v√¶re autentisert mot Google Cloud med riktige tilganger. Dette gj√∏res i byggel√∏ypa f√∏r Terraform blir kj√∏rt, se avsnittet under for hvordan man autentiserer med Google Cloud som en del av Github Actionen.  ","version":"Next","tagName":"h2"},{"title":"Kj√∏re Terraform i GitHub Actions‚Äã","type":1,"pageTitle":"Bruk av Terraform","url":"/docs/github-actions/bruk-av-terraform#kj√∏re-terraform-i-github-actions","content":" Se https://github.com/kartverket/github-workflows for hvordan man bruker Terraform som en del av GitHub Actions. ","version":"Next","tagName":"h2"},{"title":"Autentisering til GitHub i terminalen","type":0,"sectionRef":"#","url":"/docs/github/autentisering-til-github-i-terminalen","content":"","keywords":"","version":"Next"},{"title":"Oppdater Git‚Äã","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#oppdater-git","content":" warning Ikke hopp over dette steget . Du finner oversikt over s√•rbare versjoner av git her: https://github.com/git/git/security/advisories  Velg ditt operativsystem og f√∏lg instruksene for √• installere den nyeste versjonen av Git.  Oppdater Git for LinuxOppdater Git for macOSOppdater Git for Windows  Du kan sjekke hvilken versjon du har med denne kommandoen:  git --version   ","version":"Next","tagName":"h2"},{"title":"Generer SSH n√∏kkel‚Äã","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#generer-ssh-n√∏kkel","content":" Du kan velge mellom ed25519 og RSA-4096.  (det finnes flere alternativer, men disse er vurdert som akseptable)  Bruk ssh-keygen for √• generere en ny n√∏kkel lokalt p√• din maskin. Husk √• bytt ut ‚ÄúDINEPOST‚Äù med Kartverket eposten din (f.eks. &quot;jell.fjell@kartverket.no&quot; ).  ssh-keygen -a 50 -t ed25519 -f ~/.ssh/github -C ‚ÄúDINEPOST‚Äù   Alternativt kan du bruke RSA-4096 ssh-keygen -t rsa -b 4096 -f ~/.ssh/github -C &quot;DINEPOST&quot;   warning NB! Husk √• sette passord n√•r du blir spurt. Ikke la passordfeltet st√• tomt.  ","version":"Next","tagName":"h2"},{"title":"Sett lokale rettigheter p√• SSH n√∏kkelen‚Äã","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#sett-lokale-rettigheter-p√•-ssh-n√∏kkelen","content":" SSH n√∏kkelen er privat for din bruker, og skal kun leses av din bruker.  chmod 600 ~/.ssh/github   ","version":"Next","tagName":"h3"},{"title":"Legg til n√∏kkelen (public key) i GitHub‚Äã","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#legg-til-n√∏kkelen-public-key-i-github","content":" Vis og kopier din public key fra~/.ssh/github.pubfra terminalen.  cat ~/.ssh/github.pub   Marker utskriften og kopier innholdet.  Logg inn p√• GitHub.com med Kartverket kontoen din.Trykk p√• profilbildet ditt, √∏verst i h√∏yre hj√∏rne.Velg ¬´ Settings ¬ª.Naviger deg til ¬´ SSH and GPG keys ¬ª (under kategorien ¬´Access¬ª) i venstre kolonne.Trykk p√• den gr√∏nne ¬´ New SSH key ¬ª knappen.Skriv inn en passelig tittel (f.eks. ‚ÄúMin private SSH n√∏kkel‚Äù).Kopier og lim inn innholdet fra~/.ssh/github.pub(ikke private key), som vist i f√∏rste steg.Trykk p√• ¬´ Add SSH key ¬ª.Du skal n√• se oversikten over dine n√∏kler, med den nye n√∏kkelen i listen.For √• bruke kartverket n√∏kkelen m√• man bekrefte n√∏kkelen med SSO. Dette gj√∏res ved √• trykke configure SSO p√• n√∏kkelen.  ","version":"Next","tagName":"h2"},{"title":"Test n√∏kkelen‚Äã","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#test-n√∏kkelen","content":" Du kan raskt teste n√∏kkelen din mot GitHub ved √• kj√∏re:  ssh -T git@github.com -i ~/.ssh/github   Du skal f√• tilbakemelding om vellykket autentisering:  Hi! You've successfully authenticated, but GitHub does not provide shell access.   ","version":"Next","tagName":"h2"},{"title":"Automatisk bruk av n√∏kkelen din‚Äã","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#automatisk-bruk-av-n√∏kkelen-din","content":" note Det finnes flere m√•ter √• ta i bruk n√∏kkelen din. Dette er et eksempel p√• hvordan, men du st√•r fritt til √• bruke andre l√∏sninger.  Opprett filen~/.ssh/configog fyll den ut med innholdet for GitHub med n√∏kkelen din:  Host github HostName github.com User git IdentityFile ~/.ssh/github   ","version":"Next","tagName":"h2"},{"title":"Ta i bruk n√∏kkelen n√•r du kloner et repo‚Äã","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#ta-i-bruk-n√∏kkelen-n√•r-du-kloner-et-repo","content":" Du kan ta i bruk n√∏kkelen din ved √• refere til github n√•r du skal klone et repo. Husk √• bytt ut ‚ÄúDITTREPO‚Äù med navnet p√• repoet du pr√∏ver √• klone.  git clone github:kartverket/DITTREPO.git   N√•r du kloner repoet p√• denne m√•ten vil Git automatisk ta i bruk remote med din konfigurasjon (tar automatisk i bruk n√∏kkelen din ved git pull / push ).  ","version":"Next","tagName":"h3"},{"title":"Legg til navn og epost for riktig eier av commits‚Äã","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/github/autentisering-til-github-i-terminalen#legg-til-navn-og-epost-for-riktig-eier-av-commits","content":" For at commits du gj√∏r p√• din maskin skal stemme overens med GitHub brukeren din m√• du sette brukernavn og epost i Git. Husk √• bytt ut ‚ÄúDITT NAVN‚Äù med github brukernavnet ditt (f.eks. ‚Äújellfjell‚Äú) og ‚ÄúDINEPOST‚Äù med Kartverket eposten din (f.eks. &quot;jell.fjell@kartverket.no&quot; ).  git config --global user.name &quot;DITT NAVN&quot; git config --global user.email &quot;DINEPOST&quot;   TLDR For deg som ikke leste i gjennom og vil rett p√• sak uten forklaring. Oppdater Git: https://git-scm.com/downloads - IKKE HOPP OVER DETTE STEGET ssh-keygen -a 50 -t ed25519 -f ~/.ssh/github -C ‚ÄúDINEPOST‚Äù chmod 600 ~/.ssh/github cat ~/.ssh/github.pub Kopier og lim inn public key p√• GitHub https://github.com/settings/ssh/new ssh -T git@github.com -i ~/.ssh/github git config --global user.name &quot;DITT NAVN&quot; git config --global user.email &quot;DINEPOST&quot; git clone URL --config core.sshCommand=&quot;ssh -i ~/.ssh/github&quot; N√• er du klar for √• begi deg ut p√• eventyr. ","version":"Next","tagName":"h2"},{"title":"‚öôÔ∏è Kubernetes","type":0,"sectionRef":"#","url":"/docs/kubernetes","content":"‚öôÔ∏è Kubernetes","keywords":"","version":"Next"},{"title":"Autentisering mot GCP fra applikasjon","type":0,"sectionRef":"#","url":"/docs/kubernetes/autentisering-mot-gcp-fra-applikasjon","content":"","keywords":"","version":"Next"},{"title":"1. Opprett Servicekonto‚Äã","type":1,"pageTitle":"Autentisering mot GCP fra applikasjon","url":"/docs/kubernetes/autentisering-mot-gcp-fra-applikasjon#1-opprett-servicekonto","content":" Dersom man √∏nsker √• f√• tilgang til GCP-tjenester fra Kubernetes gj√∏res dette med √• f√∏rst opprette en servicekonto i GCP og √• gi den IAM-rettigheter til det man √∏nsker at den skal gj√∏re.  Servicekontoer b√∏r enten opprettes med terraform eller via gcp-service-accounts repoet til SKIP.  ","version":"Next","tagName":"h2"},{"title":"2. Gi WIF IAM Policy til Servicekonto‚Äã","type":1,"pageTitle":"Autentisering mot GCP fra applikasjon","url":"/docs/kubernetes/autentisering-mot-gcp-fra-applikasjon#2-gi-wif-iam-policy-til-servicekonto","content":" To authenticate this service account in GCP from Kubernetes, the service account in Kubernetes needs to be given permission to impersonate the GCP service account. This is done by giving the Kubernetes Service Account the role iam.workloadIdentityUser through a so called Workload Identity Pool.  Given the following variables:  GCP_SA_NAME - Name of the GCP service account GCP_SA_PROJECT_ID - GCP Project ID where the service account resides KUBERNETES_PROJECT_ID - GCP Project ID for the Kubernetes cluster (for example kubernetes-dev-94b9 for dev-cluster) KUBERNETES_NAMESPACE - The Kubernetes namespace where the Pod will run KUBERNETES_SA_NAME - The Kubernetes service account name that your Pod is using (typically same name as Application, and with the -skipjob suffix for SKIPJobs)   Run the following command using the gcloud CLI:  gcloud iam service-accounts add-iam-policy-binding \\ GCP_SA_NAME@GCP_SA_PROJECT_ID.iam.gserviceaccount.com \\ --role=roles/iam.workloadIdentityUser \\ --member=&quot;serviceAccount:KUBERNETES_PROJECT_ID.svc.id.goog[KUBERNETES_NAMESPACE/KUBERNETES_SA_NAME]&quot;   ","version":"Next","tagName":"h2"},{"title":"3. Legg inn config i Skiperatormanifest‚Äã","type":1,"pageTitle":"Autentisering mot GCP fra applikasjon","url":"/docs/kubernetes/autentisering-mot-gcp-fra-applikasjon#3-legg-inn-config-i-skiperatormanifest","content":" Til slutt legger man til gcp config i sin skiperator Application for √• lage kubernetes-config slik at podden kan autentisere mot GCP.  //yaml format spec: gcp: auth: serviceAccount: GCP_SA_NAME@GCP_SA_PROJECT_ID.iam.gserviceaccount.com   N√• kan man f√∏lge ‚ÄúAuthenticate from your code‚Äù under https://cloud.google.com/anthos/fleet-management/docs/use-workload-identity#-python for √• autentisere mot GCP fra koden sin.  N√•r dette er gjort kan applikasjonen snakke med GCP under runtime.  ","version":"Next","tagName":"h2"},{"title":"Alternativ til 1 / 2‚Äã","type":1,"pageTitle":"Autentisering mot GCP fra applikasjon","url":"/docs/kubernetes/autentisering-mot-gcp-fra-applikasjon#alternativ-til-1--2","content":" Dersom man ikke √∏nsker √• legge til roller manuelt har SKIP lagt til en ny m√•te √• legge til Workload Identity User p√• en service account, ved hjelp av Crossplane.  apiVersion: 'skip.kartverket.no/v1alpha1' kind: 'WorkloadIdentityInstance' metadata: name: 'service-account-wi' spec: parameters: gcpKubernetesProject: 'some-kubernetes-project' #eks: 'kubernetes-dev-94b9' gcpProject: 'gcp-project-where-service-account-is' #eks: 'dsa-dev-e32c' gcpServiceAccount: 'name-of-service-account-in-gcp' #eks: 'dsa-runtime@dsa-dev-e32c.iam.gserviceaccount.com' serviceAccount: 'name-of-service-account-in-kubernetes' #eks 'dsa-backend', typically same name as your Application   Se Provisjonere infrastruktur med Crossplane om du ikke har brukt Crossplane tidligere. ","version":"Next","tagName":"h2"},{"title":"End-user IP-Addresses in Containers","type":0,"sectionRef":"#","url":"/docs/kubernetes/end-user ip-addresses-in-containers","content":"End-user IP-Addresses in Containers To forward end-user IP-Addresses to a kubernetes container running spring boot, you need to add the following line to your configuration: server.forward-headers-strategy=NONE After testing, we found that this setting should be ‚ÄúNONE‚Äù. Running spring Behind a Front-end Proxy Server Spring server.forward-headers-strategy NATIVE vs FRAMEWORK","keywords":"","version":"Next"},{"title":"Bruk av porter i pods","type":0,"sectionRef":"#","url":"/docs/kubernetes/bruk-av-porter-i-pods","content":"Bruk av porter i pods Porter under 1024 er priviligierte og krever at prosessen som kj√∏rer kj√∏rer som root. Dette er ikke tillatt p√• SKIP, og prosessen m√• derfor binde til en h√∏yere port. Dette betyr at man ofte m√• gj√∏re tilpasninger p√• Docker-imaget man bygger slik at f.eks. nginx binder til en annen port. N√•r prosessen i containeren binder seg til en upriviligert port, kan man spesifisere denne porten i Skiperator-manifestet slik som under. I bakgrunnen vil Skiperator ta seg av √• lage en Kubernetes-service for denne porten slik at trafikken kan rutes til riktig sted. apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: teamname-frontend namespace: yournamespace spec: image: &quot;kartverket/eksempel-image&quot; port: 8080 additionalPorts: - name: metrics-port port: 8181 protocol: TCP - name: another-port port: 8090 protocol: TCP En annen ting √• merke seg her er muligheten for √• spesifisere ekstra porter som kan benyttes til andre form√•l som f.eks. helsesjekker eller prometheus-metrikker. Disse portene vil automatisk f√• opprettet en service, men det er fortsatt kun hovedporten som kobles opp mot en ingress-gateway. P√• den m√•ten kan man skille ut endepunktet som ikke trengs eksternt.","keywords":"","version":"Next"},{"title":"Certificates outside ACME","type":0,"sectionRef":"#","url":"/docs/kubernetes/certificates-outside-acme","content":"","keywords":"","version":"Next"},{"title":"Create certificate secret resource in istio-gateways‚Äã","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kubernetes/certificates-outside-acme#create-certificate-secret-resource-in-istio-gateways","content":" To be able to use a custom certificate we need a secret to mount to the gateway resource. This is a kubernetes.io/tls type secret and can be created via external secrets like this:  apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: star-matrikkel namespace: istio-gateways spec: dataFrom: - extract: conversionStrategy: Default decodingStrategy: Auto key: star-matrikkel-no-key refreshInterval: 1h secretStoreRef: kind: SecretStore name: gsm target: creationPolicy: Owner deletionPolicy: Retain name: star-matrikkel # Secret in Kubernetes template: engineVersion: v2 mergePolicy: Replace type: kubernetes.io/tls   This fetches the secret from Google Secret Manager. This secret should look like this:  { &quot;tls.crt&quot;:&quot;[base64 encoded cert chain]&quot;, &quot;tls.key&quot;:&quot;[base64 encoded tls.key]&quot; }   ","version":"Next","tagName":"h2"},{"title":"Edit the gateway resource‚Äã","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kubernetes/certificates-outside-acme#edit-the-gateway-resource","content":" The gateway resource should then be updated with the new secret:  apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: gateway-ingress namespace: matrikkel-keycloak spec: selector: app: istio-ingress-external servers: - hosts: - auth.matrikkel.no port: name: http number: 80 protocol: HTTP - hosts: - auth.matrikkel.no port: name: https number: 443 protocol: HTTPS tls: credentialName: star-matrikkel # Secret created by externalsecret mode: SIMPLE   ","version":"Next","tagName":"h2"},{"title":"If Skiperator is the gateway creator‚Äã","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kubernetes/certificates-outside-acme#if-skiperator-is-the-gateway-creator","content":" When the gateway is created via Skiperator it will have a credentialName corresponding to the secret created by the certificate from Skiperator. Skiperator will reset configurations to its resources unless the resource labeled ‚Äúskiperator.kartverket.no/ignore: &quot;true&quot;‚Äú. This will make skiperator ignore this specific resource during reconciliation loops.  apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: labels: skiperator.kartverket.no/ignore: &quot;true&quot;   This is meant to be a temporary solution, and ACME is the prefered way to get certificates in SKIP.  ","version":"Next","tagName":"h3"},{"title":"Change to ACME certificate‚Äã","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kubernetes/certificates-outside-acme#change-to-acme-certificate","content":" ","version":"Next","tagName":"h2"},{"title":"Non-Skiperator apps‚Äã","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kubernetes/certificates-outside-acme#non-skiperator-apps","content":" Using ACME certificate on a non skiperator app requires a certificate resource, and using the resulting secret in the gateway. This resource must be created in the istio-gateways namespace and therefore in the skip-apps :  apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: certificate-name namespace: istio-gateways spec: dnsNames: - appname.kartverket.no issuerRef: kind: ClusterIssuer name: cluster-issuer secretName: desired-secret-name   After this is created and the secret is created, the gateway resource can be edited, and spec.tls.credentialName set to the secret.  ","version":"Next","tagName":"h3"},{"title":"Skiperator apps‚Äã","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kubernetes/certificates-outside-acme#skiperator-apps","content":" Remove the ‚Äúskiperator.kartverket.no/ignore: &quot;true&quot;‚Äú label, and skiperator will handle the rest. ","version":"Next","tagName":"h3"},{"title":"Jobbe med Kubernetes cluster","type":0,"sectionRef":"#","url":"/docs/kubernetes/jobbe-med-cluster","content":"","keywords":"","version":"Next"},{"title":"K9s‚Äã","type":1,"pageTitle":"Jobbe med Kubernetes cluster","url":"/docs/kubernetes/jobbe-med-cluster#k9s","content":" K9s er terminalbasert men gir deg mer informasjon enn du ellers ville f√•tt ved enkle kubectl kommandoer. Se her en oversikt over alle Podder som kj√∏rer i et namespace.    Her f√•r vi for eksempel en stor toast p√• at alle poddene kj√∏rer med mer minne enn de requester. I tillegg har vi en fin oversikt over generell ressursbruk og forhold mellom request/limit og faktisk bruk.  Man kan enkelt sortere p√• alle felter, s√∏ke p√• vilk√•rlige ressurstyper, redigere ressurser, filtere basert p√• s√∏k og masse mer.  Nedlasting: K9s - Manage Your Kubernetes Clusters In Style ","version":"Next","tagName":"h2"},{"title":"Retningslinjer for Kubernetes","type":0,"sectionRef":"#","url":"/docs/kubernetes/retningslinjer-for-kubernetes","content":"","keywords":"","version":"Next"},{"title":"Minstekrav for sikkerhet‚Äã","type":1,"pageTitle":"Retningslinjer for Kubernetes","url":"/docs/kubernetes/retningslinjer-for-kubernetes#minstekrav-for-sikkerhet","content":" I Kubernetes bruker vi Pod Security Standards for √• sikre at alle pods som kj√∏rer i clusteret v√•rt er sikre. Dette er en standard som er satt av CNCF, og som vi f√∏lger for √• sikre at vi ikke har noen √•penbare sikkerhetshull i Kubernetes-clusteret v√•rt. Alle workloads skal f√∏lge PSS niv√• &quot;restricted&quot;, som er et niv√• som f√∏lger dagens best practices for sikring av containere. Applikasjoner som kj√∏rer som Skiperator Applications f√∏lger allerede denne standarden. Les mer om Pod Security Standards her.  ","version":"Next","tagName":"h2"},{"title":"Namespaces som avgrensning mellom teams‚Äã","type":1,"pageTitle":"Retningslinjer for Kubernetes","url":"/docs/kubernetes/retningslinjer-for-kubernetes#namespaces-som-avgrensning-mellom-teams","content":" Hvert team kan lage s√• mange Namespaces som de har behov for. Dette er for √• kunne skille p√• ressurser og tilganger mellom forskjellige applikasjoner og team. Dette er ogs√• for √• kunne gi teamene mulighet til √• eksperimentere og teste ting uten at det p√•virker andre team. Les mer om dette p√• Argo CD.  Kommunikasjon mellom tjenester internt i ett namespace er helt lukket (‚Äúdefault deny‚Äù-policy), og det er opp til teamet selv √• s√∏rge for √• √•pne for kommunikasjon mellom tjenester. Les mer om dette p√• Skiperator.  Informasjon om kommunikasjon mot tjenester som ligger i andre namespaces finnes her: Anthos Service Mesh Brukerdokumentasjon  Tjenester og fellesfunksjoner som brukes av flere teams skal settes i egne namespaces. Tilgang til disse namespacene gis ved at det opprettes en ny gruppe i AD p√• samme m√•te som et produktteam.  ","version":"Next","tagName":"h2"},{"title":"Ressursbruk i Kubernetes‚Äã","type":1,"pageTitle":"Retningslinjer for Kubernetes","url":"/docs/kubernetes/retningslinjer-for-kubernetes#ressursbruk-i-kubernetes","content":" Ressursbruk i Kubernetes dreier seg om hvor mye CPU og RAM hver pod skal bruke.  Requests er hvor mye CPU og minne hvercontainersp√∏r om n√•r den f√∏rst settes p√• en node. Hvis man for eksempel ber om 500 mCPUer, men noden bare har 250 mCPU ledig, kan containeres ikke kj√∏res p√• den noden.  Merk at man kan spesifisere CPU helt ned i millicpuer (mCPU).  Minne-requests kan settes i mange forskjellige enheter, se dokumentasjonen for detaljer. Vi anbefaler dog at man holder seg til M - megabytes.  Limits er hvor mye ressurser en container maksimalt f√•r lov til √• bruke. Dette er med andre ord noe som settes for √• forhindre at en container med en bug tar over alle ressursene, og gj√∏r det umulig for andre containere √• skalere.  Vi anbefaler at du ser p√• Limiten som en mulighet til √• finne bugs og memory leaks. Sett den s√• lavt du er komfortabel med, og f√∏lg med p√• det faktiske forbruket. Hvis noe kr√¶sjer er det da god sjanse for at det ble innf√∏rt en bug.  Dersom det faktiske forbruket n√¶rmer seg limiten p√• grunn av naturlige grunner - flere requests eller tyngre load+ er det p√• tide √• √∏ke limiten. Ikke vent til appliasjonen kr√¶sjer - det skaper en d√•rlig brukeropplevelse.  tip En god tommelfingerregel for requests og limits er f√∏lgende: For minne b√∏r man profilere applikasjonens gjennomsnittlige minnebruk og doble denne som limit. For CPU trenger man ikke limit, men heller definere en fornuftig request.  Logikken bak dette er at dersom en applikasjon bruker altfor mye minne kan det f√∏re til at andre applikasjoner g√•r ned. Dersom en applikasjon bruker mye CPU f√∏rer det derimot bare i verste fall til at ting g√•r tregere.  Dette er reglene for ressursbruk i Kubernetes p√• SKIP  Produktteamet velger selv hva som er naturlig ressursbruk for sine containere, og skal ha et bevisst forhold til hvilke grenser som er satt. Produktteamet skal f√∏lge med p√• ressursbruken over tid, og oppdatere grensene slik at de til enhver tid reflekterer hva applikasjonen faktisk trenger. Resource requests og limits skal settes p√• alle containere slik at det blir tydelig hva som er forventet ressursbruk. Resource limits skal skal alltid settes h√∏yere enn requests, men aldri unaturlig h√∏yt. Husk at dette fungerer b√•de som dokumentasjon og som en sikring mot bugs og feilkonfigurasjon. Resource limits skal aldri fjernes permanent, men kan fjernes for debugging. Da skal SKIP-teamet gj√∏res oppmerksom p√• dette. Godt blogginnlegg om korrelasjonen mellom JVMs og Kubernetes‚Äô minnebruk  Kubernetes‚Äô dokumentasjon om ressursbruk  Google Clouds dokumentasjon om ‚Äúcost effective apps‚Äù (Merk at Google anbefaler √• sette limit til det samme som requests - vi setter driftsstabilitet over kostnad, og er derfor uenig i dette.) ","version":"Next","tagName":"h2"},{"title":"Logge inn p√• cluster","type":0,"sectionRef":"#","url":"/docs/kubernetes/logge-inn-p√•-cluster","content":"","keywords":"","version":"Next"},{"title":"CLI (kubectl)‚Äã","type":1,"pageTitle":"Logge inn p√• cluster","url":"/docs/kubernetes/logge-inn-p√•-cluster#cli-kubectl","content":" F√∏rst installer gcloud og kubectl .  For √• logge inn med kubectl gj√∏r f√∏lgende:  # Login med gcloud hvis du ikke har gjort det allerede $ gcloud auth login # S√∏rg for at du st√•r i riktig gcp-prosjekt # Hvis du ikke vet hele navnet p√• prosjektet kan du finne dette vet √• liste prosjekter $ gcloud projects list # GCP-prosjektet vil v√¶re et kubernetes-prosjekt med format kubernetes-&lt;env&gt;-xxxx # Jobber du f.eks. i dsa-dev-e32c velger du kubernetes-dev-94b9 $ gcloud config set project kubernetes-dev-94b9 # Finn riktig clusternavn $ gcloud container hub memberships list # Per 14.02.2023 er clusternavn alltid p√• formatet atkv1-&lt;env&gt; (on-premise) # Logg inn, generer kubeconfig og sett som aktiv context $ gcloud container hub memberships get-credentials atkv1-dev # Forrige kommando oppretter en ny context, som kan autentisere deg mot clusteret # Contexten som blir opprettet her ser noe a la slik ut: # connectgateway_kubernetes-&lt;env&gt;-xxxx_global_atkv1-&lt;env&gt; # Eksempel: connectgateway_kubernetes-dev-94b9_atkv1-dev # Har du lastet ned kubectx kan du bytte til contexten slik: $ kubectx connectgateway_kubernetes-dev-94b9_atkv1-dev # Om ikke kan du gj√∏re det med f√∏lgende kommando i kubectl: $ kubectl config use-context connectgateway_kubernetes-dev-94b9_atkv1-dev # Du kan ogs√• rename disse contextene til noe litt mer spiselig med f√∏lgende kommando # Her er navn 2 vilk√•rlig $ kubectl config rename-context connectgateway_kubernetes-dev-94b9_atkv1-dev atkv1-dev   Se ogs√• https://cloud.google.com/anthos/multicluster-management/gateway/using .  For √• ha adgang til √• logge p√• clusteret m√• du v√¶re lagt inn i en CLOUD_SK_TEAM AD-gruppe som er synket med GCP. ","version":"Next","tagName":"h2"},{"title":"üíæ Lagring","type":0,"sectionRef":"#","url":"/docs/lagring","content":"üíæ Lagring","keywords":"","version":"Next"},{"title":"Objektlagring med Scality S3","type":0,"sectionRef":"#","url":"/docs/lagring/objektlagring-scality-s3","content":"Objektlagring med Scality S3 I Kartverket har vi en lokalt S3-kompatibel lagringsl√∏sning ved navn Scality. Denne er mulig √• f√• tilgang til, og er godt egnet i tilfellet at dere trengre √• lagre filer fra en container. √Ö f√• tilgang til denne krever f√∏lgende: SKIP oppretter bruker og lagringsb√∏tter for dere i scality-l√∏sningen Admin interface Dere f√•r access key og secret","keywords":"","version":"Next"},{"title":"URLer og sertifikat for tjenester p√• SKIP","type":0,"sectionRef":"#","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-p√•-skip","content":"","keywords":"","version":"Next"},{"title":"Interne tjenester‚Äã","type":1,"pageTitle":"URLer og sertifikat for tjenester p√• SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-p√•-skip#interne-tjenester","content":" ","version":"Next","tagName":"h2"},{"title":"kartverket-intern.cloud‚Äã","type":1,"pageTitle":"URLer og sertifikat for tjenester p√• SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-p√•-skip#kartverket-interncloud","content":" Dersom du har en tjeneste som kun skal v√¶re tilgjengelig for folk p√• kartverkets nettverk og VPN og ikke p√• internett for allmennheten har man flere forskjellige alternativer. Avhengig av bruksomr√•de og hva slags URL man √∏nsker seg fungerer dette litt forskjellig, og beskrives i paragrafene under.  For tjenester som skal n√•s p√• et domene under kartverket-intern.cloud h√•ndteres alt automatisk, inkludert utstedelse og fornying av sertfikater. Det ligger et wildcard record i DNS som h√•ndterer innkommende trafikk, og bruker cluster-leddet i URL-en p√• Load Balanceren til √• rute denne inn til riktig cluster. Deretter rutes denne til applikasjonen din basert p√• URL-konfigurasjonen din i Skiperator.  info Eksempel: minapp.atkv3-prod.kartverket-intern.cloud  ","version":"Next","tagName":"h3"},{"title":"Vanity URL-er‚Äã","type":1,"pageTitle":"URLer og sertifikat for tjenester p√• SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-p√•-skip#vanity-url-er","content":" note Akkurat n√• st√∏ttes kun kartverket-intern.cloud URL-er pga. en begrensning i utstedelse av sertfikater ( SKIP-1459 ) og en begrensning i lastbalanserer p√• atkv3-dev cluster ( SKIP-1458 ). Dette skal utbedres.  Dersom du √∏nsker et annet hostname enn app.&lt;cluster&gt;.kartverket-intern.cloud er dette mulig, men krever noe mer setup. Den nye URL-en m√• registreres i DNS og skiperator-applikasjonen din m√• settes opp til √• lytte p√• denne. Utstedelse og fornying av sertfikater vil fremdeles h√•ndteres automatisk av Skiperator.  For √• sette opp DNS m√• du gj√∏re f√∏lgende: F√∏rst bestem hvilke URL du vil ha, deretter sett opp et CNAME for denne URL-en til &lt;cluster&gt;.kartverket-intern.cloud . Dersom du √∏nsker et CNAME som ligger under kartverket-intern.cloud (for eksempel minapp.kartverket-intern.cloud) kan dette gj√∏res av SKIP, for alle andre domener ta kontakt med eier av domenet via bestilling i pureservice. N√•r dette er gjort vil alle sp√∏rringer som g√•r mot URL-en du har bestemt ende opp host lastbalansereren foran clusteret, og sendes videre inn til Kubernetes.  Neste steg er at Kubernetes sender sp√∏rringen videre til din applikasjon. Da m√• du registere URL-en i Skiperator som vanlig under ingresses .  ","version":"Next","tagName":"h3"},{"title":"Cluster-intern‚Äã","type":1,"pageTitle":"URLer og sertifikat for tjenester p√• SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-p√•-skip#cluster-intern","content":" Alle applikasjoner som kj√∏rer p√• SKIP har en kubernetes Service tilknyttet seg. Med denne servicen kan man sende sp√∏rringer direkte til applikasjonen uten √• sende trafikken ut av clusteret.  Merk at man her bruker http og ikke https. Trafikken vil allikevel krypteres av service meshet s√• trafikken vil g√• over https mellom tjenestene, men fra ditt perspektiv skal du bruke http og trenger ikke tenke p√• sertfikater.  For √• sende en sp√∏rring p√• denne m√•ten bruker du en URL i f√∏lgende format:  http://&lt;appnavn&gt;.&lt;namespacenavn&gt;:port   Merk at √• snakke med en annen tjeneste p√• denne m√•ten krever at du har √•pnet opp for at trafikk kan flyte mellom disse tjenestene. I utgangspunktet blir all trafikk blokkert av sikkerhetshensyn. √Ö √•pne opp gj√∏res ved √• spesifisere spec.accessPolicy.outbound.rules i applikasjonen som skal sende sp√∏rringen og spec.accessPolicy.inbound.rules i applikasjonen som skal motta sp√∏rringene.  Dersom du har samme tjeneste i sky og √∏nsker √• presisere at du skal g√• mot samme cluster m√• man legge p√• dette i URL. Hvis ikke blir den ‚Äúround robined‚Äù mellom remote og lokal. Eksempel:  http://&lt;appnavn&gt;.&lt;namespacenavn&gt;.svc.cluster.local:port   ","version":"Next","tagName":"h3"},{"title":"Mesh-intern‚Äã","type":1,"pageTitle":"URLer og sertifikat for tjenester p√• SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-p√•-skip#mesh-intern","content":" Dersom du har behov for √• sende en sp√∏rring til en annen applikasjon som ikke ligger p√• samme cluster, men er en del av samme service mesh (for eksempel fra atkv3-prod til atgcp1-prod) s√• kan dette rutes p√• nesten samme m√•te som cluster-intern trafikk.  Merk at man her bruker http og ikke https. Trafikken vil allikevel krypteres av service meshet s√• trafikken vil g√• over https mellom tjenestene, men fra ditt perspektiv skal du bruke http og trenger ikke tenke p√• sertfikater.  For √• sende trafikk til et annet cluster over service meshet sender du en sp√∏rring i f√∏lgende format:  http://&lt;appnavn&gt;.&lt;namespacenavn&gt;.svc.cluster.&lt;cluster&gt;:port   TODO: Hvordan blir networkpolicies for Skiperator apper p√• mesh?  ","version":"Next","tagName":"h3"},{"title":"Tjenester eksponert p√• internett‚Äã","type":1,"pageTitle":"URLer og sertifikat for tjenester p√• SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-p√•-skip#tjenester-eksponert-p√•-internett","content":" Det er to alternativer for √• eksponere ting p√• internett. Bruk kartverket.cloud eller en penere ‚Äúvanity URL‚Äù.  Merk at skiperator-tjenester som eksponeres p√• andre domenenavn enn subdomener av kartverket-intern.cloud vil automatisk bli √•pnet for trafikk fra internett, men vil ikke v√¶re tilgjengelig f√∏r DNS konfigureres.  ","version":"Next","tagName":"h2"},{"title":"kartverket.cloud‚Äã","type":1,"pageTitle":"URLer og sertifikat for tjenester p√• SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-p√•-skip#kartverketcloud","content":" For tjenester som skal n√•s p√• et domene under kartverket.cloud h√•ndteres alt automatisk, inkludert utstedelse og fornying av sertfikater. Det ligger et wildcard record i DNS som h√•ndterer innkommende trafikk, og bruker cluster-leddet i URL-en p√• Load Balanceren til √• rute denne inn til riktig cluster. Deretter rutes denne til applikasjonen din basert p√• URL-konfigurasjonen din i Skiperator.  info Eksempel: minapp.atkv3-prod.kartverket.cloud  ","version":"Next","tagName":"h3"},{"title":"Vanity URL-er‚Äã","type":1,"pageTitle":"URLer og sertifikat for tjenester p√• SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-p√•-skip#vanity-url-er-1","content":" Dersom du √∏nsker et annet hostname enn app.&lt;cluster&gt;.kartverket.cloud er dette mulig, men krever noe mer setup. Den nye URL-en m√• registreres i DNS og skiperator-applikasjonen din m√• settes opp til √• lytte p√• denne. Utstedelse og fornying av sertfikater vil fremdeles h√•ndteres automatisk av Skiperator.  For √• sette opp DNS m√• du gj√∏re f√∏lgende: F√∏rst bestem hvilke URL du vil ha, deretter sett opp et CNAME for denne URL-en til &lt;cluster&gt;.kartverket.cloud . Dersom du √∏nsker et CNAME som ligger under kartverket.cloud (for eksempel minapp.kartverket.cloud) kan dette gj√∏res av SKIP, for alle andre domener ta kontakt med eier av domenet via bestilling i pureservice. N√•r dette er gjort vil alle sp√∏rringer som g√•r mot URL-en du har bestemt ende opp host lastbalansereren foran clusteret, og sendes videre inn til Kubernetes.  Neste steg er at Kubernetes sender sp√∏rringen videre til din applikasjon. Da m√• du registere URL-en i Skiperator som vanlig under ingresses .  Dersom du √∏nsker √• ha en URL p√• toppniv√• (annentjeneste.no) er ikke CNAME st√∏ttet i DNS. Her m√• man bruke an A record, og her kan man i s√• fall f√• IP-adresser med √• gj√∏re et DNS-oppslag p√• &lt;cluster&gt;.kartverket.cloud .  ","version":"Next","tagName":"h3"},{"title":"HTTPS by default‚Äã","type":1,"pageTitle":"URLer og sertifikat for tjenester p√• SKIP","url":"/docs/kubernetes/urler-og-sertifikat-for-tjenester-p√•-skip#https-by-default","content":" N√•r man eksponerer en applikasjon f√•r man ogs√• HTTPS automatisk satt opp og eksponert. I dette tilfellet kan man fort sp√∏rre seg om man burde redirecte HTTP til HTTPS for at alle brukerene skal nyte godt av dette, og svaret p√• det er i nesten alle tilfeller ja.  For √• sette opp en slik redirect er det enkleste √• f√• applikasjonen som serverer ressurser til klienten (nettleseren) √• sende en https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Strict-Transport-Security header (HSTS). N√•r en nettleser laster en nettside og oppdager en HSTS header vil den legge denne nettsiden i sin interne cache med et flagg som sier at denne nettsiden alltid skal lastes med HTTPS. Lengden p√• denne cachen kan settes i flagget, og i de fleste tilfeller vil denne settes ganske h√∏yt.  Den eneste tiden hvor dette kan bli problematisk er om det plutselig skjer en endring som gj√∏r at nettsiden ikke lenger serveres p√• HTTPS. For √• forhindre downgrade attacks vil nettleseren serveres en feilmelding om at nettsiden kun kan √•pnes p√• HTTPS og det vil ikke v√¶re mulig √• g√• forbi denne for √• n√• HTTP-siden. Men i de aller fleste tilfeller er ikke dette noe √• bekymre seg over. ","version":"Next","tagName":"h2"},{"title":"Helsesjekker i Kubernetes","type":0,"sectionRef":"#","url":"/docs/kubernetes/helsesjekker-i-kubernetes","content":"","keywords":"","version":"Next"},{"title":"Bakgrunn‚Äã","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kubernetes/helsesjekker-i-kubernetes#bakgrunn","content":" Helsesjekker i kubernetes er en veldig viktig del av mikrotjeneste-arkitekturen, og det er derfor lurt √• sette seg litt inn i hensikten og funksjonen til de ulike helse-endepunktene man kan konfigurere. Det finnes mange m√•ter √• konfigurere disse p√•, alt i fra helt egenlagde endepunkter til ferdige rammeverk som eksponerer dette automatisk.  I hovedsak finnes det tre typer prober som kubernetes opererer med:  Liveness probe - Sjekker om containeren kj√∏rer og fungerer, hvis ikke s√• restarter kubernetes containeren    Readiness probe - Brukes for √• bestemme om en pod en klar for √• ta i mot trafikk. En pod er klar n√•r alle containere i poden er klare.    Startup probe - Hvis denne er konfigurert s√• avventer kubernetes med liveness og readiness til dette endepunktet svarer. Dette kan brukes for √• gi trege containere noe mer tid til √• starte opp.  Det finnes flere m√•ter √• sette opp helsesjekker p√•, som f.eks. kommandoer, HTTP-requests, TCP-requests og gRPC-requests. Den aller vanligste m√•ten er √• konfigurere et endepunkt (f.eks /health, /liveness) i applikasjonen som svarer p√• HTTP-requests, og spesifisere dette som en del av pod-spesifikasjonen.  Litt mer om helsesjekker generelt: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/  Det er viktig √• merke seg at man ikkem√•benytte seg av alle disse helsesjekkene, men man b√∏r ta et bevisst valg om det er hensiktsmessig eller ikke. Sjekk den lenken her for √• en oversikt over hva man b√∏r gj√∏re: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#when-should-you-use-a-liveness-probe  ","version":"Next","tagName":"h2"},{"title":"Skiperator‚Äã","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kubernetes/helsesjekker-i-kubernetes#skiperator","content":" De aller fleste som har applikasjoner p√• SKIP benytter Skiperator for √• forenkle oppsettet rundt applikasjonen. I Skiperator-manifestet kan man konfigurere helsesjekker p√• samme m√•te man ville gjort for en vanilla-pod i kubernetes. Detaljer rundt dette st√•r i dokumentasjonen for Skiperator . Se under ApplicationSpec og Liveness / Readiness / Startup .  ","version":"Next","tagName":"h2"},{"title":"Sikkerhetshensyn‚Äã","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kubernetes/helsesjekker-i-kubernetes#sikkerhetshensyn","content":" Det er noen fallgruver √• v√¶re klar over, s√• denne siden skal pr√∏ve √• gi en oversikt over hvordan man typisk b√∏r konfigurere dette p√• en sikker og god m√•te.  Hvis man ikke konfigurerer applikasjonen sin riktig kan man i verste fall ende opp med √• eksponere de samme endepunktene som kubernetes bruker internt ut p√• internett. Et enkelt /health endepunkt som svarer med HTTP 200 OK, gj√∏r ikke s√• stor skade. Et feilkonfigurert management-endepunkt derimot kan eksponere interne milj√∏variabler, debug-info og minnedump.  Ta en titt p√• f√∏lgende flytskjema f√∏r du g√•r videre, og g√• til det punktet som gjelder deg  ","version":"Next","tagName":"h2"},{"title":"Unders√∏ke hva som eksponeres som standard‚Äã","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kubernetes/helsesjekker-i-kubernetes#unders√∏ke-hva-som-eksponeres-som-standard","content":" En veldig vanlig m√•te √• l√∏se helsejsekker p√• n√•r man bruker Spring Boot er √• benytte seg av sub-prosjektet Spring Boot Actuator .  For √• ta det i bruk trenger man bare √• legge til f√∏lgende for Maven-prosjekt (pom.xml)  &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;   Eller dette hvis man bruker Gradle (build.gradle)  dependencies { implementation 'org.springframework.boot:spring-boot-starter-actuator' }   Rammeverket setter automatisk opp endepunktet /actuator/health som en trygg default (gjelder versjon 2 og h√∏yere av Spring Boot). N√•r man starter opp en Spring-applikasjon i kubernetes vil Spring Boot Actuator ogs√• automatisk tilgjengeliggj√∏re /actuator/health/liveness og /actuator/health/readiness som man benytte for helsesjekker. For √• teste disse manuelt kan du legge til management.endpoint.health.probes.enabled=true i application.properties .  Disse endepunktene kan du s√• bruke i Skiperator-manifestet:  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: some-backend namespace: yournamespace spec: # √òvrig konfigurasjon liveness: path: /actuator/health/liveness port: 8080 failureThreshold: 3 timeout: 1 initialDelay: 3 readiness: path: /actuator/health/readiness port: 8080 failureThreshold: 3 timeout: 1 initialDelay: 5   Det er viktig √• merke seg f√∏lgende notat:  warning Konfigurasjon som management.endpoints.web.exposure.include=* frar√•des ettersom det eksponerer alle endepunkt som er skrudd p√•. I s√• fall m√• man passe p√• √• sette management.endpoints.enabled-by-default=false og manuelt skru p√• de man √∏nsker √• bruke.  √ònsker man √• eksponere ytterligere endepunkt , som f.eks. √• eksponere /info for √• presentere informasjon om bygget eller lignende er det tryggere √• eksplisitt man gj√∏re det p√• denne m√•ten i application.properties :  management.endpoint.info.enabled=true management.endpoint.health.enabled=true management.endpoints.web.exposure.include=health,info   info Savner du ditt rammeverk? Legg det til da vel   ","version":"Next","tagName":"h3"},{"title":"Eksponer endepunktene p√• dedikert port uten service‚Äã","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kubernetes/helsesjekker-i-kubernetes#eksponer-endepunktene-p√•-dedikert-port-uten-service","content":" Her vil det variere litt hvordan man g√•r frem avhengig av om man bruker et rammeverk eller ikke, siden prosessen i containeren m√• lytte p√• ekstra port.  F√∏rst m√• man velge seg en port, f.eks. 8081, og s√• eksponere denne i Dockerfile. I dette eksempelet vil 8080 v√¶re applikasjonsporten, og 8081 management/helseporten.  EXPOSE 8080 8081   Deretter m√• man konfigurere management-porten i application.properties p√• f√∏lgende m√•te:  management.server.port=8081 management.endpoint.info.enabled=true management.endpoint.health.enabled=true management.endpoints.web.exposure.include=health,info   Skiperator-manifestet vil v√¶re helt likt, men man insturerer kubernetes til √• gj√∏re helsesjekkene p√• en annen port.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: some-backend namespace: yournamespace spec: port: 8080 # √òvrig konfigurasjon liveness: path: /actuator/health/liveness port: 8081 failureThreshold: 3 timeout: 1 initialDelay: 3 readiness: path: /actuator/health/readiness port: 8081 failureThreshold: 3 timeout: 1 initialDelay: 5   ","version":"Next","tagName":"h3"},{"title":"Eksponer endepunktene p√• dedikert port med service‚Äã","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kubernetes/helsesjekker-i-kubernetes#eksponer-endepunktene-p√•-dedikert-port-med-service","content":" note For √∏yeblikket kan man ikke spesifisere hvilken port man skal tillate trafikk til via skiperator sin accessPolicy  Hvis man har behov for at en annen applikasjon skal kunne n√• endepunktet p√• en annen port m√• man gj√∏re ytterligere konfigurasjon. Man b√∏r ta en ekstra vurdering p√• om det er hensiktmessig √• eksponere denne typen informasjon via actuator-endepunkter.  Sett opp konfigurasjonen p√• samme m√•te som punktet over, men manifestet vil n√• inkludere spesifikasjon for ekstra porter og tilgangssstyring.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: some-backend namespace: yournamespace spec: port: 8080 additionalPorts: - name: actuator port: 8081 protocol: TCP # .. √∏vrig konfigurasjon liveness: path: /actuator/health/liveness port: 8081 failureThreshold: 3 timeout: 1 initialDelay: 3 readiness: path: /actuator/health/readiness port: 8081 failureThreshold: 3 timeout: 1 initialDelay: 5 accessPolicy: inbound: rules: - application: some-frontend port: 8081 # Ikke mulig akkurat n√•  ","version":"Next","tagName":"h3"},{"title":"Metrikker og varslinger","type":0,"sectionRef":"#","url":"/docs/metrics","content":"Metrikker og varslinger P√• SKIP gir vi deg verkt√∏yene som trengs for √• mestre den komplekse verden av skybasert teknologi. En av v√•re allierte i denne reisen er Grafana - en kraftig plattform som gir deg visuell innsikt og kontroll over ytelse og helse i sanntid. Grafana er ditt verkt√∏y til √• forst√• og optimalisere dine tjenester p√• SKIP-plattformen. Med Grafana kan du overv√•ke og analysere data fra en rekke kilder, og dermed f√• en dypere forst√•else av hvordan dine applikasjoner kj√∏rer. Men Grafana g√•r utover bare overv√•king. Det gir deg muligheten til √• lage skreddersydde dashbords som gir deg akkurat den innsikten du vil ha. Du kan spore trender, identifisere potensielle problemer f√∏r de eskalerer, og automatisere reaksjoner p√• avvikende hendelser. Ved √• bruke Grafana p√• SKIP-plattformen f√•r teknologer som deg selv muligheten til √• gj√∏re mer enn bare √• reagere p√• hendelser - du kan v√¶re proaktiv ved √• tolke data og ta beslutninger f√∏r avviket skjer. Grafana gir deg verkt√∏yene til √• ta velinformerte beslutninger, optimalisere ytelse og levere stabile og gode brukeropplevelser. S√• ta p√• deg v√•tdrakten og dykk inn i dataene til Grafana p√• SKIP-plattformen!","keywords":"","version":"Next"},{"title":"üî≠ Observability","type":0,"sectionRef":"#","url":"/docs/observability","content":"","keywords":"","version":"Next"},{"title":"Handy resources‚Äã","type":1,"pageTitle":"üî≠ Observability","url":"/docs/observability#handy-resources","content":" Intro to o11y: What is Observability?Bloggen til Charity Majors ","version":"Next","tagName":"h2"},{"title":"Databaser","type":0,"sectionRef":"#","url":"/docs/lagring/databaser","content":"","keywords":"","version":"Next"},{"title":"Lokal Postgres‚Äã","type":1,"pageTitle":"Databaser","url":"/docs/lagring/databaser#lokal-postgres","content":" Dersom man √∏nsker en lokal postgres tar man kontakt med DBA-ene for √• bestille opp en server. Da vil man f√• en Postgres-database og en administratorbruker som man kan bruke til √• opprette tabeller.  For √• bestille dette sender man ticket gjennom service desken med hvor mye lagring man trenger og circa hvor mye CPU-kraft man trenger.  N√•r man har f√•tt en database s√• er det to ting man m√• gj√∏re f√∏r man kan ta den i bruk fra en applikasjon p√• SKIP:  Bestill brannmurs√•pning for databasen ved √• opprette en sak i ServiceNow. F.eks. Jeg √∏nsker √• bestille en brannmurs√•pning for en database som skal aksesseres fra SKIP. Det er clusteret ‚Äúatkv1-dev‚Äù som trenger √• n√• ‚ÄúXXXX.statkart.no‚Äù p√• TCP port XXXX.Sett opp tilgang til databasen i Kubernetes. I Skiperator gj√∏res dette ved hjelp av external accessPolicies. Her m√• applikasjonen definere at den skal kunne snakke med den eksterne serveren som databasen lever p√•.  accessPolicy: outbound: external: - host: XXXX.statkart.no ip: &quot;XXX.XXX.XXX.XXX&quot; ports: name: db port: 5432 protocol: TCP   ","version":"Next","tagName":"h2"},{"title":"Database i sky‚Äã","type":1,"pageTitle":"Databaser","url":"/docs/lagring/databaser#database-i-sky","content":" Det er mulig √• bruke database i GCP, men her gjenst√•r det noe utforsking f√∏r vi har en god l√∏ype for produktteamene. Ta kontakt med SKIP s√• tar vi en dialog rundt database i sky. ","version":"Next","tagName":"h2"},{"title":"Distributed tracing with Tempo","type":0,"sectionRef":"#","url":"/docs/observability/distributed-tracing-with-tempo","content":"","keywords":"","version":"Next"},{"title":"What is distributed tracing?‚Äã","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#what-is-distributed-tracing","content":" In complex (and distributed) systems there are at any time many ongoing parallel processes. Some of these are interlinked or trigger each other. In order to find out which operations that originate from the same request, it is common in many systems to have a so-called Trace ID. With modern distributed tracing this is standardized, and in addition sub-operations (spans) per Trace ID are also supported. When you use a standardized setup to trace applications you also gain access to a large and exciting toolbox.  Further reading:  OpenTelemetryZipkin (interesting from a historical perspective)A general guide to getting started with distributed tracing  ","version":"Next","tagName":"h2"},{"title":"What does SKIP offer?‚Äã","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#what-does-skip-offer","content":" As part of our implementation of the LGTM stack, SKIP has chosen to offer Grafana Tempo as as service. This is a component that is fully integrated with the rest of this modern observability stack, and shares the same user interface and authentication as Grafana, Mimir and Loki.  ","version":"Next","tagName":"h2"},{"title":"How do I get started?‚Äã","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#how-do-i-get-started","content":" ","version":"Next","tagName":"h2"},{"title":"Instrumentation‚Äã","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#instrumentation","content":" warning A known limitation in the way we have collected trace data is that we up until recently have had no way of excluding certain traces automatically. This means that all Prometheus scrapes (metrics collection) and automatic health checks will also be collected. Now that issue #4628has been implemented, this can finally be rectified. Follow SKIP-1250 for updates to when this is implemented in our setup.  In order to generate, propagate and send traces the application must be instrumented.  Instrumentation can be achieved in several ways, of which 2 are relevant to us: manual and automatic instrumentation.  Manual instrumentation requires the use of a library which knows how a given integration behaves, and which enables it to connect to hooks in that integrations in order to generate new traces and/or spans if those do not already exist.  The other (and recommended) method is to use an automated approach. In the case of Java applications (the only type that has been tested as of now), you will need to bundle a java agent in your Docker image, as well as set up some extra configuration when the application is run (for example through Skiperator).  info It‚Äôs worth mentioning that the Spring ecosystem offers a form of automatic instrumentation via Micrometer Tracing and OpenTelemetry OTLP exporters. Per october 2023 this is still under development and not considered a mature enough solution to utilize in our systems.  ","version":"Next","tagName":"h3"},{"title":"Example Dockerfile‚Äã","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#example-dockerfile","content":" FROM alpine:3.18.3@sha256:c5c5fda71656f28e49ac9c5416b3643eaa6a108a8093151d6d1afc9463be8e33 AS builder ARG OTEL_AGENT_VERSION=1.29.0 # 1. Last ned p√•krevd java-agent RUN apk add --no-cache curl \\ &amp;&amp; mkdir /agents \\ &amp;&amp; curl -L https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/download/v${OTEL_AGENT_VERSION}/opentelemetry-javaagent.jar &gt; /agents/opentelemetry.jar ADD build/distributions/gbok-run*.tar /gbok FROM eclipse-temurin:11-jdk-alpine COPY cert/kartverket_root_ca.crt /usr/local/share/ca-certificates/kartverket_root_ca.crt ENV USER_ID=150 ENV USER_NAME=apprunner RUN apk add --no-cache tzdata \\ &amp;&amp; addgroup -g ${USER_ID} ${USER_NAME} \\ &amp;&amp; adduser -u ${USER_ID} -G ${USER_NAME} -D ${USER_NAME} \\ &amp;&amp; keytool -import -v -noprompt -trustcacerts -alias kartverketrootca -file /usr/local/share/ca-certificates/kartverket_root_ca.crt -keystore $JAVA_HOME/lib/security/cacerts -storepass changeit ENV TZ=Europe/Oslo COPY --from=builder --chown=${USER_ID}:${USER_ID} /gbok /gbok # 2. Kopier inn nedlastet agent COPY --from=builder --chown=${USER_ID}:${USER_ID} /agents /agents USER ${USER_NAME} EXPOSE 8081 ENTRYPOINT [&quot;sh&quot;, &quot;-c&quot;, &quot;/gbok/gbok-run*/bin/gbok-run&quot;]   ","version":"Next","tagName":"h3"},{"title":"Runtime configuration‚Äã","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#runtime-configuration","content":" In order to use the Java agent, it needs to be configured and loaded. Through testing with Grunnboken, we have arrived at the first version of configuration which can be seen here .  When this configuration is done, it is then passed to JAVA_TOOL_OPTIONS like this .  There is currently no inbuilt mechanism in ArgoKit to achieve this. We are open for PRs on this topic if anyone would like to contribute.  ","version":"Next","tagName":"h3"},{"title":"View traces‚Äã","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#view-traces","content":" Traces can be viewed through our Grafana instance at monitoring.kartverket.cloud . From here, choose Explore in the menu and then the correct Tempo data source corresponding to the environment you wish to view traces for.  After that, you have the choice of using the Search (graphical build tool for queries) or TraceQL (manual query specification) tools.  Above: The ‚ÄúSearch‚Äù tab is active, and fields have been filled through the use of dropdowns.  Above: The ‚ÄúTraceQL‚Äù tab lets you specify a user-defined query. Here is shown a query for ‚Äúgbok2-server‚Äù traces, filtering out health checks ","version":"Next","tagName":"h3"},{"title":"Grafana and GCP","type":0,"sectionRef":"#","url":"/docs/observability/grafana-and-GCP","content":"","keywords":"","version":"Next"},{"title":"Google Cloud Monitoring‚Äã","type":1,"pageTitle":"Grafana and GCP","url":"/docs/observability/grafana-and-GCP#google-cloud-monitoring","content":" It is possible to get metrics from a Google Cloud project by the use of the Grafana data source ‚ÄúGoogle Cloud Monitoring‚Äù.    Through the use of this data source, you will be able to see all metrics that are exposed through different Google Cloud services, such as CloudSQL, BigQuery, CloudKMS, Logging etc. This can then be added to your dashboards and alarms.  ","version":"Next","tagName":"h2"},{"title":"Setting up the data source‚Äã","type":1,"pageTitle":"Grafana and GCP","url":"/docs/observability/grafana-and-GCP#setting-up-the-data-source","content":" While the data source is present, it will not scrape all projects in the Kartverket organisation in GCP by default. As of writing this (13 Oct 2023), SKIP does not facilitate this setup in any particular way, but you are free to do it the ‚ÄúSKIP way‚Äù.  To add your GCP project to the list of projects, simply add the GCP role monitoring.viewer to the Google Service Account grafana-scraper@kubernetes-0dca.iam.gserviceaccount.com. It should look like the below image.    Remember that if you do not have access to editing IAM for your projects by default, you can always elevate your access using JIT Access .  Note that the setup for this may change in the future as this feature is somewhat unexplored as of writing this documentation. ","version":"Next","tagName":"h3"},{"title":"Grafana cheat sheet","type":0,"sectionRef":"#","url":"/docs/observability/grafana-cheat-sheet","content":"","keywords":"","version":"Next"},{"title":"Useful Mimir queries‚Äã","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#useful-mimir-queries","content":" ","version":"Next","tagName":"h2"},{"title":"Top 20 of metrics with high cardinality‚Äã","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#top-20-of-metrics-with-high-cardinality","content":" https://monitoring.kartverket.cloud/goto/cc_GwW1SR?orgId=1  # Set time range to &quot;Last 5 minutes&quot; topk(20, count by (__name__)({__name__=~&quot;.+&quot;}))   ","version":"Next","tagName":"h3"},{"title":"Top 10 namespaces with overallocated cpu resources‚Äã","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#top-10-namespaces-with-overallocated-cpu-resources","content":" https://monitoring.kartverket.cloud/goto/6V2jQZJIg?orgId=1  topk(10, sum by (namespace) (kube_pod_container_resource_requests{job=&quot;integrations/kubernetes/kube-state-metrics&quot;, resource=&quot;cpu&quot;}) - sum by (namespace) (rate(container_cpu_usage_seconds_total{}[$__rate_interval])))   ","version":"Next","tagName":"h3"},{"title":"Sum of overallocated cpu for containers by namespace‚Äã","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#sum-of-overallocated-cpu-for-containers-by-namespace","content":" https://monitoring.kartverket.cloud/goto/xF2DlW1SR?orgId=1  sum by (container) (kube_pod_container_resource_requests{job=&quot;integrations/kubernetes/kube-state-metrics&quot;, resource=&quot;cpu&quot;, namespace=~&quot;matrikkel.*&quot;}) - sum by (container) (rate(container_cpu_usage_seconds_total{namespace=~&quot;matrikkel.*&quot;}[$__rate_interval]))   ","version":"Next","tagName":"h3"},{"title":"Daily amount of requests by destination app and response code‚Äã","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#daily-amount-of-requests-by-destination-app-and-response-code","content":" sum by (destination_app, response_code) ( increase(istio_requests_total{namespace=&quot;&lt;namespace name&gt;&quot;, response_code=~&quot;.*&quot;, source_app=&quot;istio-ingress-external&quot;}[1d]) )   Useful Loki queries ","version":"Next","tagName":"h3"},{"title":"Alerting with Grafana","type":0,"sectionRef":"#","url":"/docs/observability/alerting-with-grafana","content":"","keywords":"","version":"Next"},{"title":"Creating alerts‚Äã","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#creating-alerts","content":" The first step to start adding alerts to your application is to onboard that app to SKIP. Grafana is only used for SKIP, the rest of Kartverket uses Zabbix. Once you have been onboarded and deployed your app to SKIP you can request access to the grafana-alerts repo.  The grafana-alerts repo is designed to be a repo that contains the alerts of all teams and handles deployment of alerts to Grafana. You will get a file which contains the configuration of your alerts in a Terraform format. For example, the file could look like this:  resource &quot;grafana_folder&quot; &quot;MYTEAMNAME_folder&quot; { for_each = local.envs title = &quot;Alerts MYTEAMNAME ${each.key}&quot; } module &quot;MYTEAMNAME_alerts_kubernetes&quot; { source = &quot;../modules/grafana_alert_group&quot; for_each = local.envs name = &quot;kube-state-metrics&quot; env = each.value runbook_base_url = # URL to document describing each alert folder_uid = grafana_folder.MYTEAMNAME_folder[each.key].uid team = { name = &quot;MYTEAMNAME&quot; } alerts = { KubernetesPodNotHealthy = { summary = &quot;Kubernetes Pod not healthy (instance {{ $labels.instance }})&quot; description = &quot;Pod has been in a non-ready state for longer than 15 minutes.\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}&quot; severity = &quot;critical&quot; for = &quot;15m&quot; expr = &lt;&lt;EOT sum by (namespace, pod) (kube_pod_status_phase{phase=~&quot;Pending|Unknown|Failed&quot;, namespace=~&quot;nrl-.*&quot;}) EOT }, # ... more alerts } }   In the above file we create an alert that monitors the health of a pod in all nrl namespaces. Pay attention to the expr field, which is the Prometheus query language PromQL. If you want to learn more about PromQL look at the documentation as well as some examples from the Prometheus documentation and the examples at awesome prometheus alerts .  This is a file that you will be given CODEOWNER access to. This means that you and your team will be able to update this file and review your own changes without involving SKIP. Your team is expected to keep them at a level that verifies the running state of the application.  Updating this file in the GitHub repo will automatically deploy the changes to Grafana.  ","version":"Next","tagName":"h2"},{"title":"Grafana Oncall Alerts‚Äã","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#grafana-oncall-alerts","content":" In addition to Grafana alerts, we have installed a plugin to Grafana called Oncall. This plugin gives us the possibility of adding schedules/shifts and custom alerting behaviour. It also gives your team an overview and a system to handle alerts.    ","version":"Next","tagName":"h2"},{"title":"Integration‚Äã","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#integration","content":" In order to start using Oncall you need an oncall integration to Grafana. This integration will show up as a contact point in Grafana which can be used in notification policies to route alerts to your integration.  From the integration you can add routes and escalation chains which decides how the integration will notify the team. The standard setup is to send all alerts to a slack channel, and also to a team member on schedule or shared inbox.      In the grafana-alerts repository we have created an oncall_integration module, which you can use to create your teams integration.  ","version":"Next","tagName":"h3"},{"title":"Routes‚Äã","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#routes","content":" In an integration you always have a default route, but you can also have a specified route. A route will decide which escalation chain the integration should use when it receives an alert. For example if you have a critical app that requires 24/7 alerting, you can create a route that checks for certain labels, and if found, it will route the alert to the ‚Äúappdrift‚Äù escalation chain.  ","version":"Next","tagName":"h3"},{"title":"Schedules‚Äã","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#schedules","content":" An Oncall Schedule is a collection of ‚ÄúShifts‚Äù. In short this means that you can assign a person to a shift, and that person will receive all alerts sent to the Oncall integration for the duration of their shift. In the grafana-alerts repository you can use the oncall_team integration to create both a schedule and escalation chain.    ","version":"Next","tagName":"h3"},{"title":"Escalation Chains‚Äã","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#escalation-chains","content":" Escalation chains are instructions to Oncall on how to notify you when the connected integration receives an alert. The standard setup here is to contact the assigned person in the set way in Oncall.  The escalation chain below will contact the person which has an assigned shift in Schedule, in the way they have set in Oncall. Usually email or slack mentions.    In Oncall ‚Üí Users ‚Üí edit user, you can decide how you want the escalation chain to contact you.    ","version":"Next","tagName":"h3"},{"title":"Terraform‚Äã","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#terraform","content":" A typical Grafana Oncall setup for a team will look like this:  module &quot;team_oncall&quot; { source = &quot;../modules/oncall_team&quot; team_name = &quot;team&quot; use_schedule = true } module &quot;team_integration&quot; { source = &quot;../modules/oncall_integration&quot; integration_name = &quot;team&quot; slack_channel_name = &quot;grafana-oncall&quot; //Not required, replace with your own vaktlag_enabled = false default_escalation_chain_id = module.team_oncall.team_escalation_chain_id }   note The slack channel must already exist in grafana. If you want to use predefined users instead of a schedule, then the users must already exist in Oncall.  ","version":"Next","tagName":"h3"},{"title":"Notification policies‚Äã","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#notification-policies","content":" You also have to configure notification policies to use your integration. Terraform doesn‚Äôt activate the contact point of the integration yet, so this has to be done manually before adding this to terraform(do this by navigating to your integration and activate the contact point). Add code here.  Example:  policy { contact_point = &quot;watchdog&quot; group_by = [&quot;cluster&quot;, &quot;alertname&quot;] matcher { label = &quot;team&quot; match = &quot;=&quot; value = &quot;Vaktlag&quot; } }   ","version":"Next","tagName":"h3"},{"title":"24/4 alerting‚Äã","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#244-alerting","content":" Once you have configured a set of alerts, you might want them to be monitored 24/7. Kartverket provides a solution for this in the form of ‚ÄúVaktlaget‚Äù. Vaktlaget is a team consisting of various people in IT-drift that have a special agreement that allows them to be notified and follow up when an alert fires outside of normal working hours.  The first step for getting your alerts onboarded onto vaktlaget is to maintain a set of alerts that only fire when there is a serious outage. Keep in mind that an alert that fires will potentially wake people in the middle of the night, so it is paramount that this set of alerts don‚Äôt contain non-critical or ‚Äúflaky‚Äù alerts. These alerts should be given a severity of ‚Äúcritical‚Äù to make them distinct from other alerts.  Once you have done this you need to contact vaktlaget to discuss the alerts you wish to onboard. They will comment on what is important enough to be onboarded and you will end up with a set of alerts that is a neat balance between ensuring the stability of our systems and preserving the mental health of the people on the alert schedule.  After you‚Äôve discussed with vaktlaget you can contact SKIP in #gen-skip to have your alert integration be switched over. When this is done, all alerts labeled with env=prod and severity=critical will be sent to vaktlaget using the following schedule:  The alerts will be sent to your slack channel all dayThe alerts will be sent to appdrift as email, SMS and phone call between 7 and 22The alerts will be sent to infrastrukturdrift as email, SMS and phone call between 22 and 7  You can also create a pull request in grafana-alerts with the vaktlag escalation chain added to your integration:  module &quot;skip&quot; { source = &quot;../modules/oncall_integration&quot; integration_name = &quot;skip&quot; slack_channel_name = &quot;grafana-oncall&quot; //Not required, replace with your own vaktlag_enabled = true vaktlag_escalation_chain_id = module.vaktlag.appdrift_escalation_chain_id default_escalation_chain_id = module.skip.team_escalation_chain_id }   When you later add more alerts to the critical level you also need to discuss with vaktlaget so they can sign off on the new alerts before they are added. ","version":"Next","tagName":"h2"},{"title":"Logs with Loki","type":0,"sectionRef":"#","url":"/docs/observability/logs-with-Loki","content":"Logs with Loki SKIP‚Äôs LGTM stack is set up to automatically collect logs from all applications running in our Kubernetes clusters. There is nothing in particular you as a developer need to configure or set in order to achieve this, apart from ensuring that your application logs to stdout . These are picked up by the Grafana Agent through the PodLogs custom resource, which specifies which namespaces to collect logs for (all of them in this case) and a set of relabeling rules to ensure that we have a common set of labels for use in searching, dashboards and alerting. Logs are collected and stored in Loki, which is backed by an on-premise S3-compatible Scality storage bucket system, one for each cluster. Each Loki instance is defined as a data source in Grafana, which provides the tools for search queries, dashboards and alerting. For an overview of the Explore section as it pertains to Loki, see https://grafana.com/docs/grafana/latest/explore/logs-integration/ . This and other pages outline the features and how to use it efficiently in relatively good detail, so we shall not attempt to reproduce such a guide here, only to point out a few things as they apply to our own setup. By necessity, the default label set is rather limited compared to what some of you might wish. This is because a large selection of labels can be extremely detrimental to performance - see https://grafana.com/docs/loki/latest/get-started/labels/bp-labels/ for an explanation. Hence, it is recommended to use filter expressions instead. You can filter on log lines containing/not containing a given text, regex expression and a host of other possibilities. The search function is also equipped with a JSON parser which makes it easier to filter on the fields you want. You can choose between two modes of searching: typing a query manually, or building a query through Grafana‚Äôs graphical query builder. As long as the query you have built or typed is valid, you can seamlessly switch between the two modes. Above: Using JSON parser to extract fields and filtering on method ‚ÄúPOST‚Äù","keywords":"","version":"Next"},{"title":"Metrics with Grafana","type":0,"sectionRef":"#","url":"/docs/observability/metrics-with-Grafana","content":"","keywords":"","version":"Next"},{"title":"Background‚Äã","type":1,"pageTitle":"Metrics with Grafana","url":"/docs/observability/metrics-with-Grafana#background","content":" So far we have been able to see metrics through Google Cloud (to be phased out) and Instana (phased out due to not fulfilling its promised potential), but from now on Grafana is the single source of truth in regards to metrics. This is because SKIP has chosen to implement the LGTM stack (Loki, Grafana, Tempo, Mimir), of which Grafana is pretty much an industry standard.  ","version":"Next","tagName":"h2"},{"title":"Getting started‚Äã","type":1,"pageTitle":"Metrics with Grafana","url":"/docs/observability/metrics-with-Grafana#getting-started","content":" ","version":"Next","tagName":"h2"},{"title":"Adjusting applications‚Äã","type":1,"pageTitle":"Metrics with Grafana","url":"/docs/observability/metrics-with-Grafana#adjusting-applications","content":" To be able to expose metrics from your own application, the following steps are required:  Expose Prometheus-format metrics on a known path, for example /metrics . We prefer this to be done on its own dedicated port for health checks, metrics and other administrative purposes. If this is not possible with your current setup, please contact us and we will work with you to find a solution.Change the Skiperator manifest by adding an extra port and enable scraping of Prometheus metrics.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: super-app namespace: team-foo-main spec: image: &quot;kartverket/example&quot; port: 8080 # Definer egen port additionalPorts: - name: management port: 8181 protocol: TCP # Skru p√• innsamling av metrikker fra den nye porten prometheus: port: management path: &quot;/actuator/prometheus&quot;   ","version":"Next","tagName":"h3"},{"title":"Viewing metrics‚Äã","type":1,"pageTitle":"Metrics with Grafana","url":"/docs/observability/metrics-with-Grafana#viewing-metrics","content":" In order to view metrics and dashboards, visit our Grafana instance at monitoring.kartverket.cloud .  Here we offer pre-provisioned dashboards that can be viewed to gain insight into how an environment is performing, as well as more detailed metrics about single applications. In addition, there is a dashboard named JVM (Micrometer) which is a generic dashboard that offers basic information about Java applications, instrumented through Micrometer.  The Explore menu offers the ability to play around with PromQL-formatted queries and view single metrics.  ","version":"Next","tagName":"h3"},{"title":"Provisioning new dashboards‚Äã","type":1,"pageTitle":"Metrics with Grafana","url":"/docs/observability/metrics-with-Grafana#provisioning-new-dashboards","content":" There are two main ways to provision dashboards in our Grafana instance.  The first way is to set it up through ‚Äúclickops‚Äù, that is, using Grafana‚Äôs own Dashboards editor in order to design and save a dashboard suited to your needs.  Building your own dashboard is a topic that is way too out of scope for this documentation, and there are many good guides and tutorials on this out there, so we shall merely include a selection of links to some of the more relevant ones here.  note We strive to continually improve our documentation, so if you have experience with building dashboards and can provide some helpful hints and tricks (or even know of other good guides out there) we would greatly appreciate any contributions to this guide.  https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/create-dashboard/ - simple introduction to building your first dashboard.https://grafana.com/docs/grafana/latest/dashboards/build-dashboards/best-practices/ - it is always worthwile to follow best practices as defined by Grafana themselves.The Four Golden Signals | Google SRE - worth reading as it mentions the most important metrics to focus on when building a monitoring solution for your system.  The second way is to add a pre-made dashboard as a JSON file (either self-made or from Grafana‚Äôs official Dashboards site ) in the skip-dashboards repository. By following the GitOps principle, you get quicker access to your own dashboards in a critical scenario in case an environment needs to be rebuilt. ","version":"Next","tagName":"h3"},{"title":"Real User Monitoring with Faro","type":0,"sectionRef":"#","url":"/docs/observability/real-user-monitoring-with-Faro","content":"","keywords":"","version":"Next"},{"title":"Getting started‚Äã","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#getting-started","content":" Setting up Faro requires two steps which are explained below:  Installing the SDKConfiguring the SDK  It will also be useful to start by reading the Faro quick start guide . See also the README of the Faro GitHub page for more links to relevant documentation.  ","version":"Next","tagName":"h2"},{"title":"Installing the SDK‚Äã","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#installing-the-sdk","content":" If you use React this is done by running one of the following commands:  # If you use npm npm i -S @grafana/faro-web-sdk # If you use Yarn yarn add @grafana/faro-web-sdk   ","version":"Next","tagName":"h3"},{"title":"Configuring the SDK‚Äã","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#configuring-the-sdk","content":" Import and configure the following options in your app‚Äôs entrypoint (main.js or similar).  import { initializeFaro } from &quot;@grafana/faro-react&quot;; initializeFaro({ app: { name: &quot;my_app_name&quot;, environment: getCurrentEnvironment(), }, url: &quot;https://faro.atgcp1-prod.kartverket.cloud/collect&quot;, });   ","version":"Next","tagName":"h3"},{"title":"List of valid options for app‚Äã","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#list-of-valid-options-for-app","content":" \tType\tDescription\tRequired?name\tstring\tThe name of the application as it will appear on dashboards in Grafana\tYes environment\t‚Äúlocalhost‚Äù | ‚Äúdev‚Äù | ‚Äútest‚Äù | ‚Äúprod‚Äù\tThe environment the frontend is currently running in. This is used to filter data in Grafana dashboards\tYes  ","version":"Next","tagName":"h3"},{"title":"Configuring the SDK with React Router integration‚Äã","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#configuring-the-sdk-with-react-router-integration","content":" Grafana Faro supports integration with React Router. This gives you events for page navigation and re-renders. See the Faro docs for more information on this.  ","version":"Next","tagName":"h3"},{"title":"Showing the data‚Äã","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#showing-the-data","content":" When the metrics have started to be gathered, they will be visible in a dedicated Grafana Faro dashboard. This dashboard can be found here .  It is also possible to search for data in the explore view . Useful labels to search for are:  faro_app_namekindenv  ","version":"Next","tagName":"h2"},{"title":"Privacy concerns‚Äã","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#privacy-concerns","content":" note It is up to you and your team to consider the how to use Faro with personal information as outlined in your IP and DPIA  When we send data to Faro, it is mostly metrics that don‚Äôt contain any PII . It is possible to include PII like name, IP or anything that is accessible from JavaScript in the SDK, but this is not done by default and requres calling the setUser function on the SDK.  A session ID is sent in to enable de-duplicating events like navigation between pages and ranking top users. This is a randomly generated string and is stored in the user‚Äôs browser SessionStorage. Note that even though this is not a cookie, this means a ‚Äúcookie banner‚Äù is required as per the EU‚Äôs ePrivacy directive .  As SessionInstrumentation is included by default in the web instrumentation of the JavaScript SDK, disabling it requires invoking the SDK with instrumentations set and omitting the SessionInstrumentation function.  Data is stored on SKIP‚Äôs atgcp1-prod cluster, which stores data in Google Cloud Storage europe-north1 region. This region is located in Finland, and is thus within EU. This means no data leaves the EU‚Äôs borders which means the storage of the data is compliant with GDPR.  ","version":"Next","tagName":"h2"},{"title":"Rate limiting‚Äã","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#rate-limiting","content":" A rate limit for requests is implemented and is currently set to 50 requests per second. This is shared between all users of Faro, so it‚Äôs possible that we eventually reach the limit. Contact SKIP if you start getting queries rejected with HTTP 429 Too Many Requests .  The rate limiting algorighm is a token bucket algorithm, where a bucket has a maximum capacity for up to burst_size requests and refills at a rate of rate per second.  Each HTTP request drains the capacity of the bucket by one. Once the bucket is empty, HTTP requests are rejected with an HTTP 429 Too Many Requests status code until the bucket has more available capacity.  ","version":"Next","tagName":"h2"},{"title":"Tracing‚Äã","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#tracing","content":" Faro supports tracing of HTTP requests, but this is not currently implemented in the collector on SKIP. Contact SKIP if you want this! ","version":"Next","tagName":"h2"},{"title":"Recording and alerting rules","type":0,"sectionRef":"#","url":"/docs/observability/recording-and-alerting-rules","content":"","keywords":"","version":"Next"},{"title":"Recording rules in Mimir‚Äã","type":1,"pageTitle":"Recording and alerting rules","url":"/docs/observability/recording-and-alerting-rules#recording-rules-in-mimir","content":" SKIP supports https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/ through Mimir and Grafana Agent. Just define a PrometheusRule object in your application‚Äôs namespace, and Grafana Agent will pick it up and add it to Mimir.  For more information, see the Prometheus documentation above, or visit https://grafana.com/docs/grafana/latest/alerting/alerting-rules/create-mimir-loki-managed-recording-rule/  ","version":"Next","tagName":"h2"},{"title":"Recording rules in Loki‚Äã","type":1,"pageTitle":"Recording and alerting rules","url":"/docs/observability/recording-and-alerting-rules#recording-rules-in-loki","content":" TODO ","version":"Next","tagName":"h2"},{"title":"Sikkerhet","type":0,"sectionRef":"#","url":"/docs/security","content":"Sikkerhet SKIP er bygget etter prinsippet om innebygget sikkerhet, slik at det blir lett √• gj√∏re rett. Standardoppf√∏rselen skal i utgangspunktet v√¶re sikker, med mulighet for produktteamene √• overstyre der det gir mening for deres applikasjon. Dette er kort fortalt hvordan SKIP balanserer behovet for sikkerhet med autonomi. Et eksempel p√• dette er prinsippet om Zero Trust i nettverkslaget. All trafikk p√• Kubernetes er i utgangspunktet stengt, en pod kan ikke snakke med en hvilken som helst annen. Kun om begge tjenestene √•pner for at de kan snakke med hverandre kan trafikken flyte mellom dem. Dette gj√∏r produktteamene selv ved √• sette accessPolicty i sitt Skiperator-manifest. All trafikk mellom podder i Kubernetes er kryptert med mTLS helt automatisk. Det eneste man trenger √• gj√∏re er √• sende sp√∏rringer til en annen pod, s√• krypterer Service Meshet koblingen automatisk. Dersom man trenger √• eksponere applikasjonen sin til omverdenen kan man konfigurere et endepunkt som applikasjonen skal eksponeres p√•. N√•r dette er konfigurert f√•r man utstedt et gyldig sertifikat som gj√∏r at all trafikk krypteres med HTTPS helt automatisk. Dette sertifikatet fornyes ogs√• automatisk. Dette og mye mer f√∏rer til at det er lett √• gj√∏re rett p√• SKIP.","keywords":"","version":"Next"},{"title":"ü•î Troubleshooting","type":0,"sectionRef":"#","url":"/docs/troubleshooting","content":"ü•î Troubleshooting","keywords":"","version":"Next"},{"title":"Teknologien bak SKIP","type":0,"sectionRef":"#","url":"/docs/tech","content":"Teknologien bak SKIP SKIP er basert p√• en moderne teknologiplattform bygget rundt Google Cloud, GitOps-prinsipper og innebygget sikkerhet. Med Kubernetes i hjertet av plattformen automatiserer SKIP orkestreringen av containeriserte applikasjoner og sikrer s√∏ml√∏s distribusjon, oppdatering og skalerbarhet. Google Clouds rike √∏kosystem gir tilgang til en rekke tjenester for √• administrere og beskytte data og applikasjoner p√• en god m√•te. Argo CD gir gjennomg√•ende innsyn og oppdateringsh√•ndtering for Kubernetes-applikasjoner, mens Skiperator gir finjustert kontroll over applikasjonsstyring og konfigurasjon. Med Grafana kan du overv√•ke ytelsen og helsen til dine tjenester i sanntid, slik at du kan identifisere og h√•ndtere eventuelle problemer raskt og effektivt. Utstrakt bruk av GitHub gj√∏r det lett √• dele kode med andre, b√•de internt og eksternt. Byggel√∏yper i form av GitHub Actions er fleksible og raske √• komme i gang med. I tillegg har man tilgang til GitHub Advanced Security for tilgang til blant annet sikkerhetsscanning og s√•rbarhetsrapporter. Med SKIP er det lett for utviklere √• utforske mulighetene innen moderne skyinfrastruktur. Vi p√• SKIP-teamet hjelper deg med √• raskt implementere, administrere og optimalisere skybaserte tjenester, og holde tritt med det stadig skiftende teknologiske landskapet. Velkommen ombord til SKIP!","keywords":"","version":"Next"},{"title":"Troubleshooting on SKIP","type":0,"sectionRef":"#","url":"/docs/troubleshooting/troubleshooting-on-skip","content":"Troubleshooting on SKIP info This page is under construction and may be updated without warning Troubleshooting an application on SKIP can be daunting. This documentation aims to give readers a rough idea of where to start and what to look for when troubleshooting. It is intended for use both by SKIP team members as well as product team members, and we will take care to specify troubleshooting steps that might need additional access that is only available to SKIP team members or administrators. Relevant links Skiperator code and documentation CLI Cheatsheet for SKIP (may require additional privileges) General checklist when troubleshooting Network/Istio related issues: Network policies - default-deny and others (if applicable).AccessPolicies both outbound and inbound.ServiceEntries+++","keywords":"","version":"Next"},{"title":"Migrering fra atkv1 til atkv3","type":0,"sectionRef":"#","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3","content":"","keywords":"","version":"Next"},{"title":"Deployment av applikasjoner til nye clustere‚Äã","type":1,"pageTitle":"Migrering fra atkv1 til atkv3","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3#deployment-av-applikasjoner-til-nye-clustere","content":" Vi har begynt √• f√• mange cluster etterhvert, og har derfor valgt √• gj√∏re om litt p√• mappestrukturen i apps repoet. I dag har vi en struktur som, litt forenklet, ser slik ut:    Vi kommer til √• havne p√• en struktur hvor hvert cluster man √∏nsker √• deployere til er representert med en egen mappe under env:    Legg merke til at de gamle mappene kan eksistere sammen med de nye, og det skjer ikke noe med det som ligger i atkv1-dev/prod f√∏r mappene fjernes.  ","version":"Next","tagName":"h2"},{"title":"√Öpninger til tjenester utenfor skip‚Äã","type":1,"pageTitle":"Migrering fra atkv1 til atkv3","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3#√•pninger-til-tjenester-utenfor-skip","content":" For √• kunne n√• tjenester som er internt p√• kartverket, men utenfor skip, eller i et annet cluster m√• det ogs√• bestilles √•pninger til disse p√• nytt, siden atkv3 clusteret er del av en annen brannmursone.  Har man i dag tilgang til et NFS share, eller en database m√• dette testes, og eventuelle √•pninger bestilles f√∏r det kan fungere.  ","version":"Next","tagName":"h2"},{"title":"DNS‚Äã","type":1,"pageTitle":"Migrering fra atkv1 til atkv3","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3#dns","content":" Dev clusteret st√∏tter kun adresser under domene atkv3-dev.kartverket-intern.cloud for interne og atkv3-dev.kartverket.cloud for eksterne. Produksjonsclusteret st√∏tter atkv3-prod.kartverket-intern.cloud og atkv3-prod.kartverket.cloud. Men det vil i produksjonsclusteret ogs√• v√¶re mulig √• definere sine egne domener eksternt. Egene vanity URL-er som f.eks nrl.kartverket.no som allerede eksisterer p√• atkv1 i dag, pekes i dag til lb01.kartverket.no m√• da pekes over til ny lastbalanserer (atkv3-prod.kartverket.cloud) for √• kunne n√•s p√• atkv3.  ","version":"Next","tagName":"h2"},{"title":"Test clusteret finnes ikke lenger!‚Äã","type":1,"pageTitle":"Migrering fra atkv1 til atkv3","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3#test-clusteret-finnes-ikke-lenger","content":" Test clusteret finnes ikke lenger, alternativet her er √• bruke namespace i enten dev eller prod cluster. Har man test-tjenester som skal v√¶re tilgjengelig for brukere utenfor kartverket, b√∏r dette legges til produksjons clusteret, og dev clusteret st√∏tter kun, som nevnt, adresser i domenet atkv3-dev.kartverket.cloud for ekstern tilgang, og atkv3-dev.kartverket-intern.cloud for interne.  ","version":"Next","tagName":"h2"},{"title":"Endringer p√• ArgoCD‚Äã","type":1,"pageTitle":"Migrering fra atkv1 til atkv3","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3#endringer-p√•-argocd","content":" For √• kunne administrere flere cluster, med f√¶rre ArgoCD instanser har vi valgt √• konsolidere alle for dev og prod til en instans for prod, og en for dev. Disse legges i sky, sammen med den nye grafana instansen. Dette gj√∏r at vi f√•r en prefix for alle argo applikasjoner med clusteret, feks atkv3-KulApplikasjon eller atgcp1-KulApplikasjon, som vil kunne ligge p√• samme argoinstans, men p√• to forskjellige clustere. ArgoCD kan n√•s HER.  ","version":"Next","tagName":"h2"},{"title":"Endringer p√• Grafana‚Äã","type":1,"pageTitle":"Migrering fra atkv1 til atkv3","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3#endringer-p√•-grafana","content":" Egress fra sky koster penger, mens ingress er gratis. S√• for √• slippe √• hente ut data fra sky cluster til on-prem for √• vise det i grafana, velger vi √• vise data onprem via sky. Det gj√∏r at logger og metrikker som vises i den nye grafana visualisers i sky, men lagres fremdeles on-prem.  note Hva betyr dette for deg? Logger lagres fremdeles p√• Kartverket men g√•r via Google Cloud idet man leser dem. Dere m√• ha et forhold til loggenes innhold og bekrefte at dere forst√•r at de g√•r via Google Cloud og aksepterer det f√∏r dere starter migrering til atkv3.  info Ny URL: https://monitoring.kartverket.cloud  ","version":"Next","tagName":"h2"},{"title":"Trafikk-flyt ArgoCD og Grafana‚Äã","type":1,"pageTitle":"Migrering fra atkv1 til atkv3","url":"/docs/troubleshooting/migrering-fra-atkv1-til-atkv3#trafikk-flyt-argocd-og-grafana","content":"  ","version":"Next","tagName":"h3"}],"options":{"languages":["en","no"],"id":"default"}}