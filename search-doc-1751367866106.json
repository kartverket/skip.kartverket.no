{"searchDocs":[{"title":"20 teams on SKIP: What we've learned along the way","type":0,"sectionRef":"#","url":"/blog/20-teams-on-skip","content":"","keywords":"","version":null},{"title":"Principles matter​","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#principles-matter","content":" When you set out to create something new, you have the privilege of setting some standards that encourage best practices. While this is possible to do for an existing system, in practice it will mean a lot of work to get to the point where you're able to enforce these standards. It's much easier to start with a clean slate.  For our platform, we decided on a set of principles that we wanted to follow. Some of these are:  Stateless: Our clusters are stateless, which means that we can easily replace them if something goes wrong. All configuration is held in a GitOps repository and all state is held in external systems like managed databases, object storage, etc. This significantly reduces operational complexity and recovery time. If a cluster fails we can easily replace or revert it by applying the configuration from the GitOps repository without worrying about losing state, or doing time-consuming data recovery operations.Ownership: For each application, there is a clear owner. This owner is responsible for the application and maintains and supports it. This way we're able to avoid the &quot;tragedy of the commons&quot;, where no one is responsible for an application. If an app has unclear or short-term ownership, you simply don't get to use the platform. We're not an orphanage.Financing: You use the platform? You also need to pay for its continued support and development. We're working towards a chargeback model where your department is billed for the resources they use as a way to ensure that the platform is sustainable. Until this is ready, we expose the costs of the resources used by each team and then negotiate with the departments on how to cover these costs, but this is time-consuming work.Secure by default: We enforce security best practices by default. Examples of this are zero trust networking with Network Policies, where no app can talk to another without explicitly allowing this. Some applications will need to opt out of some of these defaults, and they can do so by altering their configuration. But the defaults are secure, which is especially useful for teams that are new to Kubernetes.  All teams that are onboarded on SKIP are given an introduction to these principles and are expected to follow them. This means that being able to use the modern platform is contingent on the teams being able to prioritize modernizing their applications and working in sustainable ways, which helps push for positive change.  ","version":null,"tagName":"h2"},{"title":"Encourage collaboration​","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#encourage-collaboration","content":" It's easy for a product team to ask the platform team for help when they're stuck. We're always happy to help, but we also have a heavy workload of exciting things we're working on. Therefore it's much better when platform users can help each other, as this facilitates collaboration and learning. This is why we highly encourage teams to help each other out - to build a community around the platform.  In practice this is done through a single Slack channel where all teams that are using the platform are invited. This is a great place to ask questions, share experiences, and learn from each other - and it's a place where all new features and changes are announced. We used to have many different channels for different teams, but we found that this was not as effective for building a community as a channel where everyone can help each other out.  And a final tip: As a platform developer, sometimes it's better to wait a little while before responding to questions in these channels to allow the community to help each other out before you jump in and help.  ","version":null,"tagName":"h2"},{"title":"Make time for innovation​","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#make-time-for-innovation","content":" It's easy to get bogged down in the day-to-day work of keeping the platform running. This is why it's important to set aside time for innovation, this is something we take very seriously.  On SKIP we have dedicated innovation days where we work on new features, improvements, and other things that we think will make the platform better. This is an extraordinarily successful initiative, and we've seen many great features come out of these days. It's also a great way to build team morale and to build a culture of learning and innovation.  In practice we have two days in a row of dedicated innovation work every other month. We used to have one day every month, but we found that this was not enough time to really get into the flow of things so we started doing two days every 2 months, which worked better. We also have a rule that you can't work on anything that's on the roadmap, as this is work that we're already going to do. This is a great way to get new ideas and to work on things that might not otherwise get done.    There's a little bit of structure around these days, but not too much.  First, it is understood by everyone that these days are for things that are &quot;useful for Kartverket&quot;. This means that you can't work on your own pet project, but it's vague enough that you can work on pretty much anything that you think will be useful for the organization.  Then, a week before the innovation day we will have a &quot;pitching session&quot;, where everyone who has an idea can pitch it to the rest of the team. This is a great way to get feedback on your idea and to get others to join you in working on it.  Finally, we have a &quot;show and tell&quot; session at the end of the last day where everyone shows what they've been working on. This way we can share our experiences and discuss if this work can be improved and put into production. We encourage everyone to show something, even if it's not finished or you did video lessons, as this creates discussion and further ideas.    There's plenty of examples of features that are results of work done on these days. On-premise Web Application Firewall with Wasm, Grafana features, open source tools like Skiperator andSkyline as well as this very website!  No one has time to prioritize innovation, and we're no different. But we prioritize it anyway, because we know that it's important to keep improving and to keep learning.  ","version":null,"tagName":"h2"},{"title":"Communication is key​","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#communication-is-key","content":"   Unfortunately a lot of infrastructure teams don't prioritize communication very well. This is a mistake. Communication is key to building a successful platform.  Your users exist in the context of all the platform features that you have shipped and the changes you will ship in the future. Not informing them and keeping them up to date with what's going on is a surefire way to lose their trust and to make them unhappy.  It starts with simply informing users of the new features that ship. This can be done through a Slack channel, a newsletter, a blog or a town hall meeting. We use a combination of all of these, but the most important thing is that you inform your users of what's coming. An added benefit of this is helps push adoption of new features and excitement around the platform by showcasing innovation.  The next step is informing users on what will ship and when. This will help users plan their work and to know what to expect, but it also helps users feel involved when they see their requests being planned. This can be done through a roadmap, a technical forum, or a blog. We use a combination of all of these, but the easiest way to do this is to have a roadmap that you keep up to date on a regular basis.  Now for the hard part: When things go wrong, you need to communicate this as well. Product teams will want to know when their applications are affected by outages or other issues, and they will want to know what you're doing to fix it. This can be done through a status page, a Slack channel, or postmortems. Again, we use a mix of these so that we can reach as many users as possible at the right time.  Do these things and you will have happy users that feel informed.  ","version":null,"tagName":"h2"},{"title":"Branding is important​","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#branding-is-important","content":"   Do you think Spotify would be as successful if it was called &quot;Music Player&quot;? Do you think Apple would be as successful if it was called &quot;Computer Company&quot;? Of course not. Branding is important. It builds a sense of identity and community around your platform.  This is especially important for a platform team, as you're not just building a product, you're building a community. You want your users to feel like they're part of something bigger, and you want them to feel excited to use the platform.  When you're starting out, you want to drive adoption. Here a brand really helps as it's easier to talk about a good brand in a positive way. It's also easier to get leadership buy-in when you have a strong brand.  This holds true when you're more established as well. When you grow larger than your ability to talk to everyone, a brand helps you communicate your values and intent to your users, which will drive organic growth from teams that want to work with you.  A minimum viable brand is a logo, a name, and a color scheme. This is something you should think deeply about, as it's something that will stick with you for a long time. After this you can think about a website, merchandise like stickers and t-shirts, and a mascot. These things are not necessary, but they can help build a sense of identity and community around your platform.  ","version":null,"tagName":"h2"},{"title":"Using the cloud is a long journey​","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#using-the-cloud-is-a-long-journey","content":" As a platform team, it's our responsibility to push for modern, user-friendly and secure solutions. This generally means using public cloud solutions like Google Cloud Platform. But for most organizations, pushing this narrative incurs significant friction and to some extent fear due to legal and cost concerns. This is understandable, as the known is always more comfortable than the unknown, and it's a view that's hard to change.  This is why it's important to take a long-term view on this. You're not going to move everything to the cloud overnight, and you're not going to convince everyone to get on board with this idea overnight. It's a long journey, and you need to be patient and persistent.  We've spent years pushing for the cloud, and we're still not there. You're going to have to participate in many (many!) meetings, and you're going to have to fight for every little thing over and over again. But it's necessary. Once everyone has a clear understanding of the risks and how to mitigate them, you will be able to formulate a document guiding the organization's teams on how to get to the cloud from a compliance point of view.  If you asked me for any recommendations on how to get to the cloud as easily as possible, it would be to first get leadership buy-in across the organization. This is important, as it will make any large initiative like cloud migration easier. After this and a competent platform team is in place, you can start pushing for the cloud technologies and eventually cloud migration. Here you need to talk directly with the legal team, not via other people. Have representatives of the platform team sit down with the lawyers and talk through the risks and how to mitigate them. This is the only way you can combine the technical and legal aspects of this work. Working in silos and not talking to each other is a surefire way to fail.  ","version":null,"tagName":"h2"},{"title":"Autonomy and platform as a product​","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#autonomy-and-platform-as-a-product","content":" Your platform is a product, and so you need to work as a product team. This means continuously improving your product, listening to your users, and building the features that they need.  Research-based literature like &quot;Team Topologies&quot; establishes the importance of autonomous teams in modern organizations. Traditional top-down organizations are just not going to be able to have as close of a relationship with their stakeholders as a team that is able to proactively understand the needs of their users and make their own decisions that push continuous improvement of their products. This is why it's important, even for infrastructure teams, to be able to own their roadmap and make decisions on what to build when.  As a team you're obviously limited to the amount of resources you have and not able to do everything, so understanding the needs of your stakeholders and prioritizing them is essential. You need to do research to know the needs of your users; sometimes requests don't align well with the actual needs. Just because someone asks loudly for something, doesn't mean it's the right fit for your platform. Saying yes to everything does not result in a good product. Dare to challenge assumptions and ask why.  ","version":null,"tagName":"h2"},{"title":"Abstractions save time​","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#abstractions-save-time","content":" It should go without saying that a platform team's job is to make tools that make product teams' jobs easier. But it really can't be said enough. The better the tooling you provide, the less you have to do support. This is a win-win for everyone.  When building tools, think about how you can abstract away complexity. This can be done in many ways, but we've had great success building an operator that abstracts away the complexity of managing applications on Kubernetes. The operator is called Skiperator and makes deploying applications on Kubernetes as easy as writing a configuration manifest.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: namespace: sample name: sample-two spec: image: nginxinc/nginx-unprivileged port: 80 replicas: 2 ingresses: - foo.com - bar.com   The key takeaway here is that abstractions like Skiperator are designed to speak the language of the user. There is no mention of NetworkPolicies or Istio VirtualServices in the configuration, as these are things that the user generally doesn't have any knowledge of. Instead, the user can specify things like &quot;I want to expose this service to the internet&quot; or &quot;I want to run this job every day at midnight&quot;. This simplifies the user experience of Kubernetes, which is a complex system, and makes it easier for users to get started.  Work smarter not harder.  ","version":null,"tagName":"h2"},{"title":"Build forward- and backwards compatibility​","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#build-forward--and-backwards-compatibility","content":" We've had multiple experiences where we've weighed our options and decided to make a breaking change. Just recently we asked our users to migrate their apps from one cluster to another in order to improve the architecture of the platform. Multiple options were considered, but in the end the scale of the changes meant that upgrading the clusters in-place would not be practical, so we commissioned new clusters with the new architecture and asked users to migrate their apps.  In our case, we had a simple way to migrate, only requiring moving a config file from one directory to another to make the change. But even so, this was a time consuming process for our users, and a laborious process for us to support. This is because even though the change was simple, it was still a change that required testing and validation, and it was a change that was not necessarily the highest priority for the teams that were asked to make it. So even though the change was simple, it took months.  If you ask your users to make changes to their applications, you're asking a team that is already busy to do more work. Any changes you ask them to make will take time, as it would not necessarily be the highest priority for them. Therefore avoiding breaking changes should be a primary goal, so wherever possible building in forward and backwards compatibility by inferring as much as possible from the existing configuration is a good thing.  When building operators, don't change or remove fields that are in use. Use default values for new fields, and use lists of objects instead of raw values like lists of strings as they are easier to extend.  ","version":null,"tagName":"h2"},{"title":"Documentation is key​","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#documentation-is-key","content":" One thing we keep hearing from our users is the need for more and better documentation. This is understandable. When you're using a platform, you don't want to have to ask for help all the time - you want to be able to discover platform features and implement them yourself with the support of good documentation.  The point here is that as a platform team you need to prioritize documentation. A task is not done until it has been documented. This way announcing new features will always include a link to the documentation where users can dive deeper into the feature and how to use it, like the example below.    The bigger challenge here is preventing documentation from going stale. It's too easy to forget about updating documentation to reflect changes in the code. Here we can share a few tips from our experience:  First, the obvious way to keep docs up to date is to allocate time to update them. One way we do this is that a few times a year we will do a documentation grooming session where we huddle together and review documentation, rewriting it when we find out of date information.  A more interesting way to keep docs up to date is changing how you respond to questions. Instead of answering questions immediately, we should be asking ourselves: &quot;How can we make sure that this question never gets asked again?&quot;. In our case we spend some time to write documentation or improve existing documentation and reply with a link to the documentation page. This is a triple win, as you will now have more updated documentation, save time in the future by being able to refer to the improved docs instead of writing a lengthy response and the user will now know where to look for answers.  ","version":null,"tagName":"h2"},{"title":"Learn from others​","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#learn-from-others","content":" When building a platform you'll quickly learn that you don't have all the answers. You might discuss how to implement a feature with your team, but you might not have the experience to know what works well in this context. When you get into this situation, an outside perspective can be crucial to avoid making costly mistakes.  One great advantage of working in the public sector is that we can ask other public sector platform teams for advice and learn from their experiences. We can also share our experiences with others, which is usually interesting. Invest some time in building these relationships of mutual benefit.    I also want to give special credit to Hans Kristian Flaatten and the Public PaaS network here. Having a shared forum to discuss platform issues is a strong asset and helps the Norwegian public sector get ahead and stay competitive.  Even if you work in the private sector, you can still learn from other organizations. Honestly, if you want to learn from someone's experiences it never hurts to ask. Teams generally want to help each other out, and it's usually possible to make a trade of some sort. I suggest to offer to give a talk on your experiences and ask if they can do the same. It's a win-win for both parties.  ","version":null,"tagName":"h2"},{"title":"Conclusion​","type":1,"pageTitle":"20 teams on SKIP: What we've learned along the way","url":"/blog/20-teams-on-skip#conclusion","content":" You may think building a platform is mostly technology, and we've written a lot about technology in previous blog posts. But it's important to remember that building a platform is also about building a community, and communities have expectations and needs that go beyond technology. This is a strength, and not a weakness, as if you're able to inspire and motivate your users you will be able to build a platform that is sustainable and that drives positive change in your organization.  Best of luck in your endeavors! ","version":null,"tagName":"h2"},{"title":"Crisis Management Exercises","type":0,"sectionRef":"#","url":"/blog/crisis-management-exercises","content":"","keywords":"","version":null},{"title":"Exercise 1: Malicious actor​","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#exercise-1-malicious-actor","content":"   The first exercise scenario revolved around a malicious actor gaining privileged access to our production Kubernetes cluster, simulated in this case by our internal sandbox cluster. Admittedly, it was somewhat difficult to set up a realistic scenario without outright disabling some of our security tools, so in the end we simulated a hostile takeover of the user account belonging to the person responsible for planning and running the exercise.  The first sign that something was amiss was an alert from our Sysdig Secure toolset, a Falco-based agent software which continually monitors our cluster for signs of abnormal activity according to a predefined ruleset and provides a SaaS portal for further analysis and management of threats. (We will cover more of our security features and mechanisms and how we try to build a modern kubernetes based application platform with built-in security and zero trust in a future blog post.) After initial examination, we found that the incident was of such a nature that we engaged our crisis management plan in order to investigate, contain and mitigate the incident. We simulated communication with the organization-level crisis management team, having regular meetings in order to keep them informed of progress. Systematic examination of logs and audit logs soon turned up suspicious activity confined to one specific platform developer account, and the decision was made to immediately suspend (simulated in this case) the account, removing all access to organizational systems and in effect locking it out. Simultaneously, the malicious software was removed once enough evidence was secured in order to further analyze the actions and impact of it. The exercise was announced as ended once we suspended the compromised user account and removed the malicious application while retaining and analyzing enough logs, forensic captures and other traces of activity.  ","version":null,"tagName":"h2"},{"title":"Exercise 2: \"Everything is on fire\"​","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#exercise-2-everything-is-on-fire","content":"   The second exercise scenario was somewhat more involved, taking place over two days. The incident itself was as follows: A software update or rogue script caused catastrophic hardware failure in production infrastructure, necessitating creation of a new Kubernetes cluster from scratch. Once the cluster itself and all underlying infrastructure had been created and configured, it would then be up to our platform team to deploy all necessary IAM configuration, service accounts, RBAC and supporting systems (Istio, ArgoCD ++) needed to deploy workloads and restore normal operations. The exercise itself focused on this second phase of restoration, as the infrastructure configuration and cluster creation itself is done by another team, with little involvement by our platform team members.  The failure itself was simulated by having our infrastructure team wipe our sandbox environment and present us with a clean-slate Kubernetes cluster. We called an all-hands meeting and set to work restoring services right away. Right at the onset, we recognized that this was a golden opportunity both to ensure that our documentation was up-to-date, consistent and easy to follow, as well as give our three newest team members some much-needed experience and insight into setting up our services from scratch. We therefore decided that the newest team members would be the ones to actually execute all the actions outlined in our documentation, while the rest of us followed along and made notes, updated documentation and otherwise provided guidance throughout the process.  The first run-through of the recovery process took around 2-3 hours before everything was in working order. Keep in mind that we took the time to update our documentation and explain everything we did while we were working, so in a real-life scenario this would have been even quicker. Once the IAM, RBAC, Istio and ArgoCD was up and running, it was merely a matter of using ArgoCD to synchronize and deploy all relevant workloads. Afterwards, we had a meeting to discuss the process and what experiences we gained from it. Based on the feedback from this meeting, we made further adjustments and updates to our documentation in order to make it even easier to follow on a step-by-step basis, focusing on removing any ambiguity and put any &quot;tribal&quot; knowledge among our platform developers into writing. This ensured that we are way less dependent on the knowledge and skillset of specific people, enabling any team member to contribute to recovery efforts by simply following the documentation.  The newest team members greatly enjoyed being responsible for the recovery effort itself, and expressed a wish to run through the scenario again in order to refine their skills and further improve the documentation. Therefore, we decided to set aside most of day 2 to do just that. We had the infrastructure team tear down and setup the cluster again, and let the newest team members loose on it - this time on their own without guidance - an additional two times. The last run-through of the exercise took between 30 and 60 minutes, a significant improvement from the initial attempt.  All in all, we considered the exercise to be a great success, with many important lessons learned and a substantial improvement in the quality of our documentation and crisis management plans.  ","version":null,"tagName":"h2"},{"title":"What did we learn?​","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#what-did-we-learn","content":"   ","version":null,"tagName":"h2"},{"title":"Lesson 1: You are only as good as your documentation​","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-1-you-are-only-as-good-as-your-documentation","content":" Documentation is vitally important during a crisis, and should be detailed enough that any team member may follow it on a step-by-step basis and be able to restore normal service, even with minimal knowledge and during a stressful situation. This ensures that you avoid being dependent upon key personnel that might or might not be available during a crisis scenario, and also ensures that you retain vital institutional knowledge even when team members move on to different tasks or even new jobs.  ","version":null,"tagName":"h3"},{"title":"Lesson 2: Logging, logging, logging! Oh, and monitoring too!​","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-2-logging-logging-logging-oh-and-monitoring-too","content":" Having the ability to search through logs of all parts of our system greatly simplifies any incident management, whether the incident revolved around malicious actors or other factors. But logs by themselves are not sufficient - you need some sort of monitoring and alerting system in order to alert on and react to abnormal situations/behaviour in your systems. Ideally, you should be able to react on these alerts instead of messages from users - or worse, customers - that something is wrong.  ","version":null,"tagName":"h3"},{"title":"Lesson 3: Test your plans!​","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-3-test-your-plans","content":" Merely having plans, routines and documentation is insufficient. Unless they have been thoroughly tested and their quality assured through crisis exercises in realistic scenarios and conditions, they should be treated as flawed and unreliable until the opposite is proven. Running crisis management exercises is a great way to expose flaws, insufficiencies and outdated documentation, and careful note-taking and postmortems should be the norm throughout the exercise in order to easily identify and update weak spots in your plans and documentation. As systems and circumstances change, so should plans and documentation too in order to reflect the new order of the day.  ","version":null,"tagName":"h3"},{"title":"Lesson 4: Communicate!​","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-4-communicate","content":" Openness and communication is critical during both exercises and real-world crisis scenarios. Plans should always involve key points of communication - who needs to be informed, whose responsibility it is to keep said people informed, and the frequency, scope and format of information to disseminate. This also applies to communication afterwards. Anyone in your organization should be able to understand what happened, how it was solved and what lessons were learned from it. In Kartverket, we solve this by writing postmortems about incidents, summing up the incident itself and what we learned from it. We favour Blameless Postmortems, enabling us to quickly and thoroughly analayze and document all aspects of an incident without focusing on individual mistakes and avoid passing blame. This contributes to a culture of openness, learning and improvement. Hoarding information and disseminating it only on a &quot;need-to-know&quot; basis only breeds distrust and contempt, as does a culture that focuses on blaming and punishing people for mistakes instead of learning from them. A further bonus when communicating the happenings and results of your crisis management exercises is the potential to inspire others - when people see the great results and lessons you yourselves have gained from such exercises, they might want to try it with their own systems and teams.  ","version":null,"tagName":"h3"},{"title":"Lesson 5: Let the \"newbies\" handle it​","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-5-let-the-newbies-handle-it","content":" Putting our newest team members in charge of the recovery operations was a great learning experience for them, as well as enabling us to quickly find flaws and shortcomings in our documentation and crisis management plans. It is also a great confidence booster, because if they succeed, they'll gain valuable insight and positive experiences with setting up all those scary critical systems from scratch - and if they don't succeed, well, that's not their fault, it was because the documentation and training was insufficent to enable them to handle the situation!  ","version":null,"tagName":"h3"},{"title":"Lesson 6: Crisis exercises as team building​","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#lesson-6-crisis-exercises-as-team-building","content":" Crisis exercises are fun and contribute to better teamwork! They bring everyone together in order to achieve a common goal - get things up and running again as quickly as possible. Combine it with &quot;pair programming&quot; - that is, if possible make sure at least two people are working on any given task together - this helps facilitate cooperation and communication, and provides an extra set of eyes to help catch any manual errors or deviations from the plan.  ","version":null,"tagName":"h3"},{"title":"Thank you for reading!​","type":1,"pageTitle":"Crisis Management Exercises","url":"/blog/crisis-management-exercises#thank-you-for-reading","content":" We appreciate you taking the time to read through this blog post. We have learned quite a lot (and had lots of fun) through our approach to crisis management exercises. We hope our experiences and thoughts regarding this subject has been interesting, and that they may inspire others to start doing crisis management exercises as well. ","version":null,"tagName":"h2"},{"title":"Hybrid Kubernetes in production pt. 1","type":0,"sectionRef":"#","url":"/blog/hybrid-kubernetes-in-production-part-1","content":"","keywords":"","version":null},{"title":"So why a hybrid cloud?​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#so-why-a-hybrid-cloud","content":"   Were you to take the time machine back a few years, you would see Kartverket as a traditional enterprise with a lot of knowledge and experience in running on-premise workloads. This knowledge served us well, but also slightly held us back in terms of our imagination. We knew that there had to be a better way, but our enterprise was simply not mature enough to adopt a pure cloud strategy. The fear of the unknown cloud weighed heavily on many people, and therefore few people wanted to take the risk of moving to the cloud.  This is something we've worked on for a long time, and still are. After a long time of working with the stakeholders in the organization, we eventually built a cloud strategy, which in simple terms stated that we would prefer SaaS-products over hosting things ourselves, and that we would gradually move our workloads to the cloud.  This cloud strategy however, which cleared up a lot of blockers, came too late for us on SKIP. At that point we had already done most of the work on our on-premise platform, building on the assumptions the organization held at the time, which was that we met our needs through existing infrastructure and that using public cloud had disqualifying cost and compliance implications. For SKIP it was therefore full steam ahead, building the on-prem part first, then adding the hybrid and cloud part later.  It's not like we would have ended up with a pure cloud setup in any case, though. If you're at all familiar with large enterprises, you will know that they are often very complex. This is also true for Kartverket, where we have a lot of existing systems that are not easy to move to the cloud. We also have a lot of systems that are not suitable for the cloud, mostly because they are designed to run in a way that would not be cost effective in the cloud. In addition we have absolutely massive datasets (petabyte-scale) that would be very expensive to move to the cloud.  Because of these limitations, a pure cloud strategy is not considered to be a good fit for us.  A hybrid cloud, however, can give us the scalability and flexibility of the cloud, while still allowing us to run some of our systems on-prem, with the experience being more or less seamless for the developers.  ","version":null,"tagName":"h2"},{"title":"Why we chose Anthos​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#why-we-chose-anthos","content":" After some disastrous issues with our previous hybrid cloud PoC (that's a whole story in itself) we decided to to look at what alternatives existed on the market. We considered various options, but eventually decided to run a PoC on Anthos. This was based on a series of conditions at the time, to name a few:  We had a decent pool of knowledge in GCP compared to AWS and Azure at the timeSome very well established platform teams in the public sector were also using GCP, which meant it would be easier to share work and learningsAnthos and GCP seemed to offer a good developer experience, which for us as a platform team is of paramount importanceA provider like Google is well established in the cloud space (especially Kubernetes), and would have a fully featured, stable and user friendly product  SKIP ran the Anthos PoC over a few months, initially as an on-prem offering only. Drawing on the knowledge of internal network and infrastructure engineers, this took us all the way from provisioning clusters and networking, to iterating on tools and docs and finally onboarding an internal product team on the platform. Once we felt we had learned what we could from the PoC, we gathered thoughts from the product team, infrastructure team and of course the SKIP platform team.  The results were unanimous. All the participants lauded the GCP user interfaces that allowed visibility into running workloads, as well as the new self-service features that came with it. Infrastructure engineers complimented the installation scripts and documentation, which would make it easier to keep the clusters up to date.  Based on the total package we therefore decided to move ahead with Anthos. To infinity and beyond! 🚀  ","version":null,"tagName":"h2"},{"title":"What is Anthos anyway?​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#what-is-anthos-anyway","content":"   Anthos is Google's solution to multicloud. It's a product portfolio where the main product is GKE (Google Kubernetes Engine) on-premise. Using GKE on-prem you can run Kubernetes clusters on-premise and manage them from the same control plane in Google Cloud, as if they were proper cloud clusters.  In fact, Anthos is truly multi-cloud. That means you can deploy Anthos clusters to GKE and on-prem, but also AWS and Azure. On other cloud platforms it uses the provider's Kubernetes distribution likeAKS, but you can still manage it from GKE alongside your other clusters.  In addition to GKE, the toolbox includes:  ","version":null,"tagName":"h2"},{"title":"Anthos Service Mesh (ASM)​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#anthos-service-mesh-asm","content":" A networking solution based on Istio. This is sort of the backbone of the hybrid features of Anthos, as provided you've configured a hybrid mesh it allows applications deployed to the cloud to communicate with on-premise workloads automatically and without manual steps like opening firewalls.  All traffic that flows between microservices on the mesh is also automatically encrypted with mTLS.  ","version":null,"tagName":"h3"},{"title":"Anthos Config Managment (ACM)​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#anthos-config-managment-acm","content":" A way to sync git repos into a running cluster. Think GitOps here. Build a repo containing all your Kubernetes manifests and sync them into your cluster, making cluster maintenance easier.  ACM also includes a policy controller based on Open Policy Agent Gatekeeper (OPA) which allows platform developers to build guardrails into developers' workflows using policies like &quot;don't allow containers to run as root&quot;.  ","version":null,"tagName":"h3"},{"title":"Anthos Connect Gateway​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#anthos-connect-gateway","content":" The connect gateway allows developers to log on to the cluster using gcloudand kubectl commands, despite the cluster potentially being behind a firewall. From a user experience standpoint this is quite useful, as devs will be logged in to GCP using two factor authentication, and the same strong authentication allows you to access kubernetes on-premise.  Connect Gateway also integrates with GCP groups, enabling RBAC in Kubernetes to be assigned to groups instead of manually administered lists of users.  Currently the connect gateway only supports stateless requests, for examplekubectl get pods or kubectl logs (including -f). It does not supportport-forward, exec or run, which can be a bit annoying.  ","version":null,"tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 1","url":"/blog/hybrid-kubernetes-in-production-part-1#summary","content":" As you can see, the above tools gives us a lot of benefits.  Combined with the power of Google Cloud and Terraform, they give us a good combination of flexibility through cloud servicesEase the maintenance by using the tools that Anthos and Terraform supply usEases the compliance and modernization burden by allowing a gradual or partial migration to cloud, allowing parts to remain on-premise while still retaining most of the modern tooling of the cloud  That's it for now! 🙂 We'll be back with more details on how we run Anthos as well as the pros and cons we've seen so far in the coming weeks. Stay tuned!  Disclaimer - Google, GKE and Anthos are trademarks of Google LLC and this website is not endorsed by or affiliated with Google in any way. ","version":null,"tagName":"h2"},{"title":"SKIP on Plattformpodden!","type":0,"sectionRef":"#","url":"/blog/skip-on-plattformpodden","content":"Very recently, SKIP was featured on thePlattformpodden podcast! Vegar and Eline were invited to talk about SKIP, how it came to be and what it's like to work on it. Give it a listen! https://plattformpodden.no/episode/6","keywords":"","version":null},{"title":"Hybrid Kubernetes in production pt. 2","type":0,"sectionRef":"#","url":"/blog/hybrid-kubernetes-in-production-part-2","content":"","keywords":"","version":null},{"title":"Installation and upgrades​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#installation-and-upgrades","content":"   We have been early adopters of Anthos, so when doing the install we did not have options for controlplane architecture. We wanted to use existing underlying VMware infrastructure, so the nodes in our clusters are VMs, provisioned by scripts provided by Google. Our cluster is installed withkubeceptioncontrolplane architechture, this no longer the only, or recommended way. The recommended model is Controlplane V2, where the controlplane nodes for the user cluster are in the user cluster itself.  In the kubeception model, Kubernetes clusters are nested inside other Kubernetes clusters. Specifically, the control plane of the user clusters runs in an admin-cluster. For each on-premise cluster created, a new set of nodes and a namespace are created in the admin cluster.  To install and make changes to the admin cluster, an admin workstation is required, which must be located in the same network as the admin cluster. All configurations are done using a CLI tool called gkectl. This tool handles most cluster administration tasks, and the cluster specific configuration is provided in YAML files.  Our cluster setup is more or less static, and most cluster administration tasks involve upgrading or scaling existing clusters. The SKIP team has a cluster referred to as “sandbox”, which is always the first recipient of potentially breaking changes. After testing in sandbox, we'll deploy changes to both development and test environments, and if nothing breaks, we roll out the changes to our production environment. This is mostly done outside work-hours, although we have not experienced downtime during cluster upgrades. Here is the general workflow for upgrading:  Upgrade your admin workstation to the target version of your upgrade.From your admin workstation, upgrade your user clusters.After all of the user clusters have been upgraded, you can upgrade your admin cluster from the admin workstation.  We have tried using Terraform where possible to simplify the setup. This can not be done in the same way for clusters using the kubeception model. When we migrate to Controlplane V2 however, clusters can be managed via GCP, and we can finally start using terraform for our on-premise cluster config in the same way as for our GKE clusters, and GCP configuration in general.  ","version":null,"tagName":"h2"},{"title":"GCP integration​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#gcp-integration","content":" When working with an on-premise Anthos cluster, some of the nice-to-have features of a standard GKE cluster have been lost. However, recently Anthos on VMware clusters have gradually received more and more features compared to GKE clusters.  ","version":null,"tagName":"h2"},{"title":"IAM and Groups​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#iam-and-groups","content":" Since we were early adaptors of Anthos, we had to endure not being able to delegate clusterroles to IAM groups, and had to add single users to clusterrole/rolebindings in Kubernetes. This was not a huge problem for us, since we were working with a very limited number of teams and devs, but it was apparent that this was not going to scale well. Luckily we got support for groups before it was a problem, and our config files went from containing way too many names and email addresses, to only containing groups.  Our Google Workspace receives groups and users from our Microsoft Active Directory. Groups are initially created either in Entra ID, or on our local Domain Controllers, and at set intervals changes are pushed to Google Workspace.Role-based access control (RBAC) based on membership in these groups was needed. We wanted to manage this through Terraform, and created a repo with where we store and configure our entire IAM configuration. Since we have had growing adoption of Kubernetes and public cloud in our organization, more teams, projects and apps have been onboarded to SKIP, and this IAM repo has grown. We've tried to simplify the structure more than once, but since this is a problem not affecting dev teams, we have chosen to prioritize other tasks.  ","version":null,"tagName":"h3"},{"title":"Workloads​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#workloads","content":" All clusters created in in Anthos can be viewed from the GCP console, and theConnect gatewaymakes it possible to do management from the console (or via kubectl) as well. The GCP console can be used to get information about, or manage the state of the cluster, workloads and resources present. This is a web GUI, part of the GCP console, and not as snappy as cli-tools, but still usable, and intuitive to use.  This view shows workloads running in the argocd namespace. All workloads displayed here can be clicked, and explored further.  When accessing the cluster via the Connect gateway there are some limits. The Connect gateway does not handle persistent connections, and this makes it impossible to do exec, port-forward, proxy or attach. This is not a problem for a production environment, where containers should never be used in this way. But for a dev, or sandbox environment, this is a bit of a pain-point.  This issue should be partially fixed in Kubernetes 1.29 and should be completely resolved in Kubernetes 1.30.  ","version":null,"tagName":"h3"},{"title":"Service Mesh​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#service-mesh","content":" A Service Mesh in Kubernetes is an infrastructure layer that manages communication between services. We are using Anthos Service Mesh (ASM), which is based on Istio and nicely integrated with the GCP console. It's easy to get an overview of services, the connection between them, and what services are connected to either our internal or external gateways. This can be displayed in a Topology view, or if you click on a service, you'll get a more detailed drilldown.  A snippet of services running in our sandbox cluster.  When we deploy services to our cluster we create almost all Kubernetes and service-mesh resources with our custom operator;Skiperator. This operator configures the resources to fit our setup, and applies &quot;best practices&quot; the easy way. This has been one of the great success stories in SKIP, and Skiperator is in continuous development.  ","version":null,"tagName":"h3"},{"title":"Deployment​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#deployment","content":" Deployment is a very interesting subject when it comes to Anthos. As a platform team, it is our job to make sure that deployment is as quick and convenient as possible for the product teams. This ambition has led us to iterate on our processes, which has finally led us to a solution that both we and the developers enjoy using.  ","version":null,"tagName":"h2"},{"title":"Iteration 1 - Terraform​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#iteration-1---terraform","content":" When we first started out with Anthos, we had a very manual process for deploying applications. A service account was provisioned in GCP, which allowed the developers to impersonate a service account in Kubernetes, which in turn allowed them to deploy apps using Terraform. This approach worked, but had a decent amount of rough edges, and also would fail in ways that was hard to debug.  With this approach the developers would have to manage their own Terraform files, which most of the time was not within their area of expertise. And while SKIP was able to build modules and tools to make this easier, it was still a complex system that was hard to understand. Observability and discoverability was also an issue.  Because of this we would consistently get feedback that this way of deploying was too complicated and slow, in addition handling Terraform state was a pain. As a platform team we're committed to our teams' well being, so we took this seriously and looked at alternatives. This was around the time we adopted Anthos, so thus Anthos Config Managment was a natural choice.  ","version":null,"tagName":"h3"},{"title":"Iteration 2 - Anthos Config Managment (ACM)​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#iteration-2---anthos-config-managment-acm","content":"   ACM is a set of tools that allows you to declaratively manage your Kubernetes resources. Here we're mostly going to talk about Config Sync, which is aGitOps system for Kubernetes.  In a GitOps system, a team will have a Git repository that contains all the Kubernetes resources that they want to deploy. This repository is then synced to the Kubernetes cluster, and the resources are applied.  This can be likened to a pull-based system, where the GitOps tool (Config sync) watches the repo for changes and pulls them into the cluster. This is in contrast to a push-based system, where a script pushes the changes to a cluster. It is therefore a dedicated system for deployment to Kubernetes, and following the UNIX philosophywhich focuses on doing that one thing well.  Using this type of a workflow solves a lot of the issues around the Terraform based deployment that we had in the previous iteration. No longer do developers need to set up a complicated integration with GCP service accounts and impersonation, committing a file to a Git repo will trigger a deployment. The Git repo and the manifests in them also works as a state of truth for the cluster, instead of having to reverse engineer what was deployed based on terraform diffs and state.    It started well, however we soon ran into issues. The system would often take a long time to reconcile the sync, and during the sync we would not have any visibility into what was happening. This was not a deal breaker, but at the same time this was not a particularly good developer experience.  We also ran into issues with implementing a level of self-service that we were satisfied with. We wanted to give the developers the ability to provision their own namespaces, but due to the multi-tenant nature of our clusters we also had to make sure that teams were not able to write to each others' namespaces. This was not a feature we were able to implement, but luckily our next iteration had this built in, and we'll get back to that.  The final nail was the user interface. We simply expected more from a deployment system than what ACM was able to provide. The only view into the deployment was a long list of resources, which to a developer that is not an expert in Kubernetes, was not intuitive enough.  ","version":null,"tagName":"h3"},{"title":"Final iteration - Argo CD​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#final-iteration---argo-cd","content":"   This finally brought us to our current iteration. We had heard about Argo CD before, but initially we were hesitant to add another system to our stack. After ACM had introduced us to GitOps and we looked deeper into Argo CD, it was obvious to us that Argo was more mature and would give our developers a better user experience.  The killer feature here is the UI. Argo CD has an intuitive and user-friendly UI that gives the developers a good overview of what is deployed. Whenever anything fails, it's immediately obvious which resource is failing, and Argo allows you to drill down into the resource to see the details of the failure, logs for deployments, Kubernetes events, etc.    The above photo illustrates this well. Here you can see a project with a number of Skiperator applications. The green checkmarks indicate that the application is synced and the green heart indicates that the application is healthy. A developer can see the underlying &quot;owned&quot; resources that Skiperator creates (such as a deployment, service, etc), and get a look &quot;behind the curtain&quot; to see what is actually deployed. This helps debugging and gives the developers a better insight into what is happening during a deployment.  In terms of multi tenancy, Argo CD has a concept of projects. A project is a set of namespaces that a team has access to, and a team can only use Argo to sync to namespaces that are part of their project. The namespace allowlist can also include wildcards, which sounds small but this solved our self-service issue! With our apps-repo architecture, we would give a team a &quot;prefix&quot; (for example seeiendom-), and that team would then be able to deploy to and create any namespace that started with that prefix. If they tried to deploy to another team's namespace they would be stopped, as they would not have access to that prefix.  The prefix feature allows product teams to create a new directory in their apps repo, which will then be synced to the cluster and deployed as a new namespace. This is a very simple and intuitive workflow for creating short-lived deployments, for example for pull requests, and it has been very well received by the developers.  The apps-repo architecture will be a blog post itself at some point, so I won't go too much into it.  And finally, if you're wondering what disaster recovery of an entire cluster looks like with Argo CD, I leave you with the following video at the end.    ","version":null,"tagName":"h3"},{"title":"Hybrid Mesh​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#hybrid-mesh","content":" A hybrid mesh service mesh configuration is a setup that allows for service networking across different environments. For Kartverket this includes a hybrid cloud environment. The setup involves several steps, including setting up cross-cluster credentials, installing the east-west gateway, enabling endpoint discovery, and configuring certificate authorities. All clusters in a hybrid mesh are registered to the same fleet host project, and istiod in each cluster must be able to communicate with the Kube-API on the opposing clusters.  ASM is as previously mentioned based on Istio, and after some internal discussion we decided to experiment with running vanilla upstream Istio in our GKE clusters running in GCP. Pairing it with ASM in our on-premise clusters worked as expected (after a bit of config), and we are now running upstream Istio in GKE, with ASM on-prem in a multi-cluster setup. We also looked into using managed ASM in our GKE cluster, this was hard for us however, due to it requiring firewall openings on-prem for sources we could not predict.    We have chosen the Multi-Primary on different networksafter reviewing our network topology and configuration. We connect our on-premise network, with the GCP VPC through a VPN connection (using host and service projects). To have a production ready environment, the VPN connection must be configured with redundancy.  We're working towards getting this architecture into production, as this will enable us to seamlessly use GKE clusters in GCP together with our on-premise clusters. The elasticity of cloud infrastructure can be utilized where needed, and we can handle communication between services on different clusters much more smoothly. This has been a bit of a journey to configure, but as a learning experience it has been valuable. Being able to address services seamlessly and communicate with mTLS enabled by default across sites, zones and clusters without developers having to think about it feels a bit like magic.  ","version":null,"tagName":"h2"},{"title":"Monitoring​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#monitoring","content":" ","version":null,"tagName":"h2"},{"title":"Google Cloud Monitoring​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#google-cloud-monitoring","content":"   GKE Enterprise includes an agent that collects metrics from the cluster and sends them to Google Cloud. This is a great feature which makes it relatively easy to get started with metrics and monitoring. However, we have decided not to use the agent, and instead use Grafana and LGTM for metrics and monitoring.  This is mainly due to a couple of challenges:  The amount of metrics that are collected out of the box and sent to GCP contributes a significant part of our total spend. It's not that we have a lot of clusters, but the amount of metrics that are collected out of the box is very high, and Anthos' default setup didn't give us the control we needed to be able to manage it in a good way.  Note that this was before Managed Service for Prometheus was released with more fine grained control over what metrics are collected. It is now the recommended default, which should make metrics collection easier to manage.  Second, while Google Cloud Monitoring has a few nice dashboards ready for Anthos, it feels inconsistent which dashboards work on-premise and which only work in cloud as they are not labeled as such. This is not a big issue, but it's a bit annoying. The bigger issue is that all the dashboards feel sluggish and slow to load. Several of us have used Grafana before, so we're used to a snappy and responsive UI. In our opinion, Google Cloud Monitoring feels clunky in comparison.  So the cost and the user experience were the main reasons we decided to look at alternatives to Google Cloud Monitoring. We ended up using Grafana and LGTM, which we'll talk about next.  ","version":null,"tagName":"h3"},{"title":"Grafana with the LGTM stack​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#grafana-with-the-lgtm-stack","content":"   When we realized that our needs were not entirely met by Google Cloud Monitoring, we started a project to develop a monitoring stack that would meet our needs. Since Grafana is open source and has a large community, we decided to use that as our frontend. Our backend is the LGTM stack, which is a set of open source tools that are designed to work well together for ingesting, storing and querying logs, traces and metrics.  What we noticed immediately was that the product teams were much more engaged with this stack than they were with Google Cloud Monitoring. Previously they would not really look at the dashboards, but now they are using them and even creating their own. This is a huge win for us, as we want the teams to be engaged with the monitoring and observability of their services.  It definitely helps that most developers on the product teams are familiar with Grafana, which makes it easier for them to get started as the learning curve is not as steep.  There was a discussion about what the backend should be, if we should useGrafana Cloud or host it ourselves. There would be a lot of benefits of using the cloud, as we would not have to maintain the stack or worry about performance or storage. There was, however, a concern about cost and whether or not log files could be shipped to a cloud provider. In the end we decided to host it ourselves, mostly because we didn't have control over what quantities of data we're processing. Now that we have a better understanding of our usage we can use that to calculate our spend, so we're not ruling out migrating to Grafana Cloud in the future.  The collection (scraping) of data is done by Grafana Agent, which is an &quot;all-in-one&quot; agent that collects metrics, logs and traces. This means a few less moving parts for the stack, as we don't have to run both Prometheus,Fluent Bit and someOpenTelemetry compatible agent for traces. It's a relatively new project, but it's already relative stable and has a lot of features. It uses a funky format for configuration called river, which is based on Hashicorp's HCL. The config enables forming pipelines to process data before it's forwarded to Loki, Tempo or Mimir. It's a bit different, but it works well and is easy to understand and configure to our needs.    Using a system like Grafana also enables us to build an integrated experience that also includes alerting. Using Grafana alerting and OnCall, we configure alerts that are sent to the correct team based on the service that is failing. This helps the teams get a better overview of what is happening in their services, and also helps us as a platform team to not have to be involved in every alert that is triggered.  Overall we're very happy with the LGTM stack, even though it's a fair bit of work to maintain the stack (especially with Istio and other security measures). We're also happy with Grafana, and we're looking forward to seeing what the future holds for monitoring and observability in Kubernetes.  ","version":null,"tagName":"h3"},{"title":"Summary​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 2","url":"/blog/hybrid-kubernetes-in-production-part-2#summary","content":" To summarize: We like Anthos, and we think it's a great platform for running hybrid Kubernetes. As a platform team we look at each feature on a case-by-case basis, with the goal of giving our developers the best possible experience instead of naively trying to use as much as possible of the platform. Because of this we've decided to use Anthos for Kubernetes and service mesh, but not for config sync and monitoring. This has given us a great platform that we're confident will serve us well for years to come.  Stay tuned for the third and final part of this series, where we'll talk about the benefits we've seen from Anthos, and what we would have done differently if we were to start over.  Disclaimer - Google, GKE and Anthos are trademarks of Google LLC and this website is not endorsed by or affiliated with Google in any way. ","version":null,"tagName":"h2"},{"title":"Hybrid Kubernetes in production pt. 3","type":0,"sectionRef":"#","url":"/blog/hybrid-kubernetes-in-production-part-3","content":"","keywords":"","version":null},{"title":"Do you really need hybrid?​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#do-you-really-need-hybrid","content":" When we started out, there was an assumption that it was simply impossible to use the cloud. This came from all sides of the organization, so this was taken as a given. SKIP was therefore started as a project to build an on-premise Kubernetes platform to service our needs as a transition to cloud native development principles.  As we moved along, a lot of these assumptions got challenged. We found that most of these assumptions were based on misunderstandings or a lack of a deeper understanding of cloud technologies and the surrounding legal aspects. This led to a fear of the unknown, and subsequent inaction. In the end it turned out that quite a lot of our workloads could indeed run in the public cloud, given some minor adjustments.  Had we started out with the knowledge we have now, we would probably have started with a public cloud provider, and then moved to hybrid when and if we saw a need for it. Using a cloud provider's managed Kubernetes offering is significantly easier than running your own, and you can get started much faster, with less risk.  Given our organization, we would probably have ended up with hybrid anyway, but that complexity could potentially have been moved down the timeline to a point where the platform was more mature.  Starting with hybrid is a massive undertaking, and you should have a good reason for doing so. Do you need hybrid, or do you just need to mature your organization? If you do, reduce the scope of the initial work to get to a workable platform, and preferably start in the cloud, adding hybrid features later. If you're not sure, you probably don't need hybrid.  ","version":null,"tagName":"h2"},{"title":"Hybrid gives your organization flexibility​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#hybrid-gives-your-organization-flexibility","content":"   Now that we've built a platform that seamlessly runs workloads in both public cloud and on-premise, we have a lot of flexibility in where we run our workloads and how we manage them. Our experience is that this makes it easier for the organization to mature legacy workloads.  All our greenfield projects are written with cloud native principles in mind, which makes it trivial to run them in the cloud. Legacy workloads, however, are not so lucky. They are often written with a lot of assumptions about the underlying infrastructure and are not cognizant of the resources they use. This means they are a poor fit to lift and shift to the cloud, as they will often be expensive and inefficient.  With a hybrid platform, we can use our on-premise offering as a spring board for modernization. Product teams will start by shifting their app to our on-premise Kubernetes platform, and then gradually modernize it to be cloud native. This method gives a few immediate benefits from the lift and shift like better observability, developer experience and security features but also gives fewer of the drawbacks, as the on-premise cloud is closer to the existing dependencies than a public cloud. Once this is done, smaller chunks kan be rewritten as microservices and moved to the cloud, communicating with the monolith seamlessly over the hybrid network. This is sometimes referred to as the strangler application.  This method significantly reduces the scope of refactoring, as one can focus on gradually rewriting smaller modules instead of rewriting the entire application.  ","version":null,"tagName":"h2"},{"title":"Service mesh is hard, but maybe a necessary evil to make hybrid less painful​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#service-mesh-is-hard-but-maybe-a-necessary-evil-to-make-hybrid-less-painful","content":"   Oh my word how we have struggled with service mesh.  Starting from nothing with a goal of providing a secure-by-default zero-trust network layer with observability and traffic control is quite an undertaking, especially when you pair that with setting up a new kubernetes-based infrastructure from scratch. Istio is famously complex, and we've had our fair share of that.  So how do we feel about Istio? There are various opinions in the team, but if we average them all out, we're content. It's quite complex and can be hard to debug, but it does the job. As we've matured and gotten more experience with Istio, we've also started to see more benefits, like extensions for handling OAuth2and the traffic control features for gradual rollouts which we used for canary-testing the migration of some of our larger applications to SKIP. Not all of these features, like EnvoyFilters, are supported by Anthos Service Mesh (ASM), which is why we're exploring using upstream Istio instead of ASM.  One thing we quickly learned is to not let the product teams configure the service mesh directly using service mesh resources. This is a recipe for disaster. We tried this in the beginning, and first of all it's a huge complexity burden for the product teams. We also started getting a lot of weird issues when product teams would configure the mesh in ways that broke their encapsulation. Since the service mesh is a cluster-wide feature, if one team makes an invalid configuration, it can break other teams' workloads. Kubernetes namespaces be damned. We've therefore moved to a model where the platform team provides an abstraction throughSkiperator which configures the service mesh on their behalf.  Finally, I think it's prudent to ask yourself wether or not you actually need a service mesh. If you're running a small cluster with a few services, you'll probably be fine with using the built-in Kubernetes features like Ingress and Network Policies. The observability features are nice, but you can get most of them with a combination of instrumentation and Grafana.  If you need service mesh then limit the scope until you get comfortable with the mesh, for example start with just mTLS and observability, and then add zero trust networking features later.  Also keep in mind there is a lot of competition in the service mesh space, and there are some interesting alternatives to Istio, likeLinkerd and the up-and-coming Cilium Service Mesh.  ","version":null,"tagName":"h2"},{"title":"Anthos helps you as a platform team getting started with best practices.. Even if you plan to move to open source components later​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#anthos-helps-you-as-a-platform-team-getting-started-with-best-practices-even-if-you-plan-to-move-to-open-source-components-later","content":"   When our platform team started out a few years ago, we picked some of the brightest cloud engineers from within the organization and combined them with some consultants to work on the platform. Most of these engineers had some experience working with Kubernetes and cloud, but not building something of this scale from scratch. The first months would therefore be a learning experience for most of the team.  I think a lot of teams will be in a similar situation, and this is where a managed service like Anthos can be a huge help. Anthos is built with best practices in mind, so a lot of the architecture decisions were built-in to the installer. Choosing a managed offering, even when running on-prem has therefore helped us deliver value to the product teams much quicker than if we had to build everything from scratch.  What's important to point out is that choosing something that is managed does not rule out using open source components later. We started out using all the parts that Anthos gave us, including service mesh, logging, monitoring and configuration management. Managed services do come with some tradeoffs, however, as you lose some of the finer control of the platform. As the team has matured and gained experience, we've started to replace some of these components with open source alternatives, which has helped us save money and gain more control over our platform. This has the downside of having to maintain these components ourselves, but with more experience in the team, this is a tradeoff we feel is worth it.  Even though we're increasingly using more open source components, we don't regret using a paid managed offering in the beginning. It helped us get started and make the right decisions early on, and we're now in a position where we can capitalize on that great start.  ","version":null,"tagName":"h2"},{"title":"Keep in mind autoscaling when choosing licensing models​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#keep-in-mind-autoscaling-when-choosing-licensing-models","content":"   This may be an obvious point to some of the more experienced platform engineers out there, but it was still something that we had to learn. When we started out, we appreciated the simplicity of SaaS products that billed per node, as it made it easy to predict costs. We could simply look at the number of nodes we had running and multiply that with the price per node to get a relatively accurate estimate of what this offering would cost. This would turn out to be a double edged sword, however.  It is safe to assume that one of the reasons people choose Kubernetes is the ability to scale workloads easily. This could be scaling up to handle more traffic, or scaling down to save money. This is a great feature, but as the number of workloads grow, the provisioned nodes will start to become insufficient and new nodes will be provisioned. With Kubernetes and Anthos on VMware this can be done automatically, which is a fantastic feature.  The problem arises when you scale out more nodes and have a static license that bills per node. We've made the mistake of getting contracts with two (now just one) SaaS providers where we order a set of nodes, let's say 10, and when workloads scale up, we end up with more than 10 nodes. This means we're not running that SaaS-service's agents on the new nodes, which can be anything from inconvenient to critical, depending on the service. In the end we've had to restrict our node scaling to avoid this issue, which goes against the whole ethos of Kubernetes. We're also provisioning bigger nodes than we need to avoid scaling out, which can be suboptimal.  We're now working with the vendors to get a more flexible license that bills per node on demand, but this is something to keep in mind when choosing a SaaS offering. Try to factor in the future scaling needs of your platform when purchasing SaaS services.  ","version":null,"tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"Hybrid Kubernetes in production pt. 3","url":"/blog/hybrid-kubernetes-in-production-part-3#summary","content":" To summarize: We've learned a lot on our journey to building a hybrid Kubernetes platform. Over the last few years we've iterated on our platform and learned lots of great lessons. It's been a huge help and privilege to have the support of our organization, especially in terms of us being allowed to fail and learn from our mistakes. The Norwegian saying &quot;it's never too late to turn around&quot; comes to mind, as we've changed course several times on our journey, sometimes to the annoyance of our product teams who depend on a stable platform - but in the end we've ended up with a better product - a platform we can be proud of and that our product teams love using.  Thanks for reading this series on Anthos and hybrid Kubernetes. We hope you've learned something from our experiences, and that our hard earned lessons can help you on your journey to building a hybrid Kubernetes platform.  Disclaimer - Google, GKE and Anthos are trademarks of Google LLC and this website is not endorsed by or affiliated with Google in any way. ","version":null,"tagName":"h2"},{"title":"Velkommen til SKIP! 🎉","type":0,"sectionRef":"#","url":"/docs","content":"Velkommen til SKIP! 🎉 SKIP står for Statens Kartverks Infrastrukturplattform. SKIP uttales som det norske ordet skip, et større sjøgående fartøy. SKIP-teamet jobber med en utviklingsplattform hvor kjernekomponentene er Kubernetes, Google Cloud, Argo CD og GitHub. Hensikten er å ha en helhetlig plattform for moderne utvikling hvor utviklere enkelt skal kunne lage, teste og kjøre containerbaserte applikasjoner basert på Cloud Native-prinsipper på en enkel og sikker måte. I menyen til venstre kan navigere deg frem i dokumentasjonen om plattformen.","keywords":"","version":"Next"},{"title":"🚀 Applikasjon & utrulling","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling","content":"🚀 Applikasjon &amp; utrulling Her kan du lære det du trenger om applikasjoner og utrulling.","keywords":"","version":"Next"},{"title":"🚀 Argo CD","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/argo-cd","content":"","keywords":"","version":"Next"},{"title":"Lenker til Argo​","type":1,"pageTitle":"🚀 Argo CD","url":"/docs/applikasjon-utrulling/argo-cd#lenker-til-argo","content":" Du må være på Kartverkets nettverk eller VPN for å kunne nå disse lenkene.  Dev (argo-dev.kartverket.dev)Prod (argo-prod.kartverket.dev)  ","version":"Next","tagName":"h2"},{"title":"GitOps​","type":1,"pageTitle":"🚀 Argo CD","url":"/docs/applikasjon-utrulling/argo-cd#gitops","content":" Argo CD er et GitOps-verktøy, det vil si at kilden til sannhet ligger i git og synkes inn i clusteret derfra. GitOps er beskrevet i bildet over og er en “Pull-basert” deployment-flyt kontra den tradisjonelle “Push-baserte” deployment-flyten. En operator kjører i clusteret og overvåker kontinuerlig ett eller flere git-repoet og synker yaml-filer inn i clusteret. På den måten kan produktteam forholde seg til noe så enkelt som filer i en mappe i git, og når disse filene endres gjøres en deploy helt automatisk.  GitOps vil gi mange fordeler, men det blir et paradigmeskifte for mange. Istedenfor å tenke “Push”-basert deploy ved å kjøre et skript for å deploye vil man legge inn ønsket state i en fil og så vil systemet jobbe for å bringe clusteret i synk med ønsket state. Denne overgangen kan også sammenlignes litt med imperativ vs. deklarativ programmering, som jQuery vs. React. For de fleste som har jobbet med Kubernetes vil det føles veldig kjent, siden Kubernetes i praksis er en stor reconciliation loop som kontinuerlig driver clusteret mot ønsket state.  Det er mange fordeler med et slikt deployment-system. Når deployment og CI er to distinktive komponenter i systemet blir deployment-systemet mye mer spisset inn mot sin rolle og vil kunne perfeksjonere den, den såkalte “Do one thing and do it well”-tankegangen.  I de neste sidene skal vi beskrive hvordan Argo CD fungerer og hvordan dere kan bruke det til å deploye til SKIP. ","version":"Next","tagName":"h2"},{"title":"SKIP has a tech blog!","type":0,"sectionRef":"#","url":"/blog/welcome","content":"","keywords":"","version":null},{"title":"Like what you see?​","type":1,"pageTitle":"SKIP has a tech blog!","url":"/blog/welcome#like-what-you-see","content":" We're a small team, but we're growing fast. We're also hiring, so if you're interested in working with us, check out our open positions. ","version":null,"tagName":"h2"},{"title":"Scaling with Argo CD: Introducing the Apps Repo Architecture","type":0,"sectionRef":"#","url":"/blog/introducing-apps-repositories","content":"","keywords":"","version":null},{"title":"Multi-tenancy in Argo CD​","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#multi-tenancy-in-argo-cd","content":" So you've deployed Argo CD on your multi-tenant cluster and given your teams access to the user interface. Let's imagine we now have tens of teams and hundreds of applications in the Argo UI. When we start scaling out to more than a handful of users we get into some issues with scale. Examples of these issues can be:  How do you organize your apps and projects?How do you make sure no two teams accidentally (or maliciously) use the same namespace?How can we make sure teams clean up unused deployment resources?How do you seamlessly deploy to multiple clusters?  As a platform team we often find ourselves thinking that everyone loves infrastructure and Kubernetes as much as we do. This is not the case! Most people have not had the joy of having their childhood ruined by installing Linux on their school laptops and configuring WLAN drivers using ndiswrapper. Believe it or not, most people just want tools to get out of their way and let them do their job, be that programming, testing or anything else. Not every team is going to be experts in Kubernetes and Argo. So should we expect all teams to know what a deletion finalizer is? What about the intricacies of serverside apply vs. clientside apply?  It's our responsibility as a platform team to make the user experience of deploying to Kubernetes as user friendly as possible. After implementing an architecture built with UX in mind we've had the joy of seeing people who are extremely skeptical of Kubernetes and the cloud be won over by how easy it is to get your workloads running on Kubernetes. This is thanks to the consistent user experience and built-in best practices of the apps-repo architecture. But we're getting ahead of ourselves, first we need to talk about a few abstractions that make this possible.  ","version":null,"tagName":"h2"},{"title":"What are ApplicationSets?​","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#what-are-applicationsets","content":" In Argo CD there's an advanced feature that allows for automating creation of Argo CD Applications calledApplicationSets. Using an ApplicationSet we can essentially make a template that generates Argo CD applications based on files or folders in a Git repository, sort of like a ReplicaSet for Pods. Using ApplicationSets we can build in features and assumptions and provide the teams with a user experience that essentially boils down to &quot;add a file to a repo and it gets deployed to the cluster&quot;. The purest form of GitOps. No messing around with Argo CD applications and projects.  A core Argo CD component called the ApplicationSet controller will detect anyApplicationSet resources deployed to the cluster and read them. After this, it will periodically scan the a repo configured in the ApplicationSet resource and generate Application resources, which in turn scan a repo for manifest files and sync them to the cluster. So in other words: ApplicationSet -&gt;Application -&gt; Deployments  For this to work you need a Git repo containing manifest files. You could have the teams put these manifest files into their source code repositories, but this is not considered best practice. Usually you would put your manifests into a separate repo so that changes to the manifests don't conflict with changes in the source code. At Kartverket we call this manifest repo an apps repo.  ","version":null,"tagName":"h2"},{"title":"Introducing apps repositories​","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#introducing-apps-repositories","content":"   The apps repo is where the product teams put their manifests. It has a consistent structure and is designed to be read by an Argo CD ApplicationSet. It also has a lot of nifty features that enable self-service which we'll get back to.  First, let's have a look at the structure of an apps repo.  teamname-apps/ env/ clustername/ namespace/ example.yaml   In the simplest of terms, this tree describes where to deploy a given manifest. By using a directory tree it makes setting up an ApplicationSet for this repo trivial.  Consider this example ApplicationSet:  apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: exampleteam-apps namespace: argocd spec: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD goTemplate: true goTemplateOptions: - missingkey=error template: metadata: name: '{{.path.basename}}' spec: destination: namespace: '{{ index .path.segments 2 }}' name: '{{ index .path.segments 1 }}' project: exampleteam source: path: '{{.path.path}}' repoURL: 'https://github.com/kartverket/exampleteam-apps.git' targetRevision: HEAD syncPolicy: syncOptions: - CreateNamespace=true automated: prune: true allowEmpty: true selfHeal: true   With this ApplicationSet any directory within env/*/* will be picked up by the ApplicationSet controller and a new Argo CD Application will be created based on the template in the template object. This enables a product team to create any number of applications for their products.    An example use for this is a product team wanting a namespace for each of their products. Instead of having to order a new namespace from the platform team when they create a new product, they can simply create it themselves by adding a new directory with the same name as the namespace they want. A new Kubernetes namespace will be automatically created thanks to theCreateNamespace=true sync option.  Ephemeral namespaces, aka. preview namespaces, is another usecase. Say a team wants to review a change before merging it to main. They could review the change in the Pull Request, but this removes us from the end user's perspective and is not suitable for non-technical people. With a preview environment the team will automatically create a new directory in the apps repo when a PR is created, and thus get a complete deployment with the change in question. This enables end-to-end testing in a browser, and also allows non-technical people to do QA before a change is merged. When it is merged another workflow can automatically delete the directory, which cleans up and deletes the preview environment.  Our convention is that namespaces are formatted with productname-branch. This allows teams to have multiple deploys per product, and also multiple products per team. So when a new PR is created all a team needs to do to automate the creation of a new directory using CI tools like GitHub actions to create a new commit in the apps-repo. This also enables the flexibility to create it as a PR in the apps-repo, but for ephemeral namespaces, this is usually not necessary.  For example:  footeam-apps/ env/ foo-cluster/ foo-main/ app.yaml foo-feature-123/ app.yaml   ","version":null,"tagName":"h2"},{"title":"Automating and avoiding duplication​","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#automating-and-avoiding-duplication","content":" Depending on the complexity of the apps repo, the amount of products and branches and a subjective &quot;ickyness&quot; with duplicating files (can you spell DRY?), you have several options on how to automate creating new namespaces.  Simple repos will probably be fine with directories containing simple yaml-files that are synced to the cluster. Newer product teams especially appreciate the simplicity of this approach. To optimize for this you may consider using atemplate directory at the base containing some example files that are copied into the sub-directories. A pseudo-coded GitHub action that uses afrontend.yaml template from the templates directory could look like the following:  jobs: build: # Build a container image and push it deploy: strategy: matrix: env: ['dev', 'test', 'prod'] steps: # .. Checkout repo &amp; other setup .. - name: Deploy to ${{ matrix.version }} run: | namespace=&quot;myapp-${{ github.ref_name }}&quot; path=&quot;./env/atkv3-${{ matrix.env }}/$namespace&quot; mkdir -p $path cp -r templates/frontend.yaml $path/frontend.yaml kubectl patch --local \\ -f $path/frontend.yaml \\ -p '{&quot;spec&quot;:{&quot;image&quot;:&quot;${{needs.build.outputs.container_image_tag}}&quot;}}' \\ -o yaml git config --global user.email &quot;github-actions@github.com&quot; git config --global user.name &quot;GitHub Actions&quot; git commit -am &quot;Deploy ${{ matrix.env }} version ${{ github.ref_name }}&quot; git push   This works for most simple apps. Our experience, however, is that as a team matures and gets more experienced with Kubernetes and Argo CD, they add more complexity and want more control. At this point most teams will migrate to usingjsonnet to enable referencing and extending a reusable library shared between multiple components. SKIP also provides some common manifests via ArgoKit, a jsonnet library.  Kustomize is also a common choice, widely used by SKIP for our own infrastructure, but not really widespread with other teams.  Despite Argo supporting Helm we mostly avoid using it to create reusable templates due to the complexity of templating YAML. Jsonnet is superior in this regard.  Fixing indentation errors in YAML templates in a Helm chart pic.twitter.com/Dv2JUkCdiM — memenetes (@memenetes) December 8, 2022  ","version":null,"tagName":"h2"},{"title":"Security considerations​","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#security-considerations","content":" You may be wondering: &quot;This seems great and all, but what about the security implications of allowing teams to create and edit namespaces in a multi-tenant cluster? That seems really dangerous!&quot;.  First of all, I love you for thinking about security. We need more people like you. Second, Argo CD has some great features we can leverage to make this work without removing the self-service nature of the apps repo architecture.  ","version":null,"tagName":"h2"},{"title":"Prefixes​","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#prefixes","content":" In order to make this work we need to give each team a set of prefixes. A prefix will usually be the name of a product that a product team has responsibility for maintaining. The only important part is that it is unique and that no other teams have been allocated the same prefix. At Kartverket this is done by the platform team as part of the team onboarding process.  The prefix is used as part of all namespaces that are created by the teams. In the example namespace product-feature-123, product is the prefix. By giving each team a set of prefixes it helps them separate products into easily identifiable namespaces and it ensures that a product team does not accidentally use another team's namespace.  Since each product team has an apps repo with the ability to name their directories as they wish, how can we enforce this? This is where Argo CD's Projects come into play.  Argo CD Projectsprovide a logical grouping of applications, which is useful when Argo CD is used by multiple teams. It also contains a field that allows allowlisting which clusters and namespaces are usable by a project.  Add the following to a Project to only allow this project to create and sync to namespaces prefixed with myprefix-.  metadata: name: exampleteam spec: destinations: - namespace: 'myprefix-*' server: '*'   If you scroll back up to the ApplicationSet example above, you will see that it only creates applications with the project exampleteam. This will automatically wire any applications created to the destination rules we've defined in this project and therefore deny any attempts by a team to use prefixes that they have not been allocated.  The crucial part here is that ApplicationSets and Projects are provisioned by the platform team, and therefore build in these security features. These resources must not be accessible to the teams, or an attacker can simply add exclusions.  ","version":null,"tagName":"h3"},{"title":"Namespace resources​","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#namespace-resources","content":" Another way this could be abused is if a team is able to create Namespace resources in their apps repository. This should be denied using Argo and/or cluster policies.  If a team is able to create namespace resources (or other cluster scoped resources) in their namespace an attacker can use this to break their namespace &quot;encapsulation&quot;. Imagine for example if one could use their apps repo to sync a namespace resource named kube-system into their env/foo-cluster/foo-maindirectory. Argo CD would allow this, as the manifests are read into an Argo CD application. Then the attacker could delete the namespace and take down the cluster.    It's useful in this multi-tenancy scenario to think of namespaces as resources owned by the platform team and namespace-scoped resources as owned by the product teams. This is considered a best practice, and was reiterated at KubeCon Europe 2024 by Marco De Benedictis. Allowing product teams to edit namespaces can open up a ton of attack vectors, like disabling Pod Security Admissioncontrollers, allowing an attacker to create privileged containers which can compromise the host node.  Friends don't let friends edit namespaces!  ","version":null,"tagName":"h3"},{"title":"Self service customization​","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#self-service-customization","content":" So we set up an ApplicationSet that configures best practices and secure defaults for product teams! Great! But now that team with experienced cloud engineers really wants to customize their Argo configuration. Maybe they want to configure that one app has auto sync on, but another app has it turned off. Maybe they want to disable self-healing for a short period to manually edit in the cluster. In any case, how can we let teams change this configuration self-service when applications are provisioned by theApplicationSet resource?  We could let the teams edit the ApplicationSet. In our case this would mean the teams need to learn about the ApplicationSet abstraction, gotemplate and SKIP's internal GitOps repo structure. This is overkill when a team usually just wants to flip a flag between true or false for a directory. There could also be security implications with allowing teams to edit ApplicationSet resources that could break encapsulation, which we want to avoid.  Another option would be to contact the platform team and tell us to change some config for them. This is not in line with our thinking, as we want the teams to be able to work autonomously for most operations like this. It would also mean we were given a lot of menial tasks which would mean we have less time to do other more meaningful things or become a bottleneck for the teams.  A third option is setting the ApplicationSet sync policy to create-only. This would confifure the ApplicationSet controller to create Application resources, but prevent any further modification, such as deletion, or modification of Application fields. This would allow a team to edit the application in the UI after creation, for example disabling auto sync. This last option is user friendly, but in violation of GitOps principles where config lives in git and not in a database. If you run Argo stateless like we do this would also mean the changes disappear when the pod restarts.  Because none of these options seemed to be the best, we created a better solution. By using a combination of generators and the new template patchfeature in Argo CD 2.8 we can look through every directory in the apps repo for a configuration file called config.json.  Let's look at an example config.json file. This example file is commited in the apps repo to the env/foo-cluster/foo-main directory.  { &quot;tool&quot;: &quot;kustomize&quot;, &quot;autoSync&quot;: false }   This file is not required, but if this file is found the values configured there overrides a set of default values in the ApplicationSet template. These flags are then used to determine how the resulting Application will behave. This means the team is able to change the values they care about per directory of their apps repo  footeam-apps/ env/ foo-cluster/ foo-main/ config.json app.yaml foo-feature-123/ config.json app.yaml foo-feature-with-default-config/ app.yaml   Additionaly, since the platform team is in control of the template we can eliminate the ability to maliciously change the template by parsing the inputs in a secure way.  ","version":null,"tagName":"h2"},{"title":"Example ApplicationSet​","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#example-applicationset","content":" Let's look at how we can write an ApplicationSet that allows us to useconfig.json files.  First, we need to configure the ApplicationSet to look through all directories, and at the same time use a config.json file if it is found. This is perhaps the least intuitive part of this new ApplicationSet, so let's walk through it step by step.  First we create a merge generator, which will merge two generators. The key thing here is that it only merges if the key matches in both generators, so this allows us to first find all directories (the default), then directories that contain config.json files (the override).   generators: - merge: generators: - # default - # override mergeKeys: - key   Now we're going to add the generator from before into the default. The only difference is we're doing this using a matrix generator. Doing this combines the parameters generated by the two child generators, which gives us the values from the git generator like before, but also a set of default values we can use in our template later if the config.json file is not provided.  We're also using a value from the git generator to assign a key that will uniquely identify this directory for the merge generator later.   generators: - merge: generators: - matrix: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory - # override mergeKeys: - key   Now we use a variant of the git generator to find all config.json files in the same repo and extract the values from it. Again we're using the key field to uniquely identify this directory so that it will be merged with the correct directory in the merge generator.  We're repeating the default values here as well, since not all fields are required and we don't want them to be overwritten as null in the resulting merge.   generators: - merge: generators: - matrix: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory - matrix: generators: - git: files: - path: env/*/*/config.json repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory mergeKeys: - key   That's it for the generator! Now we can use these variables in thetemplatePatch field (and other fields). In this case we want to set syncPolicy options, so we need to use the templatePatch, as gotemplates don't work for objects.  We're also adding a special case where for directory sources (the default) we exclude config.json files, as we don't want to sync the config file with Argo. This allows us to extend it later to add options for other tools like Kustomize or Helm.  Keep in mind that we don't want users to inject maliciously formed patches, so we cast booleans to booleans.   templatePatch: | spec: source: directory: {{- if eq .tool &quot;directory&quot; }} exclude: config.json {{- end }} {{- if .autoSync }} syncPolicy: automated: allowEmpty: {{ .allowEmpty | toJson }} prune: {{ .prune | toJson }} selfHeal: {{ .selfHeal | toJson }} {{- end }}   ","version":null,"tagName":"h3"},{"title":"Complete ApplicationSet​","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#complete-applicationset","content":" Here is a complete ApplicationSet containing all the features we've discussed so far.  apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: exampleteam-apps namespace: argocd spec: generators: - merge: generators: - matrix: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory - matrix: generators: - git: files: - path: env/*/*/config.json repoURL: https://github.com/kartverket/exampleteam-apps.git revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory mergeKeys: - key goTemplate: true goTemplateOptions: - missingkey=error template: metadata: name: '{{.path.basenameNormalized}}' spec: destination: namespace: '{{ index .path.segments 2 }}' name: '{{ index .path.segments 1 }}' project: exampleteam source: path: '{{.path.path}}' repoURL: 'https://github.com/kartverket/exampleteam-apps.git' targetRevision: HEAD syncPolicy: managedNamespaceMetadata: labels: app.kubernetes.io/managed-by: argocd pod-security.kubernetes.io/audit: restricted team: exampleteam syncOptions: - CreateNamespace=true - ServerSideApply=true - PrunePropagationPolicy=background templatePatch: | spec: source: directory: {{- if eq .tool &quot;directory&quot; }} exclude: config.json {{- end }} {{- if .autoSync }} syncPolicy: automated: allowEmpty: {{ .allowEmpty | toJson }} prune: {{ .prune | toJson }} selfHeal: {{ .selfHeal | toJson }} {{- end }}   ","version":null,"tagName":"h2"},{"title":"Results​","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#results","content":" With Argo CD and the apps repo architecture, we've seen some real improvements in our deploy system. Teams find it to be incredibly intuitive to just update a file in Git and have it be instantly reflected in Argo CD and Kubernetes, especially when combined with Argo CD auto-sync.  Onboarding new teams is quick and easy, since just putting files into a Git repo is something most developers are already familiar with. We just show them the structure of the apps repo and they're good to go. A team can go from not having any experience with Kubernetes to deploying their first application in a matter of minutes.  Migrating from one cluster to another is also a breeze. Just move manifests from one directory under env to another, and the ApplicationSet will take care of the rest. This is especially useful for teams that want to start developing with new cloud native principles on-premises, modernizing the application and eventually moving to the cloud.  I feel the key part of this architecture is the config.json file. It allows a degree of customization that is not possible with the default ApplicationSettemplate and was to us the last missing piece. It allows teams to change configuration without needing to know about the ApplicationSet abstraction, and it allows the platform team to enforce security and best practices.  ","version":null,"tagName":"h2"},{"title":"Tradeoffs​","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#tradeoffs","content":" But of course, there are some drawbacks. Like always, it's tradeoffs all the way down.  Since a product team uses an apps repo to organize their apps, moving apps from one team to another will require migrating files from one repo to another. This will require some manual work to prevent Argo deleting the entire namespace when the directory is removed from the old repo. Usually this is not a big issue, and moving projects between teams happens very rarely, but it's something to keep in mind.  There is also a risk that a team could accidentally delete a namespace by removing a directory in the apps repo. We have mitigated this by disabling auto-sync for most mission critical applications in production.  And finally, projects that don't have clear ownership or shared ownership can be tricky to place into a repo. You could make an apps repo for a &quot;pseudo-team&quot; consisting of the teams that need access, but generally we find that it's better that all products have a clear singular main owner. This also preventsdiffusion of responsibility.  ","version":null,"tagName":"h3"},{"title":"Thank you for reading!​","type":1,"pageTitle":"Scaling with Argo CD: Introducing the Apps Repo Architecture","url":"/blog/introducing-apps-repositories#thank-you-for-reading","content":" We hope you found this article helpful and informative. Getting intoApplicationSets can be a bit tricky, so we hope we managed to convey the most important parts in a clear and understandable way. Thanks for reading!  We recently created a Mastodon account @kv_plattform! If you want to contact us or discuss this article, feel free to reach out to us there. ","version":null,"tagName":"h2"},{"title":"Hente hemmeligheter fra hemmelighetshvelv","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv","content":"","keywords":"","version":"Next"},{"title":"Hvordan bruke External Secrets​","type":1,"pageTitle":"Hente hemmeligheter fra hemmelighetshvelv","url":"/docs/applikasjon-utrulling/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv#hvordan-bruke-external-secrets","content":" ESO lytter i clusteret etter ExternalSecret - og SecretStore -manifester. I det øyeblikket disse blir plukket opp blir de lest som konfigurasjon for ESO og en synk mot hvelvet starter som vil ende opp med å opprette en Kubernetes Secret. Kubernetes Secreten vil også synkroniseres regelmessig slik at man kan f.eks. rullere hemmeligheter ved å endre dem i hvelvet.  ","version":"Next","tagName":"h2"},{"title":"SecretStore​","type":1,"pageTitle":"Hente hemmeligheter fra hemmelighetshvelv","url":"/docs/applikasjon-utrulling/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv#secretstore","content":" SecretStore-manifestet definerer et hvelv, slik som Vault eller GSM, og må settes opp først. Denne konfigurasjonen vil også inneholde hvordan ESO skal autentisere seg og kan gjenbrukes av flere ExternalSecret-manifester. Disse settes typisk opp av et produktteam for deres namespace for å definere hvor de har lagret sine hemmeligheter.  Se GCPSMProvider for alle gyldige verdier.  apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: gsm spec: provider: gcpsm: projectID: &lt;YOUR_PROJECT_ID&gt;   For at det skal være lov å hente ut secrets må i tillegg følgende gjøres:  Man må gå inn på secreten som skal eksponeres til ESO og gi rollen roles/secretmanager.secretAccessor til servicekontoen: Dev - eso-secret-accessor@skip-dev-7d22.iam.gserviceaccount.comTest - eso-secret-accessor@skip-test-b6e5.iam.gserviceaccount.comProd - eso-secret-accessor@skip-prod-bda1.iam.gserviceaccount.com Namespacene dere oppretter må allowlistes for å kunne hente ut fra prosjektene deres, kontakt SKIP så setter vi skip.kartverket.no/gcpProject på prosjektene deres og synkroniserer Argo på nytt  ","version":"Next","tagName":"h2"},{"title":"ExternalSecret​","type":1,"pageTitle":"Hente hemmeligheter fra hemmelighetshvelv","url":"/docs/applikasjon-utrulling/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv#externalsecret","content":" Når man har definert et hemmelighetshvelv med SecretStore kan man definere hvilke hemmeligheter som skal hentes ut. Dette gjøres med ExternalSecret-manifestet. ExternalSecret-manifestet vil referere til et SecretStore for å definere backenden og bruker autentiseringen derfra. ESO vil bruke dette manifestet til å hente ut de definerte feltene fra den gitte hemmeligheten og putte dem inn i en Kubernetes Secret i det formatet som blir spesifisert. Det betyr at man kan mappe om verdier fra et felt til et annet, for eksempel om man skal uppercase navnene når man bruke dem som miljøvariabler.  I eksempelet under vises hvordan man synker inn enkeltverdier til Kubernetes. Det er også mulig å synke alle nøklene i en secret som dokumentert i All keys, One secret .  Det er også mulig å bruke templates som dokumentert i Advanced Templating .  Se ExternalSecret for alle gyldige verdier.  apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: dbpass spec: # A list of the remote secrets to sync data: - remoteRef: # The name of the secret in the GCP project key: db-pass # Will be written into the Kubernetes secret under this key secretKey: DB_PASSWORD # Refresh the secret every hour refreshInterval: 1h # Uses the gsm secret backend secretStoreRef: kind: SecretStore name: gsm # Creates a kubernetes secret named dbpass target: name: dbpass   Se også Get all keys from one GSM secret  ","version":"Next","tagName":"h3"},{"title":"Mounting av hemmelighet​","type":1,"pageTitle":"Hente hemmeligheter fra hemmelighetshvelv","url":"/docs/applikasjon-utrulling/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv#mounting-av-hemmelighet","content":" Når ESO har synkronisert inn hemmeligheten og opprettet en Kubernetes Secret er det ofte slik at man ønsker å bruke dette i en Pod. Vanligvis gjennom å mounte dette som miljøvariabler eller som en fil på filsystemet, eksempelvis for sertfikater. Bruker man Skiperator er dette veldig rett frem.  Se også Using Secrets as files from a Pod og Using Secrets as environment variables , men merk at spec er annerledes med Skiperator.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: teamname-frontend spec: # Each key will be set as an env var with its value as the value envFrom: - secret: dbpass # Each key will be created as a file with the key as filename and value as content filesFrom: - secret: dbpass mountPath: /var/run/secret   ","version":"Next","tagName":"h3"},{"title":"Hva hindrer andre å hente min hemmelighet?​","type":1,"pageTitle":"Hente hemmeligheter fra hemmelighetshvelv","url":"/docs/applikasjon-utrulling/argo-cd/hente-hemmeligheter-fra-hemmelighetsvelv#hva-hindrer-andre-å-hente-min-hemmelighet","content":" Med External Secrets gis en sentral servicekonto tilgang til å hente ut hemmelighetene i GSM. Man skulle derfor tro at det var mulig for andre som bruker den samme servicekontoen å hente ut hemmeligheten. Det er ikke tilfellet og er løst med andre policies i clusteret.  Ditt team oppretter en SecretStore, og det finnes policies i clusteret som sørger for at kun prosjekter som dere eier kan knyttes opp her. SecretStore-en er det som brukes for å hente fra GCP. Dermed er det kun prosjektet som ligger her som kan hentes fra, og kun ditt team som kan hente fra ditt prosjekt. ","version":"Next","tagName":"h3"},{"title":"Configuring apps repositories with config.json","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/argo-cd/configuring-apps-repositories-with-configjson","content":"","keywords":"","version":"Next"},{"title":"Supported options​","type":1,"pageTitle":"Configuring apps repositories with config.json","url":"/docs/applikasjon-utrulling/argo-cd/configuring-apps-repositories-with-configjson#supported-options","content":" Key\tType\tDescriptiontool (required)\tdirectory / kustomize / helm\tWhich tool should Argo CD use to sync this directory? The “Directory” option supports yaml and jsonnet files. See also tools . autoSync\tboolean ( true / false )\tWhen set to true , the directory is automatically synced when changes are detected. The default value is true in dev and false in prod. prune\tboolean ( true / false )\tWhen enabled, Argo CD will automatically remove resouces that are no longer present in Git. Default is true . See prune . Only used when autoSync is true allowEmpty\tboolean ( true / false )\tSafety mechanism. When prune is enabled it deletes resources automatically, but it will not allow empty syncs (delete all) unless allowEmpty also is enabled. Default is false . See allowEmpty . Only used when autoSync is true selfHeal\tboolean ( true / false )\tWhen changes are made on the cluster directly, Argo will not revert them unless selfHeal is provided. Default is true . See self heal . Only used when autoSync is true ","version":"Next","tagName":"h2"},{"title":"ArgoCD Notifications","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/argo-cd/argocd-notifications","content":"","keywords":"","version":"Next"},{"title":"Slack​","type":1,"pageTitle":"ArgoCD Notifications","url":"/docs/applikasjon-utrulling/argo-cd/argocd-notifications#slack","content":" For Slack er det satt opp en notifikasjonskanal for hvert team på mønster &lt;teamnavn&gt;-argocd-alerts, f.eks. #nrl-argocd-alerts. Disse kanalene er videre satt opp med integration mot Slack-appen “ArgoCD Notifications” som tar imot meldinger fra ArgoCD og dytter de inn i korrekt kanal.  (NB: Hvis du ikke finner en slik kanal for teamet ditt, kontakt en administrator for Kartverkets Slack og be om å få opprettet en kanal med korrekt navnemønster og integrasjon mot “ArgoCD Notifications”).    ","version":"Next","tagName":"h2"},{"title":"Github​","type":1,"pageTitle":"ArgoCD Notifications","url":"/docs/applikasjon-utrulling/argo-cd/argocd-notifications#github","content":" For Github er det satt opp en app kalt “KV ArgoCD Notifications” som har mulighet til å skrive til Github workflow statuser til de forskjellige apps-repoene. Kontakt en av Kartverkets Github-administratorer dersom flere apps-repoer skal legges til her.  Eksempler på notifikasjoner:  ","version":"Next","tagName":"h3"},{"title":"Standardnotifikasjoner​","type":1,"pageTitle":"ArgoCD Notifications","url":"/docs/applikasjon-utrulling/argo-cd/argocd-notifications#standardnotifikasjoner","content":" Følgende triggers er lagt til som standard for alle apps-repoer:  Trigger\tKommunikasjonskanal\tNår trigges denne?notifications.argoproj.io/subscribe.on-sync-failed.slack\tSlack\tSynkronisering av applikasjon feilet notifications.argoproj.io/subscribe.on-sync-failed.github\tGithub\tSynkronisering av applikasjon feilet notifications.argoproj.io/subscribe.on-sync-succeeded.github\tGithub\tSynkronisering av applikasjon gikk bra notifications.argoproj.io/subscribe.on-sync-running.github\tGithub\tSynkronisering av applikasjon kjører notifications.argoproj.io/subscribe.on-health-degraded.github\tGithub\tHelsesjekk av applikasjonen returnerer et “degraded”-resultat notifications.argoproj.io/subscribe.on-sync-status-unknown.github\tGithub\tUkjent synkroniseringsstatus notifications.argoproj.io/subscribe.on-deployed.github\tGithub\tNy versjon av applikasjonen deployet til miljø notifications.argoproj.io/subscribe.on-outofsync-one-day.slack\tSlack\tApplikasjonen har status OutOfSync i minst en dag (det har blitt sjekket inn endringer i apps-repoet som ikke har blitt deployet) notifications.argoproj.io/subscribe.on-outofsync-one-week.slack\tSlack\tApplikasjonen har status OutOfSync i minst en uke (det har blitt sjekket inn endringer i apps-repoet som ikke har blitt deployet)  ","version":"Next","tagName":"h3"},{"title":"Ekstra triggers​","type":1,"pageTitle":"ArgoCD Notifications","url":"/docs/applikasjon-utrulling/argo-cd/argocd-notifications#ekstra-triggers","content":" I tillegg er det mulig å spesifisere andre triggers (så lenge disse er lagt inn i ArgoCD) per team i objektet triggerSubscriptions i https://github.com/kartverket/skip-apps/blob/main/lib/argocd/argocd.libsonnet .  info Husk å spesifisere om det er slack eller github notifikasjon man ønsker ved å legge til suffikset .slack eller .github på slutten av trigger, og husk å spesifisere kanalnavn ved bruk av slack notifikasjon  { name: 'teamnavn', oidcGroup: 'aabbbcc-123-321-ccbbbaa', allowlistedPrefixes: [{ name: 'teamnavn' }], triggerSubscriptions: { 'notifications.argoproj.io/subscribe.on-sync-succeeded.slack': 'navn-paa-slack-kanal', 'notifications.argoproj.io/subscribe.eksempel-trigger.github': '', # denne er blank siden det ikke er en kanal å sende til på github } },  ","version":"Next","tagName":"h3"},{"title":"Komme i gang med Argo CD","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/argo-cd/komme-i-gang-med-argocd","content":"","keywords":"","version":"Next"},{"title":"Sjekkliste​","type":1,"pageTitle":"Komme i gang med Argo CD","url":"/docs/applikasjon-utrulling/argo-cd/komme-i-gang-med-argocd#sjekkliste","content":" For å starte med Argo CD må du gjøre følgende:  Sørg for at teamet ditt oppfyller Hva skal til for å bruke SKIP?Produktteamet deres må ha en team-gruppe i Entra IDDet må settes opp et apps-repo Les Hva er et apps-repo for å forstå hvordan apps-repoer fungererRepoet opprettes fra apps-template malenGitHub teamet deres må gis tilgang til apps-repoet som adminSKIP må gi Argo CD-appen på GitHub tilgang slik at Argo kan pulle apps-repoet, dette gjøres gjennom Github IAC repoet Det bestemmes et “prefiks” som dere deployer til Vanligvis er dette navnet på applikasjonen som skal deploye til SKIPDere kan administrere alle Kubernetes namespacer som starter med dette prefikset SKIP må konfigurere Argo til å lese og synkronisere fra apps-repoet SKIP gjør en endring i skip-apps repoet Nå skal du kunne logge inn på Argo CD og se applikasjonen din! 🚀 Du finner lenker til Argo på Argo CDVidere dokumentasjon finnes på Hvordan bruke Argo CD ","version":"Next","tagName":"h2"},{"title":"Hvordan bruke Argo CD","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/argo-cd/hvordan-bruke-argocd","content":"","keywords":"","version":"Next"},{"title":"Applikasjoner​","type":1,"pageTitle":"Hvordan bruke Argo CD","url":"/docs/applikasjon-utrulling/argo-cd/hvordan-bruke-argocd#applikasjoner","content":" Det første man gjør når man skal ta i bruk Argo er å gå til nettsiden og logge inn. Lenkene til nettsiden finner man på Argo CD og alle kan logge inn med kartverket-brukeren sin hvis man er på et team som har fulgt Komme i gang med Argo CD.    Det neste som møter deg er en oversikt over applikasjonene som Argo leser ut, avbildet over. Dersom man ikke sere noen applikasjoner her, sjekk om dere har fulgt alle stegene i Komme i gang med Argo CD og at dere har manifester som er satt opp til å bli synket inn fra apps-repoet deres. Disse prosjektene blir automatisk opprettet basert på mappestrukturen i apps-repoet deres, så det er ingen behov for å opprette eller rydde opp prosjekter manuelt.  Klikk på et av kortene på denne siden og dere vil gå inn i en mer detaljert visning hvor man ser alle ressursene som blir synkronisert.    Dersom man bruker Skiperator og eksponererer en URL via ingresses vil man også kunne se små ikoner som er lenker og om man klikker på dem åpnes applikasjonen i nettleseren.  Det er også et sett med filtere på venstre side som er lurt å bli kjent med, spesielt dersom applikasjonene blir store og vanskelige å se på en skjerm uten å scrolle.  ","version":"Next","tagName":"h2"},{"title":"Sync​","type":1,"pageTitle":"Hvordan bruke Argo CD","url":"/docs/applikasjon-utrulling/argo-cd/hvordan-bruke-argocd#sync","content":"   info Merk at i dev synkroniseres applikasjoner automatisk  På prosjektsiden ser man alle kubernetes-ressurser som er en del av applikasjonen. Legg merke til de små fargede symbolene på hvert kort som sier noe om statusen på ressursen. Hvis de er grønne viser det at den ressursen er “healthy”. Dersom den er rød er det et tegn på at noe er galt med ressursen. Dersom den er gul er den “ute av synk”, og da må man synkronisere applikasjonen.  Bildet over viser hvordan man kan synkronisere ut endringene til kubernetes-miljøet. Sync-knappen i menylinjen lar deg velge hvordan ting skal synkroniseres ut, og man kan til og med gjøre en Selective Sync av kun noen av ressursene. Det vanligste og tryggeste er vel å merke å synkronisere alt med default-innstillingene.  Dersom en synk ikke har fungert vil man se en feilmelding i menylinjen øverst. I det tilfellet kan det være lurt å trykke på “sync status”-knappen øverst for å få en mer detaljert oversikt over hva som har gått galt.  ","version":"Next","tagName":"h2"},{"title":"Rollback​","type":1,"pageTitle":"Hvordan bruke Argo CD","url":"/docs/applikasjon-utrulling/argo-cd/hvordan-bruke-argocd#rollback","content":" I noen tilfeller kan man tenke seg at en uønsket endring er kommet ut i kjøremiljøet. Da vil den raskeste og enkleste måten å gjenopprette funksjonaliteten for brukerene ofte være en rollback til en tidligere kjent fungerende versjon.  Rollbacks er det innebygget støtte for i Argo CD som en del av applikasjonsvisningen. Klikk “History and rollback” for å få en liste over alle tidligere synker som er gjort i denne applikasjonen. Dersom man ønsker å rulle tilbake finner man versjonen man ønsker i listen og trykker på de tre prikkene og velger rollback. “Revisjonene” i listen peker på en commit i git-historikken til apps-repoet.  Ved en rollback gjør Argo CD en synk som vanlig, men mot en tidligere kjent tilstand. Den vil da ikke bruke tilstanden som ligger i git, men tilstanden til en tidligere synk. Etter en rollback vil applikasjonen stå som “out of sync”, og det er forventet siden den ikke matcher tilstanden i git.  info Husk at container imaget må finnes for at det skal være mulig å rulle tilbake. Om container imaget er slettet i ghcr.io , for eksempel av en oppryddingsjobb, så vil det ikke være mulig å starte opp den tidligere versjonen.  ","version":"Next","tagName":"h2"},{"title":"Detaljer og Web Terminal​","type":1,"pageTitle":"Hvordan bruke Argo CD","url":"/docs/applikasjon-utrulling/argo-cd/hvordan-bruke-argocd#detaljer-og-web-terminal","content":"   Dersom man klikker på en ressurs i prosjektvisningen vil man se flere detaljer om denne ressursen. Man finner blant annet en oversikt over metadata, manfiest-filen som Argo CD skal synke ut, events og logger.  Det er også mulig å endre på manifestfilen som ligger i clusteret om man går på “live manifest” og trykker “edit”. Dette vil føre til at applikasjonen kommer ut av synk, og i miljøer hvor auto-synking er skrudd på vil det tilbakestilles med en gang. Men i noen tilfeller kan det være nyttig.    Legg også merke til “terminal”-fanen. Denne er kun synlig om man velger en pod. Velger man denne fanen får man en live terminaltilkobling inn til podden som man kan bruke til feilsøking.  info Web terminal er ikke tilgjengelig i prod  ","version":"Next","tagName":"h2"},{"title":"Hvordan bruke Argo gjennom API​","type":1,"pageTitle":"Hvordan bruke Argo CD","url":"/docs/applikasjon-utrulling/argo-cd/hvordan-bruke-argocd#hvordan-bruke-argo-gjennom-api","content":" Visst du ønsker å automatisere oppgaver, for eksempel synk ved ny image versjon så kan det være greit å ha muligheten til å gjøre dette fra Github. Det første du trengre da er nettverkstilgang fra Github, det får du med tailscale.  For å autentisere mot Argo så må du generere en JWT, dette kan du gjøre i Argo UIet. Gå inn på f.eks , trykk på settings oppe til venstre → Projects → ditt prosjekt → trykk på “Roles” fanen, og deretter på apiuser. Scroll helt ned på modalen som kommer opp og trykk Create under JWT Tokens. Det er samme framgangsmåte i andre miljø.    Etter at token er generert kan du testen den med kommandoen:  curl https://argo-dev.kartverket.dev/api/v1/applications/&lt;min-app&gt; -H &quot;Content-Type: application/json&quot; -H &quot;Authorization: Bearer &lt;token&gt;&quot;   Argos API spec kan man finne her:  ","version":"Next","tagName":"h2"},{"title":"Provisjonere infrastruktur med Crossplane","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/argo-cd/provisjonere-infrastruktur-med-crossplane","content":"","keywords":"","version":"Next"},{"title":"Hvordan komme i gang​","type":1,"pageTitle":"Provisjonere infrastruktur med Crossplane","url":"/docs/applikasjon-utrulling/argo-cd/provisjonere-infrastruktur-med-crossplane#hvordan-komme-i-gang","content":" La oss si vi har en applikasjon som er deployed med Argo CD og vi ønsker å sette opp en database for denne applikasjonen med Cloud SQL. Da vil vi ha en mappestruktur i vårt apps-repo som ser slik ut:  dev/ namespace/ app.yaml # Skiperator-manifest for applikasjonen db.yaml # Crossplane-manifester for databasen på GCP   Det første steget er å få autentisert mot GCP slik at Crossplane får tilgang til å opprette ressurser i prosjektet deres. Dette gjøres ved å kontakte SKIP og få lagt inn mapping for prefikset deres i skip-apps .  Deretter kan man opprette ressurser som er støttet av SKIP dokumentert lenger ned. Crossplane støtter mye mer, se CRD-er i GCP provideren , men det må lages støtte for disse, se “Tilgang til ressurser”.  For å provisjonere opp ressurser oppretter produktteamet manifester på Kubernetes som blir lest av Crossplane. Et eksempel på å opprette lagring (bucket).  apiVersion: skip.kartverket.no/v1alpha1 kind: BucketInstance metadata: name: my-bucket spec: parameters: bucket: name: dsa-test-bucket-123 serviceAccount: name: crossplane-test displayName: Testing Crossplane Integration   Etter dette er lagt ut vil man kunne se status på crossplane ressursene som et hvilket som helst annen kubernetes-ressurs.  kubectl get bucketinstance   Man kan også bruke kubectl describe for å hente ut events på disse ressursene. Events sier mer om hva som skjer og er nyttig til feilsøking.  Mer om feilsøking finnes på https://docs.crossplane.io/knowledge-base/guides/troubleshoot/ .  ","version":"Next","tagName":"h2"},{"title":"Støttede ressurser​","type":1,"pageTitle":"Provisjonere infrastruktur med Crossplane","url":"/docs/applikasjon-utrulling/argo-cd/provisjonere-infrastruktur-med-crossplane#støttede-ressurser","content":" Følgende ressurser er støttet for å provisjoneres med Crossplane i dag:  Buckets (Lagring i Google Cloud Storage)GCP Service AccountsBucket Access (Kubernetes SA to Bucket)Workload Identity (Kubernetes SA to GCP SA)  ","version":"Next","tagName":"h2"},{"title":"Oppsett​","type":1,"pageTitle":"Provisjonere infrastruktur med Crossplane","url":"/docs/applikasjon-utrulling/argo-cd/provisjonere-infrastruktur-med-crossplane#oppsett","content":" For å komme i gang med Crossplane må du gjøre noe setup. Alle produktteam får automatisk opprettet en servicekonto på GCP som vil brukes av Crossplane til å autentisere mot GCP, og for at Crossplane skal få brukt denne må det ligge en secret i namespacet deres. For å få inn denne kan dere opprette en secret ved hjelp av en ExternalSecret (se Hente hemmeligheter fra hemmelighetshvelv) som kopierer hemmeligheten fra Google Secret Manager inn i Kubernetes. Dette må dere sette opp for hvert prefiks i &lt;prefix&gt;-main mappen deres i apps-repoet:  apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: crossplane-secret spec: refreshInterval: 1h secretStoreRef: name: gsm kind: SecretStore target: name: crossplane-secret data: - secretKey: creds remoteRef: conversionStrategy: Default decodingStrategy: None key: crossplane-credentials metadataPolicy: None   SKIP setter automatisk opp en ProviderConfig når man får knyttet sitt prefix i Argo CD mot GCP. Denne forutsetter en secret i -main namespacet deres som heter crossplane-secret . Hvis ikke denne secreten blir plukket opp så hør med SKIP om knytningen til GCP mangler.  For øvrig må vi bruke JSON keys for GCP service kontoer her siden crossplane støtter ikke Workload Identity on-prem.  ","version":"Next","tagName":"h2"},{"title":"Tilgang til ressurser​","type":1,"pageTitle":"Provisjonere infrastruktur med Crossplane","url":"/docs/applikasjon-utrulling/argo-cd/provisjonere-infrastruktur-med-crossplane#tilgang-til-ressurser","content":" I utgangspunktet kan ikke produktteamene få tilgang til crossplane CRD-er direkte ettersom disse ikke er namespaced-ressurser og produktteamene kun har tilgang til å opprette ressurser i sitt eget namespace. Dette betyr at SKIP må opprette såkalte “Compositions” for hver ting som produktteamene skal kunne opprette gjennom Crossplane.  Dersom du som utvikler på et produktteam har et ønske om å f.eks. kunne opprette en database eller provisjonere andre ressurser gjennom Crossplane som ikke allerede er støttet må det bestilles en ny Composition fra SKIP.  For at SKIP skal opprette en ny composition må det lages en XRD og en composition .  Se støttede ressurser over. ","version":"Next","tagName":"h2"},{"title":"🧰 GitHub Actions","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/github-actions","content":"","keywords":"","version":"Next"},{"title":"Generelt​","type":1,"pageTitle":"🧰 GitHub Actions","url":"/docs/applikasjon-utrulling/github-actions#generelt","content":" GitHub actions er GitHubs CI/CD-system. Med dette systemet kan man kjøre bygg som er tett integrert med kodebasen og bruke et økosystem av integrasjoner og ferdiglagde actions via GitHub Marketplace .  Dere kommer til å møte på en del forskjellige verktøy når dere skal deploye til SKIP:  SKIP er kjøremiljøet for containere i Kartverket. Vi regner ikke GitHub som en del av SKIP, men det er en så sentral komponent i å deploye til SKIP-teamet er med å drifte GitHub-organisasjonen til KartverketGitHub Actions som er CI/CD-miljøet for å kjøre jobber som å bygge containere fra kildekode og kjøre terraform plan og applyTerraform som er IaC -verktøyet som lar oss beskrive det ønskede miljøet i kode og eksekverer kommandoer for å modifisere miljøet slik at det blir slik som beskrevetgithub-workflows som er gjenbrukbare jobber man kan bruke i sine pipelines for å gjøre oppsettet lettere. Denne inneholder hovedsakelig den gjenbrukbare jobben “run-terraform”. Denne kan benyttes for å enkelt autentisere seg mot GCP og bruke terraform på en sikker måte.Google Cloud og Google Anthos som er miljøet som kjører Kubernetes -miljøet hvor containerene kjørerskiperator er en operator som gjør det enklere å sette opp en applikasjon som følger best practices. Skiperator definerer en Application custom resource som blir fylt ut av produktteamene og deployet med TerraformNacho SKIP signerer container images med en kryptografisk signatur etter de er bygget  GitHub Actions er et CI-systemet som SKIP legger opp til at alle produktteam skal kunne bruke for å automatisere bygging av Docker-images i tillegg til muligheter for å opprette infrastruktur i skyen ved hjelp av Terraform på en automatisert måte.Actions lages ved å skrive YAML-filer i .github/workflows -mappa i roten av repoet. Man kan også trykke på “Actions” og “New workflow” i GitHub og få opp dialogen over. Der kan man velge fra et eksisterende bibliotek med eksempler på Actions som kan hjelpe med å komme i gang med en action. For eksempel kan man trykke “View all” på “Continous Integration” for å finne eksempler på hvordan man bygger med java eller node.js. DIsse er ofte gode utgangspunkt når man skal sette opp et nytt bygg.  Les https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions for en introduksjon til Actions.  Se https://docs.github.com/en/actions/learn-github-actions/workflow-syntax-for-github-actions for referanse av mulige verdier.  ","version":"Next","tagName":"h2"},{"title":"Lagring av images​","type":1,"pageTitle":"🧰 GitHub Actions","url":"/docs/applikasjon-utrulling/github-actions#lagring-av-images","content":" Det anbefalte måten å publisere images er nå til GitHub Container Registry (ghcr.io). Dette kan gjøres enkelt ved hjelp av GitHub Actions.  Se denne artikkelen for mer informasjon om ghcr: https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry .  Eksempler for publisering av container images til GitHub finnes her .  Dersom dere bruker metoden over vil dere merke at dere ikke trenger å sette tags på docker imaget dere bygger. Dette vil settes automatisk basert på en “sane default” ut i fra hvilke branch man er på og hvilke kontekst bygget gjøres i (commit, PR, tag). De resulterende taggene er dokumentert her . Tags kan også tilpasses om ikke default er passende for prosjektet.  Resultatet blir å finne på GitHub repositoriet til koden og ser slik ut:    Det er anbefalt å kjøre skannere på images som bygges før de deployes. Da vil sårbarheter kunne vises i Utviklerportalen. Se Pharos for å komme i gang med kodeskanning.  ","version":"Next","tagName":"h2"},{"title":"Deployment​","type":1,"pageTitle":"🧰 GitHub Actions","url":"/docs/applikasjon-utrulling/github-actions#deployment","content":" For deployment brukes Argo CD som det dedikert deployment-verktøy. Se Argo CD for mer informasjon om hvordan man tar i bruk dette.  Det vil finnes prosjekter som bruker Terraform, enten fordi de hadde oppstart før Argo CD eller fordi de har spesielle behov som tilsier at de trenger Terraform. Disse prosjektene kan se på Bruk av Terraform for videre dokumentasjon. For nye prosjekter anbefaler vi Argo CD. ","version":"Next","tagName":"h2"},{"title":"Autentisering med Workload Identity Federation","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/github-actions/autentisering-med-workload-identity-federation","content":"","keywords":"","version":"Next"},{"title":"Oppsett av GitHub Action​","type":1,"pageTitle":"Autentisering med Workload Identity Federation","url":"/docs/applikasjon-utrulling/github-actions/autentisering-med-workload-identity-federation#oppsett-av-github-action","content":" Når man skal sette opp autentisering mot GCP med Workload Identity Federation er det en fordel å ha lest gjennom GitHub sin artikkel om https://docs.github.com/en/actions/deployment/security-hardening-your-deployments/configuring-openid-connect-in-google-cloud-platform#updating-your-github-actions-workflow , og spesifikt kapittelet som heter “Updating your GitHub Actions workflow”. Her beskriver de de to trinnene man må gjøre:  Konfigurere tilgang til å generere ID-tokensBruke https://github.com/google-github-actions/auth actionen til å autentisere mot GCP  SKIP-teamet vil ha konfigurert en workload identity provider og service account som dere kan putte rett inn i provideren over. Disse er ikke hemmelige men vil variere avhengig av miljø man skal deploye mot, så det kan være hensiktsmessig å ha de som variabler, som vist lenger nede.  permissions: contents: read id-token: write jobs: build: runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - id: auth name: Authenticate to GCP uses: google-github-actions/auth@v0 with: workload_identity_provider: projects/your-project-number/locations/global/workloadIdentityPools/your-pool/providers/your-provider service_account: your-account@your-project.iam.gserviceaccount.com project_id: kubernetes-dev-94b9   Eventuelt kan du ha en egen setup-env jobb som lager outputs du kan bruke senere, slik at provider, service account og project id er variabler i stedet for hardkodede strings.  Eksempel:  permissions: contents: read id-token: write env: PROJECT_ID: kubernetes-dev-94b9 SERVICE_ACCOUNT: your-account@your-project.iam.gserviceaccount.com WORKLOAD_IDENTITY_PROVIDER: projects/your-project-number/locations/global/workloadIdentityPools/your-pool/providers/your-provider jobs: setup-env: runs-on: ubuntu-latest outputs: project_id: ${{ steps.set-output.outputs.project_id }} service_account: ${{ steps.set-output.outputs.service_account }} workload_identity_provider: ${{ steps.set-output.outputs.workload_identity_provider }} steps: - name: Set outputs id: set-output run: | echo &quot;project_id=$PROJECT_ID&quot; &gt;&gt; $GITHUB_OUTPUT echo &quot;service_account=$SERVICE_ACCOUNT&quot; &gt;&gt; $GITHUB_OUTPUT echo &quot;workload_identity_provider=$WORKLOAD_IDENTITY_PROVIDER&quot; &gt;&gt; $GITHUB_OUTPUT build: needs: [setup_env] runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - id: auth name: Authenticate to GCP uses: google-github-actions/auth@v0 with: workload_identity_provider: ${{ needs.setup-env.outputs.workload_identity_provider }} service_account: ${{ needs.setup-env.outputs.service_account }} project_id: ${{ needs.setup-env.outputs.project_id }} build-again: needs: [setup_env] runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - id: auth name: Authenticate to GCP uses: google-github-actions/auth@v0 with: workload_identity_provider: ${{ needs.setup-env.outputs.workload_identity_provider }} service_account: ${{ needs.setup-env.outputs.service_account }} project_id: ${{ needs.setup-env.outputs.project_id }}  ","version":"Next","tagName":"h2"},{"title":"Bruk av Terraform","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/github-actions/bruk-av-terraform","content":"","keywords":"","version":"Next"},{"title":"Lagring av state​","type":1,"pageTitle":"Bruk av Terraform","url":"/docs/applikasjon-utrulling/github-actions/bruk-av-terraform#lagring-av-state","content":" Terraform bruker state for å kontrollere og sammenlikne den nåværende konfigurasjonen mot det som kjører, staten må lagres lokalt eller ekstern. På SKIP bruker vi Google Cloud Storage til å lagre state, og oppsettet for dette kan man se under.  State bucket opprettes i repoet https://github.com/kartverket/gcp-service-accounts.  terraform { backend &quot;gcs&quot; { bucket = &quot;terraform_state_foobar_1e8e&quot; prefix = &quot;foobar-frontend&quot; } }   For at backenden over skal kunne nå denne bucketen må service-kontoen den kjører som være autentisert mot Google Cloud med riktige tilganger. Dette gjøres i byggeløypa før Terraform blir kjørt, se avsnittet under for hvordan man autentiserer med Google Cloud som en del av Github Actionen.  ","version":"Next","tagName":"h2"},{"title":"Kjøre Terraform i GitHub Actions​","type":1,"pageTitle":"Bruk av Terraform","url":"/docs/applikasjon-utrulling/github-actions/bruk-av-terraform#kjøre-terraform-i-github-actions","content":" Se https://github.com/kartverket/github-workflows for hvordan man bruker Terraform som en del av GitHub Actions. ","version":"Next","tagName":"h2"},{"title":"Hva er et apps-repo","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/argo-cd/hva-er-et-apps-repo","content":"","keywords":"","version":"Next"},{"title":"Mappestruktur​","type":1,"pageTitle":"Hva er et apps-repo","url":"/docs/applikasjon-utrulling/argo-cd/hva-er-et-apps-repo#mappestruktur","content":" Du vil se at et apps-repo har en predefinert mappestruktur. Den ser omtrent slik ut:  env/ # 1 [cluster]/ # 2 foo-main/ # 3 app.yaml # 4   På env nivå (2) finner man mapper som gjenspeiler hvilket miljø det skal synkroniseres til. Dette er navnet på clusteret, enten atkv3-dev, atvk3-prod, atgcp1-dev eller atgcp1-prod.  På nivå 2 finner man navnet på namespacet som det skal deployes til. Dette må starte med et gitt prefiks, vanligvis produktnavnet (i dette tilfellet heter produktet foo ). Etter prefikset kan man skrive hva man vil, vanligvis navnet på branchen i git som er deployed her. Dette kan være nyttig om man ønsker å deploye en mer stabil main branch deployed i tillegg til å deploye pull requests som testes live før de merges.  Nivå 3, altså innholdet av mappen over, er et sett med en eller flere manifestfiler som beskriver applikasjonen. I eksempelet over vil app.yaml inneholde en Skiperator Application manifest som for eksempel kan se slik ut:  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: foo-frontend spec: image: kartverket/example port: 8080     Når vi putter hele dette eksemplet sammen vil følgende skje:  Produktteamet gjør en endring i apps-repoetArgo CD vil etter kort tid lese apps-repoet og finne den endrede app.yaml filenArgo CD ser at den er plassert i dev og foo-main mappene og oppretter foo-main namespacet på dev-clusteretArgo CD legger Application definisjonen inn i namespacet på KubernetesSkiperator plukker opp endringen i namespacet og bygger ut Kubernetes-definisjonen for en applikasjon som skal kjøre kartverket/example imagetKubernetes puller container imaget og starter podder som kjører applikasjonen  ","version":"Next","tagName":"h2"},{"title":"Gjenbruke konfigurasjon​","type":1,"pageTitle":"Hva er et apps-repo","url":"/docs/applikasjon-utrulling/argo-cd/hva-er-et-apps-repo#gjenbruke-konfigurasjon","content":" Man vil ofte få gjentagende konfigurasjon når man får flere applikasjoner, namespacer og miljøer. Det finnes metoder i Argo CD for å gjøre konfigurasjonen gjenbrukbar, og du vil finne dokumentasjon om disse på Argo CD Tools .  Flere produktteam har løst gjenbruk ved å bruke http://jsonnet.org/ som er støttet ut av boksen med Argo. Man kan se et eksempel av dette på eiet-apps . SKIP jobber med et bibliotek med gjenbrukbare jsonnet-objekter .  Vi på SKIP anbefaler at dere starter med å sjekke inn vanlige YAML-filer mens dere lærer dere systemet. Når dere blir komfortable med Argo kan dere se på alternativene som er beskrevet over, da blir ikke læringskurven brattere enn nødvendig.  ","version":"Next","tagName":"h2"},{"title":"Kildekode-repoer​","type":1,"pageTitle":"Hva er et apps-repo","url":"/docs/applikasjon-utrulling/argo-cd/hva-er-et-apps-repo#kildekode-repoer","content":" Apps-repoer skal ikke inneholde kildekode. Apps-repoer har kun metadata om applikasjonen i form av manifest-filer. Dette kan man også lese om i Best Practices for Argo CD.  Dette gjør at man får et tydelig skille mellom kildekoderepoer og apps-repoer. Kildekoderepoer har ansvaret for å lagre kode, bygge artefakter og container-imager. Apps-repoer beskriver den ønskede staten til applikasjonen på clusteret og Argo jobber mot å bringe clusteret i synk med denne staten. Dette gjør det også enkelt å forholde seg til apps-repoene som en “single source of truth” til applikasjonsstaten på clusteret.  ","version":"Next","tagName":"h2"},{"title":"Deploye automatisk ved push​","type":1,"pageTitle":"Hva er et apps-repo","url":"/docs/applikasjon-utrulling/argo-cd/hva-er-et-apps-repo#deploye-automatisk-ved-push","content":"   Man ønsker ofte å deploye ut nye versjoner av applikasjoner ved push til kildekoderepoer. Hvordan kan man gjøre dette med Argo CD?  Ved hvert push til et kildekoderepo kjøres et bygg for å bygge et byggartefakt og bygge et container image. Så snart dette imaget er pushet til et registry som ghcr.io vil man at dette skal legges ut på clusteret, og da må man oppdatere manifest-filene i apps-repoet. Man kan oppdatere disse filene manuelt for å trigge en synk, men det er også mulig å gjøre dette automatisk som en del av samme pipeline.  Etter imaget er publisert til ghcr.io puller bygget apps-repoet ved å bruke https://github.com/actions/checkout. Deretter endres filene til å inneholde referansen til det nye imaget, og disse filene commites lokalt. Hvordan disse filene endres er opp til produktteamet, men et forslag ligger i Automation from CI Pipelines. Til slutt pushes filene til repoet som vil trigge en synk med de oppdaterte manifestene.  Dette kan også gjøres med en PR istedenfor å pushe rett til apps-repoet om man vil ha en godkjenning før deploy.  For å logge inn på apps-repoet brukes metoden som beskrives i Tilgang til repoer med tokens fra GitHub Actions.  info Dersom man bruker Argo CD til å opprette namespacer for alle branches og pull requests er det viktig å slette branchene når de ikke lenger er i bruk. Det er begrenset med kapasitet på clusterene og å anskaffe hardware, både on-prem og i sky, er ekstremt kostbart. Det holder å slette filene i apps-repoet for å rydde opp, noe som kan gjøres automatisk ved sletting av branches.  ","version":"Next","tagName":"h2"},{"title":"Eksempel på Github Actions​","type":1,"pageTitle":"Hva er et apps-repo","url":"/docs/applikasjon-utrulling/argo-cd/hva-er-et-apps-repo#eksempel-på-github-actions","content":" name: build-and-deploy on: pull_requests: target: - main workflow_dispatch: push: branches: - main env: prefix: prefix jobs: build: # Her bygges et artefakt og et container image pushes til ghcr.io deploy-argo: needs: build runs-on: ubuntu-latest strategy: matrix: env: ['dev', 'test', 'prod'] steps: - uses: octo-sts/action@6177b4481c00308b3839969c3eca88c96a91775f # v1.0.0 id: octo-sts with: scope: kartverket/example-apps identity: example_name - name: Checkout apps repo uses: actions/checkout@v3 with: repository: kartverket/example-apps token: ${{ steps.octo-sts.outputs.token }} - name: Deploy to ${{ matrix.version }} run: | namespace=&quot;${{ env.prefix }}-${{ github.ref_name }}&quot; mkdir -p ./${{ matrix.version }}/$namespace cp -r templates/frontend.yaml ./${{ matrix.version }}/$namespace/frontend.yaml kubectl patch --local \\ -f ./${{ matrix.version }}/$namespace/frontend.yaml \\ -p '{&quot;spec&quot;:{&quot;image&quot;:&quot;${{needs.build.outputs.new_tag}}&quot;}}' \\ -o yaml git config --global user.email &quot;noreply@kartverket.no&quot; git config --global user.name &quot;GitHub Actions&quot; git commit -am &quot;Deploy ${{ matrix.version }} version ${{github.ref_name}}&quot; git push   name: clean-up-deploy on: delete: env: prefix: prefix jobs: delete-deployment: runs-on: ubuntu-latest strategy: matrix: env: ['dev', 'test', 'prod'] steps: - uses: octo-sts/action@6177b4481c00308b3839969c3eca88c96a91775f # v1.0.0 id: octo-sts with: scope: kartverket/example-apps identity: example_name - name: Checkout apps repo uses: actions/checkout@v3 with: repository: kartverket/example-apps token: ${{ steps.octo-sts.outputs.token }} - name: Delete ${{ matrix.version }} deploy run: | namespace=&quot;${{ env.prefix }}-${{ github.ref_name }}&quot; rm -rfv ./${{ matrix.version }}/$namespace git config --global user.email &quot;noreply@kartverket.no&quot; git config --global user.name &quot;GitHub Actions&quot; git commit -am &quot;Delete ${{ matrix.version }} deploy ${{github.ref_name}}&quot; git push  ","version":"Next","tagName":"h3"},{"title":"Dependency review på PR","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/github-actions/dependency-review","content":"Dependency review på PR Dependency review kan brukes for å få oversikt over sårbarheter og lisenser i nye eller oppdaterte avhengigheter. GitHub actions kan brukes for å få beskjed om sårbarheter og lisenser på PR, slik som i eksempelet under. Dette forutsetter at dependency graph er riktig satt opp for økosystemet. Gradle-økosystemet må håndteres på en spesiell måte, som vist i Transitive avhengigheter i Dependency Graph med Gradle. Se også Dependency graph i Sikkerhetshåndboka. name: Review dependencies on: pull_request: branches: - 'main' jobs: dependency-review: permissions: contents: read # Required for reading repository pull-requests: write # Required for dependency review comments runs-on: ubuntu-latest steps: - name: Checkout repository uses: actions/checkout@v4 - name: Perform dependency review uses: actions/dependency-review-action@v4 if: github.event_name == 'pull_request' with: comment-summary-in-pr: always fail-on-severity: moderate Team kan selv konfigurere hvordan dependency review skal fungere. For flere innstillinger, se GitHubs egen dokumentasjon. For bruk sammen med dependency sumbission API-et, se GitHub egne anbefalinger for bruk av dependency submission og dependency review sammen.","keywords":"","version":"Next"},{"title":"Transitive avhengigheter i Dependency Graph med Gradle","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/github-actions/gradle-dependency-graph","content":"Transitive avhengigheter i Dependency Graph med Gradle Dependency Graph brukes for å få oversikt over avhengigheter og sårbarheter. For JVM-prosjekter med Gradle må det settes opp en egen action som sender inn alle transitive avhengigheter. Det kan opprettes en egen action som både sender inn avhengigheter og gjør dependency review som i eksempelet under. Dependency sumbission bør kun kjøres fra en enkelt action per repo. Om dependency submission kjøres fra flere actions vil dette føre til uønsket oppførsel ved håndtering av sårbarheter i Dependency Graph. Se GitHub egne anbefalinger for bruk av dependency submission og dependency review sammen. Merk at oppsett mot private pakkebrønner må gjøres i tillegg (Tailscale eller annet), slik som for vanlige bygg. Java-distribusjon og -versjon må også konfigureres til å tilsvare det som brukes i kodebasen. name: Publiser Gradle-avhengigheter og PR review on: pull_request: push: branches: - 'main' jobs: dependency-submission: permissions: contents: write # Required for submitting dependencies pull-requests: write # Required for dependency review comments in PR runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - uses: actions/setup-java@v4 with: # TODO: Update distribution and version if necessary distribution: temurin java-version: 17 - name: Generate and submit dependency graph uses: gradle/actions/dependency-submission@v4 - name: Perform dependency review uses: actions/dependency-review-action@v4 if: github.event_name == 'pull_request' with: comment-summary-in-pr: always fail-on-severity: moderate Dependency review kan fjernes eller konfigureres slik teamet ønsker det. Se også Dependency review.","keywords":"","version":"Next"},{"title":"Kubectl fra GitHub Actions","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/github-actions/kubectl-fra-github","content":"","keywords":"","version":"Next"},{"title":"Oppsett​","type":1,"pageTitle":"Kubectl fra GitHub Actions","url":"/docs/applikasjon-utrulling/github-actions/kubectl-fra-github#oppsett","content":" Før du kan bruke denne actionen må du gjøre noen endringer i gcp-service-accounts og i ditt teams apps-repo.  ","version":"Next","tagName":"h2"},{"title":"1. Legg til ekstra permissions til deploy service accounten​","type":1,"pageTitle":"Kubectl fra GitHub Actions","url":"/docs/applikasjon-utrulling/github-actions/kubectl-fra-github#1-legg-til-ekstra-permissions-til-deploy-service-accounten","content":" run-kubectl tar i bruk Workload Identity Federation som du kan lese mer om her, men den krever også ekstra tilganger for å kunne koble til clusteret. I gcp-service-accounts har du sannsynligvis definert opp ditt gcp project for å kunne bruke det i GitHub Actions, og dermed fått laget en deploy service account og et workload identity pool. Da må du bare legge til en ekstra rolle i modul-definisjonen slik:  module &quot;utviklerportal&quot; { source = &quot;./project_team&quot; team_name = &quot;utviklerportal&quot; repositories = [ &quot;kartverket/kartverket.dev&quot;, ] env = var.env project_id = var.utviklerportal_project_id kubernetes_project_id = var.kubernetes_project_id extra_kubernetes_sa_roles = [ &quot;roles/container.clusterViewer&quot;, # &lt;--- Legg til denne linjen i extra_kubernetes_sa_roles ] }   Nå skal deploy kontoen kunne koble seg til clusteret.  ","version":"Next","tagName":"h3"},{"title":"2. Legg til role og rolebinding i ditt apps-repo​","type":1,"pageTitle":"Kubectl fra GitHub Actions","url":"/docs/applikasjon-utrulling/github-actions/kubectl-fra-github#2-legg-til-role-og-rolebinding-i-ditt-apps-repo","content":" For at man skal kunne f.eks restarte et deployment, så må vi legge til en kubernetes rbac rolle som gir kontoen tilgang til dette.  I apps repoet, legg til:  kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: deployment-restart-role rules: - apiGroups: [&quot;apps&quot;] resources: [&quot;deployments&quot;] verbs: [&quot;get&quot;, &quot;patch&quot;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: deploy-sa-rolebinding subjects: - kind: User name: your-project-deploy@your-project-id.iam.gserviceaccount.com roleRef: kind: Role name: deployment-restart-role apiGroup: rbac.authorization.k8s.io   navn på service accounten er &quot;modulnavn&quot;-deploy, hvor modulnavn finnes i gcp-service-accounts. du kan også finne den med gcloud config set project &lt;projectid&gt; &amp;&amp; gcloud iam service-accounts list | grep deploy  ","version":"Next","tagName":"h3"},{"title":"3. Legg til GitHub workflow​","type":1,"pageTitle":"Kubectl fra GitHub Actions","url":"/docs/applikasjon-utrulling/github-actions/kubectl-fra-github#3-legg-til-github-workflow","content":" Nå skal alt være konfigurert og du kan legge til en GitHub workflow som kjører run-kubectl workflow.  eksempel:  name: Get pods on: push jobs: sandbox: name: get pods uses: kartverket/github-workflows/.github/workflows/run-kubectl.yaml@v4.2.2 with: cluster_name: atgcp1-sandbox service_account: test-deploy@test-sandbox-5cx6.iam.gserviceaccount.com kubernetes_project_id: kube-sandbox-6e32 project_number: 833464945837 namespace: default commands: | get pods get pods -l app=nginx   Forklaring:  cluster_name: navnet på clusteret du vil koble til, dette kan du finne med gcloud container fleet memberships list. mer herservice_account: navnet på service accounten som skal brukes. denne blir opprettet i gcp-service-accounts, og slutter på -deploykubernetes_project_id: id til prosjektet som clusteret ligger i, finnes med gcloud projects list | grep kubernetesproject_number: nummeret til prosjektet som service accounten ligger i, dette er produkt prosjektet, finnes med gcloud projects list | grep produktcommands: kubectl kommandoene du vil kjøre, uten kubectl foran. husk å bruk multiline string.namespace: namespace du vil kjøre kommandoen ikubectl_version: versjonen av kubectl du vil bruke, default er latest stable. format: v1.30.0 ","version":"Next","tagName":"h3"},{"title":"Tilgang til on-prem infrastruktur fra GitHub Actions","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/github-actions/tilgang-til-on-prem-infrastruktur-fra-github-actions","content":"","keywords":"","version":"Next"},{"title":"Bakgrunn​","type":1,"pageTitle":"Tilgang til on-prem infrastruktur fra GitHub Actions","url":"/docs/applikasjon-utrulling/github-actions/tilgang-til-on-prem-infrastruktur-fra-github-actions#bakgrunn","content":" warning Tailscale i denne konteksten er ment som et hjelpemiddel for å migrere pakker ut til et ekstern pakkeregister, og som et verktøy for å bli kvitt interne avhengigheter. Anbefales ikke for allmenn bruk.  For å understøtte produktteamene med å migrere bort fra intern kode- og artifakthosting, samt avhengigheter på interne databaser har SKIP introdusert Tailscale.  Tailscale er en mesh-basert peer-to-peer VPN-løsning, som du kan lese mer om i deres egen dokumentasjon .  ","version":"Next","tagName":"h2"},{"title":"Komme i gang​","type":1,"pageTitle":"Tilgang til on-prem infrastruktur fra GitHub Actions","url":"/docs/applikasjon-utrulling/github-actions/tilgang-til-on-prem-infrastruktur-fra-github-actions#komme-i-gang","content":" Kontakt en GitHub-administrator for å be om tilgang for ditt repository  Hei $NAVN! Teamet mitt trenger tilgang til å benytte Tailscale på repoet https://github.com/kartverket/mittRepo . Jeg trenger at du granter organisasjonshemmelighetene TS_OAUTH_CLIENT_ID og TS_OAUTH_SECRET (+ tilsvarende for Dependabot org-wide) på repoet, så klarer vi resten selv.  På forhånd takk 🙌  Etter du har fått tilgang til hemmelighetene, legg til følgende i din GitHub workflow  - name: Tailscale uses: tailscale/github-action@v2 with: oauth-client-id: ${{ secrets.TS_OAUTH_CLIENT_ID }} oauth-secret: ${{ secrets.TS_OAUTH_SECRET }} tags: tag:github-runner   Du kan nå benytte deg av utvalgte interne tjenester. Lykke til!  Vil du vite hvilke tjenester du får tilgang til eller behov for flere tjenester enn dagens utvalg? Ta kontakt med SKIP på Slack. ","version":"Next","tagName":"h2"},{"title":"Tilgang til repoer med tokens fra GitHub Actions","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/github-actions/tilgang-til-repoer-med-tokens-fra-github-actions","content":"","keywords":"","version":"Next"},{"title":"Secure Token Service (STS)​","type":1,"pageTitle":"Tilgang til repoer med tokens fra GitHub Actions","url":"/docs/applikasjon-utrulling/github-actions/tilgang-til-repoer-med-tokens-fra-github-actions#secure-token-service-sts","content":" En Secure Token Service (STS) er en tjeneste som utsteder sikkerhetstokener som kan brukes til autentisering og autorisering i ulike systemer og applikasjoner. I vårt tilfelle ønsker vi å utstede kortlevde tokens som kun er gyldige i perioden de brukes som en erstatning for PAT-er. Vi har derfor implementert et verktøy som heter Octo STS for å levere denne funksjonaliteten.  Måten STS fungerer på er at man etablerer tillit mellom to repoer. Dette gjøres ved å legge inn en konfigurasjonsfil i repoet du ønsker å ha tilgang til som sier noe om hvem som skal kunne få tilgang til repoet. Deretter bruker man en ferdig GitHub action i repot som skal få tilgang til å etablere et kortlevd tiken via STS-tjenesten.  Les denne artikkelen for mer detaljer om Octo STS.  ","version":"Next","tagName":"h2"},{"title":"Etablere tillit​","type":1,"pageTitle":"Tilgang til repoer med tokens fra GitHub Actions","url":"/docs/applikasjon-utrulling/github-actions/tilgang-til-repoer-med-tokens-fra-github-actions#etablere-tillit","content":" Først må man etablere tillit ved å legge inn en config-fil i repoet man skal få tilgang til. Dette legges i mappen .github/chainguard/&lt;navn&gt;.sts.yaml . Erstatt &lt;navn&gt; med identiteten som skal ha tilgang og bruk dette navnet i GitHub actionen senere.  Eksempelet under viser hvordan man gir tilgang fra GitHub actions som kjører på repoet kartverket/mittrepo på branchen main .  issuer: https://token.actions.githubusercontent.com subject: repo:kartverket/mittrepo:ref:refs/heads/main permissions: contents: write   Dersom du ønsker å bruke et wildcard til å gi tilgang, for eksempel dersom det deployes ved hjelp av “environments” i GitHub slik at dette blir subjektet ditt kan man bruke et subject_pattern . Dette er et regex.  issuer: https://token.actions.githubusercontent.com subject_pattern: repo:kartverket\\/mittrepo:environment:(sandbox|prod) permissions: contents: write   ","version":"Next","tagName":"h3"},{"title":"Få tilgang​","type":1,"pageTitle":"Tilgang til repoer med tokens fra GitHub Actions","url":"/docs/applikasjon-utrulling/github-actions/tilgang-til-repoer-med-tokens-fra-github-actions#få-tilgang","content":" Når man skal ha tilgang til dette repoet så bruker man en GitHub action til å snakke med STS-tjenesten og få en kortlevd token som brukes på samme måte som en PAT. For en deploy til et apps-repo kan du for eksempel skrive følgende i din GitHub action:  permissions: id-token: write # Required for Octo STS steps: - uses: octo-sts/action@6177b4481c00308b3839969c3eca88c96a91775f # v1.0.0 id: octo-sts with: scope: kartverket/skip-apps identity: utviklerportal - name: Checkout apps repo uses: actions/checkout@v4 with: repository: kartverket/skip-apps token: ${{ steps.octo-sts.outputs.token }}   Når dette blir kjørt vil det bli gjort en spørring til Octo STS-tjenesten, som deretter sjekker filen vi laget i repoet over og om det har blitt etablert tillit. Dersom dette er tilfellet så genereres en token som brukes i dette eksempelet til å sjekke ut et annet repo.  Se også https://github.com/octo-sts/action for dokumentasjon på GitHub actionen. ","version":"Next","tagName":"h3"},{"title":"Vedlikehold av applikasjoner","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/maintenance-of-apps","content":"","keywords":"","version":"Next"},{"title":"Stoppe kjørende applikasjon i ArgoCD​","type":1,"pageTitle":"Vedlikehold av applikasjoner","url":"/docs/applikasjon-utrulling/maintenance-of-apps#stoppe-kjørende-applikasjon-i-argocd","content":" For å kunne stoppe en kjørende applikasjon som er administrert av ArgoCD må man først være sikker på at autosync/self heal er deaktivert for produktteamet som eier applikasjonen. Hvis ikke vil bare applikasjonen spinne opp igjen automatisk.  Se denne filen for å sjekke hva som er status, eventuelt spør noen på SKIP hvis du er usikker. Hvis ikke annet er satt kan du gå ut i fra at autosync er skrudd på i dev og test, men avslått i prod.  For å stoppe en applikasjon trykker du på menyen til en application-ressurs og velger “Stop”. Dette vil midlertidig sette antall kopier til 0 slik at skiperator skalerer ned applikasjonen. Du vil da kunne se at pods forsvinner fra grensesnittet, og “Sync Status” for applikasjonen vil stå som “OutOfSync”Når man er ferdig med vedlikeholdet og ønsker å gjennopprette tidligere konfigurasjon trenger man bare å trykke “Sync” for at applikasjonen skal spinne opp igjen.  ","version":"Next","tagName":"h2"},{"title":"Stoppe kjørende applikasjon manuelt​","type":1,"pageTitle":"Vedlikehold av applikasjoner","url":"/docs/applikasjon-utrulling/maintenance-of-apps#stoppe-kjørende-applikasjon-manuelt","content":" For å stoppe en applikasjon som kjører på SKIP må man i praksis skalere ned antallet kjørende kopier til 0. Den største hindringen ved dette er en policy som vi håndhever i prod-miljøet, som heter “K8sReplicaLimits”. Denne krever at en applikasjon skal ha mellom 2 og 30 kjørende kopier til en hver tid.  For å manuelt stoppe en skiperator-applikasjon er det to ting man må gjøre:  Sette en annotation for å ignorere k8sReplicaLimits policySette antall replicas til 0  Se følgende eksempel på manifest som skalerer til 0  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: some-app annotations: skip.kartverket.no/k8sReplicaLimits: ignore spec: replicas: 0  ","version":"Next","tagName":"h2"},{"title":"Oppsett og bruk av Google Secret Manager","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/oppsett-og-bruk-av-secret-manager","content":"","keywords":"","version":"Next"},{"title":"Hvordan komme i gang?​","type":1,"pageTitle":"Oppsett og bruk av Google Secret Manager","url":"/docs/applikasjon-utrulling/oppsett-og-bruk-av-secret-manager#hvordan-komme-i-gang","content":" GSM fungerer ganske likt Vault. Vault har noe mer funksjonalitet for avansert bruk, men vi bruker for det meste som et KV secret store. For å bruke GSM må det opprettes en secret, og denne secreten må tilgangsstyres.  ","version":"Next","tagName":"h2"},{"title":"Hvordan opprette Secret​","type":1,"pageTitle":"Oppsett og bruk av Google Secret Manager","url":"/docs/applikasjon-utrulling/oppsett-og-bruk-av-secret-manager#hvordan-opprette-secret","content":"   Velg Security under navigasjonsmenyen (de tre strekene ved Google Cloud i høyre hjørne).Velg Secret Manager i venstre kolonne.Hvis API’et ikke er skrudd på, skru på API’et ved å trykke “Enable”Trykk på + CREATE SECRET    Name, og Value kan tenkes på som et Key/Value par. Resten av valgene trenger man ikke gjøre noe med med mindre man har spesielle behov. Noen felter man kan merke seg er:  Replication Policy: Dette er hvor hemmeligheten lagres. Det kan være en fordel å lagre hemmeligheter i flere datacenter for redundans, vi har vanligvis holdt oss i europe-north1.  Encryption: Om det er spesielle behov for å administrere krypteringsnøkkel selv er det også en mulighet. Dette må produktteamene ta ansvar for selv. SKIP teamet administrerer ikke krypteringsnøkler.  ","version":"Next","tagName":"h3"},{"title":"Tilgangsstyring​","type":1,"pageTitle":"Oppsett og bruk av Google Secret Manager","url":"/docs/applikasjon-utrulling/oppsett-og-bruk-av-secret-manager#tilgangsstyring","content":" Når en secret er opprettet, kan man klikke seg inn på den, og velge PERMISSIONS fanen. Man får da opp hvem som har tilgang til denne secreten, og hvilke rettigheter de har.    I de fleste tilfeller vil man bruke External Secret til å hente ut disse hemmelighetene. Det kan gjøres ved å opprette ExternalSecrets-ressurser i Kubernetes som henter ned hemmeligheten til en Kubernetes Secret. Det står mer om dette inkludert tilgangsstyring på Hente hemmeligheter fra hemmelighetshvelv . ","version":"Next","tagName":"h3"},{"title":"Sikkerhetsscanning med Pharos","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/github-actions/pharos","content":"","keywords":"","version":"Next"},{"title":"Eksempler​","type":1,"pageTitle":"Sikkerhetsscanning med Pharos","url":"/docs/applikasjon-utrulling/github-actions/pharos#eksempler","content":" ","version":"Next","tagName":"h2"},{"title":"Kjør skannere en gang i døgnet​","type":1,"pageTitle":"Sikkerhetsscanning med Pharos","url":"/docs/applikasjon-utrulling/github-actions/pharos#kjør-skannere-en-gang-i-døgnet","content":" Scanning en gang i døgnet kan se ut som eksempelet under. Merk at matrix er brukt for å scanne flere images i parallel, men det er ikke nødvendig for repoer med kun ett image:  name: Sikkerhetsscanning av images on: schedule: - cron: '00 5 * * *' # 05:00 UTC each day. env: registry: ghcr.io jobs: pharos: name: Run Pharos runs-on: ubuntu-latest strategy: fail-fast: false # De andre jobbene i matrisen vil kjøre selv om en av dem feiler matrix: package-name: [ 'image1', 'image2', 'image3', 'image4' ] # Permissions påkrevd av pharos permissions: actions: read packages: read contents: read security-events: write steps: - name: Run Pharos uses: kartverket/pharos@v0.2.5 with: trivy_category: ${{ matrix.package-name }} image_url: ${{ env.registry }}/${{ github.repository_owner }}/${{ matrix.package-name }}:latest   ","version":"Next","tagName":"h3"},{"title":"Kjør skannere for hver gang et nytt image bygges​","type":1,"pageTitle":"Sikkerhetsscanning med Pharos","url":"/docs/applikasjon-utrulling/github-actions/pharos#kjør-skannere-for-hver-gang-et-nytt-image-bygges","content":" Det er lurt å skanne images ved hvert push til hovedbranchen. Da må imaget pushes først, og så kan det skannes av Pharos. Det er anbefalt å kjøre scanning for push til hovedbranchen, og ikke ved f.eks. push til tag. Om scanning kjøres ved push til hovebranchen for repoet blir &quot;Code scanning&quot;-oversikten blir intuitiv og resultatene dukker også opp i sikkerhetsmetrikkerverktøyet i Utviklerportalen.  on: push: branches: - main env: image_name: ghcr.io/${{ github.repository }} # ghcr.io/kartverket/repo-name jobs: build: steps: # Most steps skipped for brevity - name: Build and push container image id: build-image uses: docker/build-push-action@v6 with: push: true # Image must be pushed before scanning tags: ${{ env.image_name }}:latest # Remaining properties skipped for brevity outputs: image_digest: ${{ steps.build-image.outputs.image_digest }} run-pharos: name: Run Pharos runs-on: ubuntu-latest # Only run on pushes to default branch if: github.event_name == 'push' &amp;&amp; github.ref_name == github.event.repository.default_branch permissions: actions: read packages: read contents: read security-events: write steps: - name: &quot;Run Pharos&quot; uses: kartverket/pharos@v0.2.5 with: image_url: ${{ env.image_name }}@${{ needs.build.outputs.image_digest }}   ","version":"Next","tagName":"h3"},{"title":"Konfigurasjon av skannere​","type":1,"pageTitle":"Sikkerhetsscanning med Pharos","url":"/docs/applikasjon-utrulling/github-actions/pharos#konfigurasjon-av-skannere","content":" Konfigurasjonsskanning gjøres automatisk om det ikke skrus av. Da vil bl.a. Dockerfile og Terraform-kode skannes. Skanning av container images gjøres automatisk om image_url er spesifisert.  Workflowen vil feile om sårbarheter vurdert som high eller critical finnes. Dette kan konfigureres enten ved å sette allow_serverity_level til high eller critical, eller med å sette disable_serverity_check til false.  For mer informasjon, se på dokumentasjonen for konfigurerbare inputs. ","version":"Next","tagName":"h2"},{"title":"🤖 Skiperator","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/skiperator","content":"🤖 Skiperator Skiperator is an operator intended to make the setup of applications simple from the users' point of view. You can look at it as a replacement of Helm, but with a more Kubernetes-native approach. Skiperator is developed by SKIP for the Kubernetes platform and is based on the Operator SDK, which is a framework that uses the controller-runtime library to make writing operators easier. The operator is designed to be used by application developers to deploy their applications and jobs into a Kubernetes cluster. It will create all the necessary resources for the application to run, such as deployments, services, and ingress resources, and also handle security aspects like setting up network policies and service accounts so you as a developer don't have to worry about it. Logs and metrics will be automatically available on monitoring.kartverket.cloud Skiperator offers three CRDs (Custom Resource Definitions) to make it easy to deploy applications and jobs into a Kubernetes cluster: Application - for deploying applicationsSKIPJob - for running jobs and cron jobsRouting - for setting up routing rules, for example frontend and backend services under the same domain To get started check out the Requirements and Getting started pages.","keywords":"","version":"Next"},{"title":"Sjekkliste før internetteksponering","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/sjekkliste-før-internett-eksponering","content":"Sjekkliste før internetteksponering info Denne siden er under utarbeidelse og er et samarbeid mellom utvikling og sikkerhet For å eksponere en applikasjon som kjører på SKIP mot internett må man: Opprette en DNS-record som ikke er under statkart.no-domenet, f.eks. applikasjonX.kartverket.no . Det gjøres ved å opprette en ticket i PureService og be om at dette domenet skal ha et CNAME som peker mot SKIP-lastbalansereren (atkv3-prod.kartverket.cloud for on-prem og atgcp1-prod.kartverket.cloud for sky)Legge til det nye domenenavnet under ingresses i Skiperator-manifestet eller hostname for Routing-manifestet , slik at applikasjonen registrerer seg mot ekstern ingress gateway Før dette kan gjøres må man gå igjennom denne sjekklisten: Gjør dere kjent med Overordnede føringer og spesielt Ansvarsfordeling fra Sikkerhetshåndboka Opprett metadata i GitHub-repoene tilknyttet applikasjonen i henhold til sikkerhet i repoet. Dette sikrer at applikasjonen blir integrert i Utviklerportalen og får tilgang til sikkerhetsmetrikker. Foranalyse må være gjennomført (det kommer løype for dette i PureService) Det er gjort IP (Innledende Personvernsvurdering) og eventuelt DPIA. Kopier malen IP, DPIA og ROS-analyse for [det som vurderes] til deres område og fyll ut informasjonen der. ROS-analyse gjennomført og godkjent av risikoeier/systemeier Codeowners definert i koderepo CODEOWNERS Gjennomført initiell penetrasjonstesting (hvem og hvordan?) eller manuell avsjekk med SKIP rundt konfigurasjon Følgende headere blir sendt på alle kall: HTTP Strict Transport SecurityContent Security PolicyX-Frame-OptionsX-Content-Type-OptionsReferrer PolicyPermissions Policy Når appen er eksponert skal sikkerhetsheaders testes med https://securityheaders.com og https://observatory.mozilla.org Monitorering og varsling er satt opp i Grafana, og vaktlaget er onboardet disse alarmene Metrics with GrafanaLogs with LokiAlerting with Grafana Denne sjekklisten gjelder eksponering av tjenester som skal være tilgjengelig på internett, uavhengig av miljø (prod/dev). Hvis man har behov for å eksponere en applikasjon eksternt i dev må man i tillegg kontakte SKIP for å sikre at alle sikkerhetskrav overholdes. Navnekonvensjon for eksternt tilgjengelig domenenavn vil i så fall være &lt;applikasjonX&gt;.atkv3-dev.kartverket.cloud","keywords":"","version":"Next"},{"title":"Getting started","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/skiperator/get-started","content":"","keywords":"","version":"Next"},{"title":"Application​","type":1,"pageTitle":"Getting started","url":"/docs/applikasjon-utrulling/skiperator/get-started#application","content":" An Application is our abstraction of a deployment. Skiperator will create all the necessary resources for you. Create a file named app.yaml in env/atkv3-dev/myapp with the following content:  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: myapp spec: image: ghrc.io/kartverket/myapp:latest port: 8080 ingresses: - myapp.atkv3-dev.kartverket-intern.cloud redirectToHTTPS: true accessPolicy: inbound: rules: - application: myjob-skipjob   You can then go into argo search for myappand sync the application. Skiperator will then read the Application resource and create a bunch of resources for you, like a deployment, service, ingress and network policy. Your app should be reachable from the domain myapp.atkv3-dev.kartverket-intern.cloud.  ","version":"Next","tagName":"h2"},{"title":"SKIPJob​","type":1,"pageTitle":"Getting started","url":"/docs/applikasjon-utrulling/skiperator/get-started#skipjob","content":" A SKIPJob is our abstraction of a job or a cron job. Skiperator will create all the necessary resources for you. Create a file named job.yaml in env/atkv3-dev/myjob with the following content:  apiVersion: skiperator.kartverket.no/v1alpha1 kind: SKIPJob metadata: name: myjob spec: container: image: ghrc.io/kartverket/myjob:latest command: - &quot;sleep 10&quot; accessPolicy: outbound: rules: - application: myapp cron: schedule: &quot;* * * * *&quot;   This creates a cron job that executes every minute. It also has an access policy that allows it to connect to myapp. Skiperator will create network policies that allow the SKIPJob to connect to the Application. If you look at the application above you can see that it has an access policy that allows myjob to connect to it. SKIPJobs must be postfixed with -skipjob in the access policy. You can also connect to applications in other namespaces, see more in configuring or the api docs.  ","version":"Next","tagName":"h2"},{"title":"Routing​","type":1,"pageTitle":"Getting started","url":"/docs/applikasjon-utrulling/skiperator/get-started#routing","content":" A Routing is an optional resource that you can use to facilitate path based routing, allowing multiple microservices to share the same hostname. Under the hood it's using Istio to proxy requests based on the http path. By using Routing you should remove the ingresses field in you Application manifest. For example if you have two applications, frontend and backend, you can create a routing rule that routes requests to /api to the backend and everything else to the frontend.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Routing metadata: name: myrouting spec: hostname: kartverket.com routes: - pathPrefix: /api targetApp: backend-app - pathPrefix: / targetApp: frontend-app  ","version":"Next","tagName":"h2"},{"title":"Requirements","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/skiperator/requirements","content":"","keywords":"","version":"Next"},{"title":"Application and job requirements​","type":1,"pageTitle":"Requirements","url":"/docs/applikasjon-utrulling/skiperator/requirements#application-and-job-requirements","content":" First of all your applications and jobs need to be containerized. This means that your application or job needs to be packaged in a linux container image that can be run in a Kubernetes cluster. In SKIP we recommend to use the scratch image as a base image for your application or job. This is a minimal image that only contains the necessary files to run your application or job.  Next the image needs to be hosted in a container registry that is accessible from the Kubernetes cluster. In SKIP we use github as our container registry. It doesn't matter if the image is publicly available or private, as long as the repository is under the Kartverket organization.  ","version":"Next","tagName":"h2"},{"title":"CI CD requirements​","type":1,"pageTitle":"Requirements","url":"/docs/applikasjon-utrulling/skiperator/requirements#ci-cd-requirements","content":" In order to deploy your application you need to have set up a CI/CD pipeline that builds and pushes your container image to the container registry. As previously mentioned, in SKIP we use github as the repository. You can read more about how to set up a CI/CD pipeline in the github actions documentation.  We also need to set up Argo CD for deployment of the application. You can read more about how to set up Argo CD in the Argo CD documentation.  ","version":"Next","tagName":"h2"},{"title":"Summary​","type":1,"pageTitle":"Requirements","url":"/docs/applikasjon-utrulling/skiperator/requirements#summary","content":" So to summarize, in order to use Skiperator and run your applications in SKIP you need to have the following in place:  Your application or job needs to be containerizedThe container image needs to be hosted in a container registry that is accessible from the Kubernetes cluster (github)A CI/CD pipeline that builds and pushes the container image to the container registryArgo CD set up for deployment of the application from a team-apps repository  Now that you have the prerequisites in place you can move on to the Getting started page to learn how to deploy your application or job to SKIP. ","version":"Next","tagName":"h2"},{"title":"🔐 Tilgangsstyring på SKIP","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/tilgangsstyring","content":"","keywords":"","version":"Next"},{"title":"Valg av identitetstilbyder​","type":1,"pageTitle":"🔐 Tilgangsstyring på SKIP","url":"/docs/applikasjon-utrulling/tilgangsstyring#valg-av-identitetstilbyder","content":" Før du konfigurerer klientregistrering, token-validering eller innlogging, bør du vurdere hvilken identitetstilbyder som best passer til behovene i din applikasjon.  Microsoft Entra ID: Brukes når applikasjonen er ment for internt bruk i Kartverket, og brukerne er ansatte i Kartverket. Dette gjelder enten hvis man skal tilby et API for andre interne tjenester eller ansatte i Kartverket, eller hvis man ønsker å konsumere et API som er beskyttet med Entra ID.ID-porten: Egnet for borgertjenester som skal brukes av Ola og Kari Nordmann.Maskinporten: Benyttes når applikasjonen tilbyr et API for andre offentlige virksomheter eller når man ønsker å konsumere andre API-er som benytter seg av Maskinporten. ","version":"Next","tagName":"h2"},{"title":"Common Skiperator configuration","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/skiperator/configuring","content":"","keywords":"","version":"Next"},{"title":"Application​","type":1,"pageTitle":"Common Skiperator configuration","url":"/docs/applikasjon-utrulling/skiperator/configuring#application","content":" ","version":"Next","tagName":"h2"},{"title":"Ingress​","type":1,"pageTitle":"Common Skiperator configuration","url":"/docs/applikasjon-utrulling/skiperator/configuring#ingress","content":" An ingress is a way to expose your application to the outside world. It is a Kubernetes resource that manages external access to services in a cluster, typically HTTP. This sets up all the necessary configuration behind the scenes to route traffic to your application, and also sets up a lets encrypt certificate for your application.  Simple example of an ingress:  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: ingressapp spec: image: image port: 8080 ingresses: - ingressapp.atkv3-dev.kartverket-intern.cloud redirectToHTTPS: true   This sets up an ingress to your application that can be reached from Kartverkets internal network. The redirectToHTTPS field is optional and will redirect all incoming traffic to HTTPS. To make it publicly available you can remove the -intern part of the domain name.  If you want, or already have a different domain name for your application then we most likely need to set up a CNAME record in DNS. You can read more about domain names here.  ","version":"Next","tagName":"h3"},{"title":"Access policy​","type":1,"pageTitle":"Common Skiperator configuration","url":"/docs/applikasjon-utrulling/skiperator/configuring#access-policy","content":" In SKIP we run istio as a service mesh. This means that all traffic between services is encrypted by default. All traffic is also blocked with network policies or istio policies by default. To allow traffic between services you need to set up an access policy. This is done by specifying spec.accessPolicy in your application.  ","version":"Next","tagName":"h3"},{"title":"allowing communication between two applications in the same namespace​","type":1,"pageTitle":"Common Skiperator configuration","url":"/docs/applikasjon-utrulling/skiperator/configuring#allowing-communication-between-two-applications-in-the-same-namespace","content":" creates rules to allow traffic between application app1 and app2 in the same namespace on service ports  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: app1 spec: image: image port: 8080 accessPolicy: inbound: rules: - application: app2 outbound: rules: - application: app2 --- apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: app2 spec: image: image port: 8080 accessPolicy: inbound: rules: - application: app1 outbound: rules: - application: app1   allowing in and outbound traffic to an application in a different namespace​  creates network policy rules to allow inbound and outbound traffic on service port to application app2 in namespace namespace2  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: app1 spec: image: image port: 8080 accessPolicy: inbound: rules: - application: app2 namespace: namespace2 outbound: rules: - application: app2 namespace: namespace2   allowing outbound traffic to a job in namespaces with label​  creates outbound rules to allow traffic to the skipjob job2 in all namespaces with label team: someteam on service port for app2Note that all skipjobs must have the postfix -skipjob in the name when defining the application name in the access policy.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: app1 spec: image: image port: 8080 accessPolicy: outbound: rules: - application: job2-skipjob namespaceByLabel: team: someteam   access policy to allow traffic to a public domain​  creates istio policies to allow traffic to a public domain on port 443, and different public domain on port 80  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: app1 spec: image: image port: 8080 accessPolicy: outbound: external: - host: kartverket.no - host: google.com ports: - name: http port: 80 protocol: HTTP   ","version":"Next","tagName":"h3"},{"title":"Replicas​","type":1,"pageTitle":"Common Skiperator configuration","url":"/docs/applikasjon-utrulling/skiperator/configuring#replicas","content":" you can either specify a fixed number of replicas or let the autoscaler handle it for you.  if not specified skiperator uses autoscaler by default:  minReplicas: 2 maxReplicas: 5   static:  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: static-replicas spec: image: image port: 8080 replicas: 2   autoscaler:  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: auto-replicas spec: image: image port: 8080 replicas: min: 3 max: 6 targetCpuUtilization: 60   This will always have minimum 3 pods running, and scale up to more pods (max 6) if cpu utilization hits 60%. Only minimum value is required.  ","version":"Next","tagName":"h3"},{"title":"Environment variables​","type":1,"pageTitle":"Common Skiperator configuration","url":"/docs/applikasjon-utrulling/skiperator/configuring#environment-variables","content":" Environment values can be set directly in the application spec with spec.env or by using a secret or config map with spec.envFrom.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: auto-replicas spec: image: image port: 8080 env: - name: ENV_VAR value: &quot;value&quot; envFrom: - configMap: config-map-name - configMap: config-map-name2 - secret: secret-name   ","version":"Next","tagName":"h3"},{"title":"GCP​","type":1,"pageTitle":"Common Skiperator configuration","url":"/docs/applikasjon-utrulling/skiperator/configuring#gcp","content":" If your application needs to read a gcp bucket for example you need to set up a service account with the correct permissions and add it to the application spec. Best practice here is to create a service account with the same name as the application, for example myapp@some-project-id.iam.gserviceaccount.com, then give this service account minimal permissions in GCP Console.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: auto-replicas spec: image: image port: 8080 gcp: auth: serviceAccount: myapp@some-project-id.iam.gserviceaccount.com   ","version":"Next","tagName":"h3"},{"title":"SKIPJob​","type":1,"pageTitle":"Common Skiperator configuration","url":"/docs/applikasjon-utrulling/skiperator/configuring#skipjob","content":" ","version":"Next","tagName":"h2"},{"title":"Cron - SkipJob​","type":1,"pageTitle":"Common Skiperator configuration","url":"/docs/applikasjon-utrulling/skiperator/configuring#cron---skipjob","content":" basic cron job that executes every minute  apiVersion: skiperator.kartverket.no/v1alpha1 kind: SKIPJob metadata: name: myjob spec: container: image: image:latest cron: schedule: &quot;* * * * *&quot;   ","version":"Next","tagName":"h3"},{"title":"Commands - SkipJob​","type":1,"pageTitle":"Common Skiperator configuration","url":"/docs/applikasjon-utrulling/skiperator/configuring#commands---skipjob","content":" a job that uses a command with a docker image  apiVersion: skiperator.kartverket.no/v1alpha1 kind: SKIPJob metadata: name: myjob spec: container: image: &quot;perl:5.34.0&quot; command: - &quot;perl&quot; - &quot;-Mbignum=bpi&quot; - &quot;-wle&quot; - &quot;print bpi(2000)&quot;   ","version":"Next","tagName":"h3"},{"title":"Access policy - SkipJob​","type":1,"pageTitle":"Common Skiperator configuration","url":"/docs/applikasjon-utrulling/skiperator/configuring#access-policy---skipjob","content":" This is the same as for applications, except we don't define inbound policies for jobs.  ","version":"Next","tagName":"h3"},{"title":"Routing​","type":1,"pageTitle":"Common Skiperator configuration","url":"/docs/applikasjon-utrulling/skiperator/configuring#routing","content":" ","version":"Next","tagName":"h2"},{"title":"Frontend and backend services under the same domain​","type":1,"pageTitle":"Common Skiperator configuration","url":"/docs/applikasjon-utrulling/skiperator/configuring#frontend-and-backend-services-under-the-same-domain","content":" One thing that is important to remember with routes is that the order of the routes matters. The route that is defined first will be the one that is matched first.  If your backend service expects requests without the leading pathPrefix, you can configure rewriteUri to remove the prefix before it arrives at the backend.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Routing metadata: name: myrouting spec: hostname: kartverket.com routes: - pathPrefix: /api # Highest priority rewriteUri: true targetApp: backend-app - pathPrefix: / # Lowest priority targetApp: frontend-app  ","version":"Next","tagName":"h3"},{"title":"Auto-login","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/tilgangsstyring/auto-login","content":"","keywords":"","version":"Next"},{"title":"📖 Eksempler​","type":1,"pageTitle":"Auto-login","url":"/docs/applikasjon-utrulling/tilgangsstyring/auto-login#-eksempler","content":" Under følger eksempler på hvordan å konfigurere automatisk innlogging på SKIP.  Eksempel 1 konfigurerer automatisk innlogging for alle beskyttede endepunkt, noe som passer godt for typiske hyllevare-løsninger.  Eksempel 2 konfigurerer automatisk innlogging med en dedikert innloggingssti (loginPath). En kan da konfigurere beskyttede endepunkt under authRules med feltet denyRedirect: true. Ztoperator vil da ikke omdirigere til innlogging og heller returnere 401 Unauthorizedhvis en innkommende forespørsel ikke er autentisert. En frontend applikasjon kan da heller omdirigere brukeren til innloggingsstien for å få opprettet en innlogget sesjon. Dette er noe som passer godt for Single Page Applications (SPA).  Eksempel 1: Automatisk innlogging for alle beskyttede endepunkt SkiperatorZtoperator 🚧 UNDER UTVIKLING 🚧 Støtte for å sette opp automatisk innlogging via Skiperator-manifestet er under utvikling og er foreløpig ikke tilgjengelig.  Eksempel 2: Automatisk innlogging med dedikert innloggingssti (loginPath) SkiperatorZtoperator 🚧 UNDER UTVIKLING 🚧 Støtte for å sette opp automatisk innlogging via Skiperator-manifestet er under utvikling og er foreløpig ikke tilgjengelig. ","version":"Next","tagName":"h2"},{"title":"Ztoperator","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/tilgangsstyring/ztoperator","content":"Ztoperator Ztoperator er en Kubernetes-operator som gjør det mulig å tilgangsstyre trafikk inn mot din applikasjon på en sikker og fleksibel måte. Operatoren gir deg kontroll over hvilke forespørsler som får aksessere applikasjonen din basert på Json Web Tokens (JWT). Hvilke felter som eksisterer og deres oppførsel er dokumentert i Ztoperator sin API-dokumentasjon.","keywords":"","version":"Next"},{"title":"Klientregistrering","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/tilgangsstyring/klientregistrering","content":"Klientregistrering Klientregistrering er en sentral del av tilgangsstyring med OAuth 2.0 og OIDC. Når du registrerer en klient, får applikasjonen sin egen identitet og kan autentisere seg mot de valgte identitetstilbyderne. Vi anbefaler å ha et bevisst forhold til hvilke tjenester som skal dele samme audience. Det er ofte mer hensiktsmessig å velge en restriktiv tilnærming, der hver tjeneste får sin egen klientregistrering, fremfor å gruppere flere tjenester under én felles klient. Dette gir bedre kontroll over tilgangsstyring og sikkerhet. Felles for alle identitetstilbydere på SKIP er at klientregistrering støttes fra plattformen ved hjelp av Kubernetes-operatorer. Digdirator håndterer klientregistrering mot Digdir sine fellesløsninger: ID-porten og Maskinporten.Azurerator håndterer klientregistrering mot Microsoft Entra ID (tidligere kjent som Azure Active Directory). Når man skal registrere en klient mot en eller flere av identitetstilbyderne, har man tre valg. Man kan benytte seg av Skiperator dersom klientregistreringen hører til en SKIP-applikasjon. Alternativt kan man bruke CRD-ene fra Digdirator og Azurerator direkte. SkiperatorDigdirator / Azurerator Skiperator tilbyr et forenklet API i CRD-en Application for å registrere klienter mot ID-porten og Maskinporten. Det jobbes også med et forenklet API for å registrere klienter mot Entra ID, men dette er ikke helt klart ennå. Fordelen med å benytte Skiperator for klientregistrering er at hemmelighetene som Digdirator oppretter blir automatisk injisert som miljøvariabler i deploymenten til applikasjonen. Dette gjør at applikasjonen enkelt kan få tilgang til hemmelighetene i et kjøretidsmiljø. Entra IDID-portenMaskinporten 🚧 UNDER UTVIKLING 🚧​ Opprettelse av app-registrering i Entra ID via spesifikasjonen til Application jobbes med, men er ikke klart helt ennå.","keywords":"","version":"Next"},{"title":"📅 Jobber på SKIP","type":0,"sectionRef":"#","url":"/docs/jobber-skip","content":"📅 Jobber på SKIP Her kan du lære det du trenger om å kjøre jobber (skedulerte oppgaver).","keywords":"","version":"Next"},{"title":"Token-validering og grovkornet autorisasjon","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/tilgangsstyring/token-validering","content":"","keywords":"","version":"Next"},{"title":"📖 Eksempler​","type":1,"pageTitle":"Token-validering og grovkornet autorisasjon","url":"/docs/applikasjon-utrulling/tilgangsstyring/token-validering#-eksempler","content":" Under følger en rekke eksempler på hvordan å konfigurere token-validering og grovkornet autorisasjon på SKIP.  Eksempel 1: Token-validering for alle endepunkt SkiperatorZtoperator 🚧 UNDER UTVIKLING 🚧 Støtte for å sette opp token-validering og grovkornet autorisasjon via Skiperator-manifestet er under utvikling og er foreløpig ikke tilgjengelig.  Eksempel 2: Legg til åpne endepunkt SkiperatorZtoperator 🚧 UNDER UTVIKLING 🚧 Støtte for å sette opp token-validering og grovkornet autorisasjon via Skiperator-manifestet er under utvikling og er foreløpig ikke tilgjengelig.  Eksempel 3: Legg til autoriserte endepunkt SkiperatorZtoperator 🚧 UNDER UTVIKLING 🚧 Støtte for å sette opp token-validering og grovkornet autorisasjon via Skiperator-manifestet er under utvikling og er foreløpig ikke tilgjengelig. ","version":"Next","tagName":"h2"},{"title":"💡 Kom i gang","type":0,"sectionRef":"#","url":"/docs/kom-i-gang","content":"","keywords":"","version":"Next"},{"title":"Velkomstord og introduksjon​","type":1,"pageTitle":"💡 Kom i gang","url":"/docs/kom-i-gang#velkomstord-og-introduksjon","content":" Gratulerer og velkommen! Du har nå begynt reisen inn mot vår fantastiske infrastrukturplattform SKIP og de omliggende systemer.  Under finner du en presentasjon som gir en introduksjon til SKIP. Presentasjonen er laget av Eline Henriksen, som var en av utviklerne bak SKIP. Thomas Berg har nå overtatt vedlikeholdet av den. Trykk på pilene for å gå gjennom presentasjonen.   ","version":"Next","tagName":"h2"},{"title":"📋 Praktisk intro til SKIP","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro","content":"📋 Praktisk intro til SKIP Github 101Sett opp apps repoKjør en applikasjon på SKIPOvervåk applikasjonen på SKIP","keywords":"","version":"Next"},{"title":"Oversikt over tjenester SKIP tilbyr","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/oversikt-over-tjenester-SKIP-tilbyr","content":"","keywords":"","version":"Next"},{"title":"Formål med dokument​","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/kom-i-gang/oversikt-over-tjenester-SKIP-tilbyr#formål-med-dokument","content":" Dette dokumentet er ment som en teknisk introduksjon til nye utviklere på SKIP-plattformen. I stedet for å skrive en full manual om alle de tjenestene og programvarene som SKIP tilbyr og er basert på, har vi skrevet litt kort om hver tjeneste du som utvikler kan benytte deg av, og hvordan den ser ut under panseret. Hvis det finnes mer dokumentasjon fra SKIP om tjenesten vil det være linket til her, men hvis du ønsker et skikkelig dypdykk vil det som regel være like greit å søke på nettet etter mer dokumentasjon - det er ikke noe vi har som ambisjon å produsere selv.  ","version":"Next","tagName":"h2"},{"title":"GKE Enterprise / Anthos​","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/kom-i-gang/oversikt-over-tjenester-SKIP-tilbyr#gke-enterprise--anthos","content":" GKE Enterprise er Googles hybridplatformsløsning for å kunne kjøre et Kubernetes cluster on-premise, men likevel være administrert via et sky-interface. GKE Enterprise følger med en rekke verktøy som gjør det enkelt for brukere å få innsyn i egne applikasjoner, håndheve policyer som skal virke på tvers av multisky-implementasjoner og gir oss mulighet til å integrere sikkerhet gjennom en &quot;develop-build-run cycle&quot;.  ","version":"Next","tagName":"h2"},{"title":"Logging, metrikker, tracing og alarmer​","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/kom-i-gang/oversikt-over-tjenester-SKIP-tilbyr#logging-metrikker-tracing-og-alarmer","content":" SKIP tilbyr innsyn i applikasjoners metrikker og logger ved hjelp av Grafana. Dette kan nås på https://monitoring.kartverket.cloud .  ","version":"Next","tagName":"h2"},{"title":"Grafana​","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/kom-i-gang/oversikt-over-tjenester-SKIP-tilbyr#grafana","content":" Grafana Loki er et logglagringsverktøy som brukes som datakilde for Grafana.  Grafana Mimir lagrer metrikker fra appliasjoner, og brukes som datakilde for Grafana.  Grafana Tempo lagrer tracing for applikasjoner, og brukes som datakilde for Grafana  Brukes for å sende ut varslinger basert på data i grafana.  ","version":"Next","tagName":"h3"},{"title":"Google Secret Manager​","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/kom-i-gang/oversikt-over-tjenester-SKIP-tilbyr#google-secret-manager","content":" For hemmelighetshåndtering anbefaler vi bruk av Google Secret Manager (GSM). Her har vi solid adgangskontroll og kan enkelt hente hemmeligheter både i build time og run time til applikasjoner vi kjører i Kubernetes.  I GSM opprettes hemmeligheter per prosjekt, og man kan adgangskontrollere både for et helt prosjekt og for individuelle hemmeligheter. Hemmeligheter kan versjoneres og rulleres automatisk.  Ved hjelp av et system kalt External Secrets er det enkelt å hente disse hemmelighetene til build time. Se Hente hemmeligheter fra hemmelighetshvelv.  For å hente hemmeligheter fra GSM under run time, se Autentisering mot GCP fra Applikasjon .  ","version":"Next","tagName":"h3"},{"title":"GitHub​","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/kom-i-gang/oversikt-over-tjenester-SKIP-tilbyr#github","content":" GitHub er en skybasert git-repository-tjeneste som vi bruker til å lagre kildekoden til Kartverkets prosjekter. Med GitHub får vi også mye annet også som kontinuerlig integrasjon, kodescanning.  ","version":"Next","tagName":"h3"},{"title":"Objektlagring​","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/kom-i-gang/oversikt-over-tjenester-SKIP-tilbyr#objektlagring","content":" SKIP tilbyr flere objektlagringstjenester som blant annet gir deg mulighet å lagre filer i sky eller on-prem.  For å lagre filer i sky anbefaler vi å benytte Google cloud storage . Dette er en lagringstjeneste som følger med Google Cloud Platform. Her kan du f. eks provisjonere bøtter via terraform, og laste opp filer til denne bøtten via en applikasjon på et Kubernetes cluster. Se Autentisering mot GCP fra Applikasjonfor å koble seg til GCP via en applikasjon.  I tillegg til lagring med Google cloud storage så har man mulighet til å benytte Scality on-prem som er et AWS S3-kompatibel løsning.  ","version":"Next","tagName":"h3"},{"title":"Continuous Deployment​","type":1,"pageTitle":"Oversikt over tjenester SKIP tilbyr","url":"/docs/kom-i-gang/oversikt-over-tjenester-SKIP-tilbyr#continuous-deployment","content":" ArgoCD er et deklarativt, GitOps-kontinuerlig leveranseverktøy for Kubernetes-applikasjoner. Det automatiserer distribusjon og administrasjon av applikasjoner i Kubernetes ved å synkronisere den ønskede tilstanden som er definert i Git-repositorier med den faktiske cluster konfigurasjonen. ","version":"Next","tagName":"h3"},{"title":"Ztoperator API Reference","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/tilgangsstyring/ztoperator/api-docs","content":"","keywords":"","version":"Next"},{"title":"AuthPolicy​","type":1,"pageTitle":"Ztoperator API Reference","url":"/docs/applikasjon-utrulling/tilgangsstyring/ztoperator/api-docs#authpolicy","content":" ↩ Parent  AuthPolicy is the Schema for the authpolicies API.  Name\tType\tDescription\tRequiredapiVersion\tstring\tztoperator.kartverket.no/v1alpha1\ttrue kind\tstring\tAuthPolicy\ttrue metadata\tobject\tRefer to the Kubernetes API documentation for the fields of the metadata field.\ttrue spec\tobject AuthPolicySpec defines the desired state of AuthPolicy. false status\tobject AuthPolicyStatus defines the observed state of AuthPolicy. false  ","version":"Next","tagName":"h2"},{"title":"AuthPolicy.spec​","type":1,"pageTitle":"Ztoperator API Reference","url":"/docs/applikasjon-utrulling/tilgangsstyring/ztoperator/api-docs#authpolicyspec","content":" ↩ Parent  AuthPolicySpec defines the desired state of AuthPolicy.  Name\tType\tDescription\tRequiredenabled\tboolean Whether to enable JWT validation. If enabled, incoming JWTs will be validated against the issuer specified in the app registration and the generated audience. true selector\tobject The Selector specifies which workload the defined auth policy should be applied to. true wellKnownURI\tstring WellKnownURI specifies the URi to the identity provider's discovery document (also known as well-known endpoint). true acceptedResources\t[]string AcceptedResources is used as a validation field following RFC8707. It defines accepted audience resource indicators in the JWT token. Each resource indicator must be a valid URI, and the indicator must be present as the aud claim in the JWT token. false audience\t[]string Audience defines the accepted audience (aud) values in the JWT. At least one of the listed audience values must be present in the token's aud claim for validation to succeed. false authRules\t[]object AuthRules defines rules for allowing HTTP requests based on conditions that must be met based on JWT claims. API endpoints not covered by AuthRules and/or IgnoreAuthRules requires an authenticated JWT by default. false autoLogin\tobject AutoLogin specifies the required configuration needed to log in users. false forwardJwt\tboolean If set to true, the original token will be kept for the upstream request. Defaults to true. false ignoreAuthRules\t[]object IgnoreAuthRules defines request matchers for HTTP requests that do not require JWT authentication. API endpoints not covered by AuthRules or IgnoreAuthRules require an authenticated JWT by default. false oAuthCredentials\tobject OAuthCredentials specifies a reference to a kubernetes secret in the same namespace holding OAuth credentials used for authentication. false outputClaimToHeaders\t[]object OutputClaimsToHeaders specifies a list of operations to copy the claim to HTTP headers on a successfully verified token. The header specified in each operation in the list must be unique. Nested claims of type string/int/bool is supported as well. If the claim is an object or array, it will be added to the header as a base64-encoded JSON string. false  ","version":"Next","tagName":"h3"},{"title":"AuthPolicy.spec.selector​","type":1,"pageTitle":"Ztoperator API Reference","url":"/docs/applikasjon-utrulling/tilgangsstyring/ztoperator/api-docs#authpolicyspecselector","content":" ↩ Parent  The Selector specifies which workload the defined auth policy should be applied to.  Name\tType\tDescription\tRequiredmatchLabels\tmap[string]string One or more labels that indicate a specific set of pods/VMs on which a policy should be applied. The scope of label search is restricted to the configuration namespace in which the resource is present. Validations: self.all(key, !key.contains('*')): wildcard not allowed in label key matchself.all(key, key.size() != 0): key must not be empty false  ","version":"Next","tagName":"h3"},{"title":"AuthPolicy.spec.authRules[index]​","type":1,"pageTitle":"Ztoperator API Reference","url":"/docs/applikasjon-utrulling/tilgangsstyring/ztoperator/api-docs#authpolicyspecauthrulesindex","content":" ↩ Parent  RequestAuthRule defines a rule for controlling access to HTTP requests using JWT authentication.  Name\tType\tDescription\tRequiredpaths\t[]string Paths specify a set of URI paths that this rule applies to. Each path must be a valid URI path, starting with '/' and not ending with '/'. true denyRedirect\tboolean DenyRedirect specifies whether a denied request should trigger auto-login (if configured) or not when it is denied due to missing or invalid authentication. Defaults to false, meaning auto-login will be triggered (if configured). false methods\t[]string Methods specifies HTTP methods that applies for the defined paths. If omitted, all methods are permitted. Allowed methods: GETPOSTPUTPATCHDELETEHEADOPTIONSTRACECONNECT false when\t[]object When defines additional conditions based on JWT claims that must be met. The request is permitted if at least one of the specified conditions is satisfied. false  ","version":"Next","tagName":"h3"},{"title":"AuthPolicy.spec.authRules[index].when[index]​","type":1,"pageTitle":"Ztoperator API Reference","url":"/docs/applikasjon-utrulling/tilgangsstyring/ztoperator/api-docs#authpolicyspecauthrulesindexwhenindex","content":" ↩ Parent  Condition represents a rule that evaluates JWT claims to determine access control.  This type allows defining conditions that check whether a specific claim in the JWT token contains one of the expected values.  If multiple conditions are specified, all must be met (AND logic) for the request to be allowed.  Name\tType\tDescription\tRequiredclaim\tstring Claim specifies the name of the JWT claim to check. true values\t[]string Values specifies a list of allowed values for the claim. If the claim in the JWT contains any of these values (OR logic), the condition is met. true  ","version":"Next","tagName":"h3"},{"title":"AuthPolicy.spec.autoLogin​","type":1,"pageTitle":"Ztoperator API Reference","url":"/docs/applikasjon-utrulling/tilgangsstyring/ztoperator/api-docs#authpolicyspecautologin","content":" ↩ Parent  AutoLogin specifies the required configuration needed to log in users.  Name\tType\tDescription\tRequiredenabled\tboolean Whether to enable auto login. If enabled, users accessing authenticated endpoints will be redirected to log in towards the configured identity provider. true logoutPath\tstring LogoutPath specifies which URI to redirect the user to when signing out. This will end the session for the application, but not end the session towards the configured identity provider. This feature will hopefully soon be available in later releases of Istio, ref. envoy/envoyproxy. true redirectPath\tstring RedirectPath specifies which path to redirect the user to after completing the OIDC flow. true scopes\t[]string Scopes specifies the OAuth2 scopes used during authorization code flow. true loginPath\tstring LoginPath specifies a list of URI paths that should trigger the auto-login behavior. When a request matches any of these paths, the user will be redirected to log in if not already authenticated. false  ","version":"Next","tagName":"h3"},{"title":"AuthPolicy.spec.ignoreAuthRules[index]​","type":1,"pageTitle":"Ztoperator API Reference","url":"/docs/applikasjon-utrulling/tilgangsstyring/ztoperator/api-docs#authpolicyspecignoreauthrulesindex","content":" ↩ Parent  RequestMatcher defines paths and methods to match incoming HTTP requests.  Name\tType\tDescription\tRequiredpaths\t[]string Paths specify a set of URI paths that this rule applies to. Each path must be a valid URI path, starting with '/' and not ending with '/'. true methods\t[]string Methods specifies HTTP methods that applies for the defined paths. If omitted, all methods are permitted. Allowed methods: GETPOSTPUTPATCHDELETEHEADOPTIONSTRACECONNECT false  ","version":"Next","tagName":"h3"},{"title":"AuthPolicy.spec.oAuthCredentials​","type":1,"pageTitle":"Ztoperator API Reference","url":"/docs/applikasjon-utrulling/tilgangsstyring/ztoperator/api-docs#authpolicyspecoauthcredentials","content":" ↩ Parent  OAuthCredentials specifies a reference to a kubernetes secret in the same namespace holding OAuth credentials used for authentication.  Name\tType\tDescription\tRequiredclientIDKey\tstring ClientIDKey specifies the data key to access the client ID. true clientSecretKey\tstring ClientSecretKey specifies the data key to access the client secret. true secretRef\tstring SecretRef specifies the name of the kubernetes secret. true  ","version":"Next","tagName":"h3"},{"title":"AuthPolicy.spec.outputClaimToHeaders[index]​","type":1,"pageTitle":"Ztoperator API Reference","url":"/docs/applikasjon-utrulling/tilgangsstyring/ztoperator/api-docs#authpolicyspecoutputclaimtoheadersindex","content":" ↩ Parent  ClaimToHeader specifies a list of operations to copy the claim to HTTP headers on a successfully verified token. The header specified in each operation in the list must be unique. Nested claims of type string/int/bool is supported as well.  Name\tType\tDescription\tRequiredclaim\tstring Claim specifies the name of the claim in the JWT token that will be copied to the header. true header\tstring Header specifies the name of the HTTP header to which the claim value will be copied. true  ","version":"Next","tagName":"h3"},{"title":"AuthPolicy.status​","type":1,"pageTitle":"Ztoperator API Reference","url":"/docs/applikasjon-utrulling/tilgangsstyring/ztoperator/api-docs#authpolicystatus","content":" ↩ Parent  AuthPolicyStatus defines the observed state of AuthPolicy.  Name\tType\tDescription\tRequiredready\tboolean true conditions\t[]object false message\tstring false observedGeneration\tinteger Format: int64 false phase\tstring false  ","version":"Next","tagName":"h3"},{"title":"AuthPolicy.status.conditions[index]​","type":1,"pageTitle":"Ztoperator API Reference","url":"/docs/applikasjon-utrulling/tilgangsstyring/ztoperator/api-docs#authpolicystatusconditionsindex","content":" ↩ Parent  Condition contains details for one aspect of the current state of this API Resource.  Name\tType\tDescription\tRequiredlastTransitionTime\tstring lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message\tstring message is a human readable message indicating details about the transition. This may be an empty string. true reason\tstring reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status\tenum status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type\tstring type of condition in CamelCase or in foo.example.com/CamelCase. Many .condition.type values are consistent across resources like Available, but because arbitrary conditions can be useful (see .node.status.conditions), the ability to deconflict is important. The regex it matches is (dns1123SubdomainFmt/)?(qualifiedNameFmt) true observedGeneration\tinteger observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false ","version":"Next","tagName":"h3"},{"title":"🗃️ GitHub","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/github","content":"🗃️ GitHub Kartverket lagrer kildekode på github.com, og gjennom organisasjonen vår distribuerer vi tilgang ved å fordele lisensene vi har kjøpt inn. For å få tilgang følg sjekklisten under. info Ved spørsmål vedrørende tilgang eller behov for støtte, ta kontakt på #gen-github på slack Kom igang ved å sjekke ut lenkene under: Tilgang til GitHubAutentisering til GitHub i terminalen (git clone / push med SSH)Opprette nytt repo på GitHubGitHub Actions som CI/CDHåndtering av sensitiv data som er kommet på repositorietBruk av GitHub med JenkinsTilgang til on-prem infrastruktur fra GitHub ActionsTilgang til repoer med tokens fra GitHub Actions","keywords":"","version":"Next"},{"title":"Bruk av GitHub med Jenkins","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/github/bruk-av-github-med-jenkins","content":"","keywords":"","version":"Next"},{"title":"📚 Autentisering 📚​","type":1,"pageTitle":"Bruk av GitHub med Jenkins","url":"/docs/kom-i-gang/praktisk-intro/github/bruk-av-github-med-jenkins#-autentisering-","content":" Det er flere måter å autentisere Jenkins mot GitHub på, blant annet; deploy keys, personal access tokens, GitHub App. Vi vil se at GitHub Apps er valget vi går for når vi autentiserer.   Deploy keys er enkle men;  👍 Eies av repoet og Jenkins (priv + pub nøkler)👊 Kan kun brukes som “Git” source på Jenkins👎 Snakker ikke med GitHub sitt API - kun pulle / pushe kode   Personal access tokens **** (PAT) gir mer;  👍 Kan brukes gjennom “GitHub” plugin på Jenkins (source)👍 Snakker med GitHub API’et - PR/Commit status triggere etc.👎 Nøkkelen følger brukeren, selv etter vedkommende bytter team eller slutter (kan slettes fra bruker)👎 Ikke i utgangspunktet gjenbrukbar (beta- fine grained PAT’er kan tilegnes flere repo pr. nøkkel)   GitHub Apps er litt mer å konfigurere, men er en kombinasjon av de over;  👍 Gjenbrukbare, som flere repoer kan bruke gjennom én privat nøkkel på Jenkins.👍 Eies av “Organisasjonen” Kartverket på Github, som da ikke er bundet til en GitHub bruker.👍 Kan brukes gjennom “GitHub” plugin på Jenkins (source)👍 Snakker med GitHub API’et - PR/Commit status triggere etc.👎 Ratelimit (men skal ikke være et problem)  ","version":"Next","tagName":"h2"},{"title":"🧑‍🚒 Brannmurer 🧑‍🚒​","type":1,"pageTitle":"Bruk av GitHub med Jenkins","url":"/docs/kom-i-gang/praktisk-intro/github/bruk-av-github-med-jenkins#-brannmurer-","content":" I utgangspunktet så skal portene til ditt Jenkins miljø være åpnet, slik at Jenkins når ut til GitHub. Men hvis det dette er første gang så må de åpnes for trafikk mot GitHub. Primært er det HTTPs og SSH trafikk som må tilgjengeliggjøres på port 443 og 22. Dette må bestilles hos drift.  ","version":"Next","tagName":"h2"},{"title":"🪝 Webhook 🪝​","type":1,"pageTitle":"Bruk av GitHub med Jenkins","url":"/docs/kom-i-gang/praktisk-intro/github/bruk-av-github-med-jenkins#-webhook-","content":" Work in progress. Er ikke ferdig testet enda.  For å få status på PR/Commits i GitHub så må GitHub ha en vei inn til Jenkins. Dette gjøres på et webhook endepunkt typisk seende slik ut https://&lt;jenkins-host&gt;/github-webhook/ . Dette er noe som må åpnes fra drift og spesifiseres inne i GitHub Appen.  ⚙️ Legg til hvordan det er med webhook secret.  ","version":"Next","tagName":"h2"},{"title":"📁 Oppsett av GitHub App 📁​","type":1,"pageTitle":"Bruk av GitHub med Jenkins","url":"/docs/kom-i-gang/praktisk-intro/github/bruk-av-github-med-jenkins#-oppsett-av-github-app-","content":" SKIP kontaktes og de setter opp en App for ditt behov. Er denne som følges: Using GitHub App authentication .  info Oppsettet av nøklen må du gjøre selv! Og dette FØR du får brukt Appen, men ETTER at SKIP setter igang med oppsett av app. SKIP sender melding når du må gjøre dette. nb: skal Appen ha flere/mindre rettigheter enn i oppskriften må du spesifisere dette  Når SKIP har satt opp Appen, må du sette den private nøkkelen, som senere skal deles med Jenkins. Dette gjøres slik som beskrevet i punktet Generating a private key for authenticating to the GitHub App .  Det er først når dette er gjort, at SKIP kan installere Appen på organisasjonen. Send en heads-up at du har lagret nøkkelen.  Hvis Appen er installert i org. og linket til ditt repo, og nøkkelen er satt opp i App og Jenkins så skal alt være på plass! 🎉 ","version":"Next","tagName":"h2"},{"title":"Dependabot","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot","content":"","keywords":"","version":"Next"},{"title":"Skru på automatiske versjonsoppdateringer​","type":1,"pageTitle":"Dependabot","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot#skru-på-automatiske-versjonsoppdateringer","content":" Automatiske versjonsoppdateringer må skrus på av en bruker med &quot;Admin&quot;-rolle i repoet. For det aktuelle repoet kan du gå til &quot;Settings &gt; Code security &gt; Dependabot &gt; Dependabot version updates&quot; og trykke på &quot;Enable&quot;.  Dersom dette ikke er gjort fra før må du sette opp en initiell versjon av filen .github/dependabot.yml. Det er anbefalt å sette opp for oppdatering av GitHub Actions for alle repoer som et minimum:  version: 2 updates: - package-ecosystem: &quot;github-actions&quot; directory: &quot;.github/workflows&quot; schedule: interval: &quot;daily&quot;   I tillegg bør det settes opp for de ulike språkene/økosystemene som ligger i repoet. Se eksemplene i avsnittet Eksempelkonfigurasjoner.  ","version":"Next","tagName":"h2"},{"title":"Feilsøking av Dependabot​","type":1,"pageTitle":"Dependabot","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot#feilsøking-av-dependabot","content":" For å se Dependabot-feil i et repo kan man gå til &quot;Insights &gt; Dependency graph &gt; Dependabot&quot; for å se feil, status og logger fra siste kjøringer.  Se også GitHub-dokumentasjonen for mer detaljert beskrivelse av feilsøking.  ","version":"Next","tagName":"h2"},{"title":"Konfigurasjon av Dependabot​","type":1,"pageTitle":"Dependabot","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot#konfigurasjon-av-dependabot","content":" ","version":"Next","tagName":"h2"},{"title":"Begrense antallet pull requests fra Dependabot​","type":1,"pageTitle":"Dependabot","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot#begrense-antallet-pull-requests-fra-dependabot","content":" Antallet PR-er er begrenset til 5 for vanlige versjonsoppdateringer og 10 for sikkerhetsoppdateringer. Antallet åpne PR-er kan konfigureres med open-pull-requests-limit for hvert økosystem. F.eks.:   - package-ecosystem: &quot;&lt;ecosystem&gt;&quot; open-pull-requests-limit: 20 # ... More config ...   ","version":"Next","tagName":"h3"},{"title":"Oppdateringsintervall​","type":1,"pageTitle":"Dependabot","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot#oppdateringsintervall","content":" Man kan styre Dependabot kjører ved å konfigurere schedule for hvert package-ecosystem.  For å kjøre oppdateringer hver morgen i ukedagene:   - package-ecosystem: &quot;pip&quot; directory: &quot;/&quot; schedule: interval: &quot;daily&quot; time: &quot;07:00&quot; timezone: &quot;Europe/Oslo&quot;   For å kjøre oppdateringer hver mandag morgen:   - package-ecosystem: &quot;pip&quot; directory: &quot;/&quot; schedule: interval: &quot;weekly&quot; day: &quot;monday&quot; time: &quot;07:00&quot; timezone: &quot;Europe/Oslo&quot;   ","version":"Next","tagName":"h3"},{"title":"Innlogging til private registries​","type":1,"pageTitle":"Dependabot","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot#innlogging-til-private-registries","content":" For at Dependabot skal kunne oppdatere dependencies som ligger i private/internal repoer (eks. actions), pakker i private package registries (npm, maven) elle  Merk at dette krever PATs eller access tokens som må ligge som en Dependabot repository secret i repoet.  Private registries må legges i registries-listen, og må i tillegg refereres til i updates-listen for relevante økosystemer. Eks.:  version: 2 registries: npm-github: type: npm-registry url: https://npm.pkg.github.com token: ${{ secrets.PAT_GHCR_READ }} updates: - package-ecosystem: npm # Check also for updates in GitHub Maven Package frontend-aut-lib registries: - npm-github # Remaining configuration skipped   NPM​  Se også dokumentasjonen.  registries: npm-github: type: npm-registry url: https://npm.pkg.github.com token: ${{ secrets.PAT_GHCR_READ }}   GitHub​  Denne brukes typisk for actions som ligger i private/interne repoer. Dependabot må bli gitt eksplisitt tilgang for å lese nye private/interne repoer (spør om hjelp i #gen-github på Slack). Se også dokumentasjonen.  registries: github-repo-name: type: git url: https://github.com username: x-access-token password: ${{ secrets.PAT_READ_REPO_NAME }}   Maven​  Denne brukes for å lese Maven-pakker som er publisert i et gitt repo. Se også dokumentasjonen.  registries: maven-repo-name: type: maven-repository url: https://maven.pkg.github.com/kartverket/repo-name username: ${{ secrets.GH_USERNAME }} password: ${{ secrets.PAT_GHCR_READ }}   ","version":"Next","tagName":"h3"},{"title":"Eksempelkonfigurasjoner​","type":1,"pageTitle":"Dependabot","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot#eksempelkonfigurasjoner","content":" De ulike økosystemene fungerer litt ulikt, og støtte kan variere. For mer informasjon se dokumentasjonen for økosystemer.  Under følger noen vanlige konfigurasjoner som flere team bruker. Disse er et gir et godt utgangspunkt men må i noen tilfeller konfigureres mer for å fungere.  ","version":"Next","tagName":"h2"},{"title":"GitHub Actions​","type":1,"pageTitle":"Dependabot","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot#github-actions","content":"  - package-ecosystem: &quot;github-actions&quot; directory: &quot;.github/workflows&quot; schedule: interval: &quot;daily&quot;   ","version":"Next","tagName":"h3"},{"title":"Git submodules​","type":1,"pageTitle":"Dependabot","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot#git-submodules","content":" Denne kan legges til i repoer som bruker git submodules, eks. apps-repoer som benytter argokit.   - package-ecosystem: &quot;gitsubmodule&quot; directory: &quot;/&quot; schedule: interval: &quot;daily&quot;   ","version":"Next","tagName":"h3"},{"title":"Docker​","type":1,"pageTitle":"Dependabot","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot#docker","content":" Pass på at directory peker til mappen som inneholder Dockerfile.   - package-ecosystem: &quot;docker&quot; directory: &quot;/&quot; schedule: interval: &quot;daily&quot;   ","version":"Next","tagName":"h3"},{"title":"Gradle​","type":1,"pageTitle":"Dependabot","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot#gradle","content":"  - package-ecosystem: &quot;gradle&quot; directory: &quot;/&quot; schedule: interval: &quot;daily&quot;   ","version":"Next","tagName":"h3"},{"title":"NPM eksempel​","type":1,"pageTitle":"Dependabot","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot#npm-eksempel","content":"  - package-ecosystem: &quot;npm&quot; directory: &quot;/&quot; schedule: interval: &quot;weekly&quot;   For å unngå for mange PR-er for minor- og patchversjoner er det i mange tilfeller ok å gruppere disse for NPM:   - package-ecosystem: &quot;npm&quot; directory: &quot;/&quot; schedule: interval: &quot;weekly&quot; groups: minor-and-patch-dependencies: patterns: - &quot;*&quot; update-types: - &quot;minor&quot; - &quot;patch&quot;   ","version":"Next","tagName":"h3"},{"title":"Go​","type":1,"pageTitle":"Dependabot","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot#go","content":"  - package-ecosystem: &quot;gomod&quot; directory: &quot;/&quot; schedule: interval: &quot;daily&quot;   ","version":"Next","tagName":"h3"},{"title":"Terraform​","type":1,"pageTitle":"Dependabot","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot#terraform","content":" Oppdater directory med riktig filsti.   - package-ecosystem: &quot;terraform&quot; directory: &quot;/&quot; schedule: interval: &quot;daily&quot;   Dersom det er flere mapper som inneholder Terraform kan directories brukes:   - package-ecosystem: &quot;terraform&quot; directories: - &quot;dev&quot; - &quot;prod&quot; schedule: interval: &quot;daily&quot;   ","version":"Next","tagName":"h3"},{"title":"Python​","type":1,"pageTitle":"Dependabot","url":"/docs/kom-i-gang/praktisk-intro/github/dependabot#python","content":"  - package-ecosystem: &quot;pip&quot; directory: &quot;/&quot; schedule: interval: &quot;daily&quot;  ","version":"Next","tagName":"h3"},{"title":"Håndtering av sensitiv data som er kommet på repositoriet","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/github/håndtering-av-sensitiv-data-som-er-kommet-på-repositoriet","content":"","keywords":"","version":"Next"},{"title":"📘 Instruksjoner​","type":1,"pageTitle":"Håndtering av sensitiv data som er kommet på repositoriet","url":"/docs/kom-i-gang/praktisk-intro/github/håndtering-av-sensitiv-data-som-er-kommet-på-repositoriet#-instruksjoner","content":" for å se om secret scanning har avduket noen sensitive data i repositoriet gå inn på repositorierts forside og klikk deg inn på security-fanen og deretter trykk deg inn på sidemeny-valget “Secret scanning alerts”trykk deg inn på det varselet for det sensitive dataen du skal løseher får du vite hvilke filer det er snakk om og akkurat hvilken linje det er snakk om.Deretter er det å følge denne guiden, Fjerne sensitive data fra repositorier for selve fjerningen av de sensitive dataenenår fjerningen er gjor kan man lukke varslet  ","version":"Next","tagName":"h2"},{"title":"📋 Relaterte artikler​","type":1,"pageTitle":"Håndtering av sensitiv data som er kommet på repositoriet","url":"/docs/kom-i-gang/praktisk-intro/github/håndtering-av-sensitiv-data-som-er-kommet-på-repositoriet#-relaterte-artikler","content":" Fjerne sensitive data fra repositorier  Secret Scanning ","version":"Next","tagName":"h2"},{"title":"Autentisering til GitHub i terminalen","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/github/autentisering-til-github-i-terminalen","content":"","keywords":"","version":"Next"},{"title":"Oppdater Git​","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/kom-i-gang/praktisk-intro/github/autentisering-til-github-i-terminalen#oppdater-git","content":" warning Ikke hopp over dette steget . Du finner oversikt over sårbare versjoner av git her: https://github.com/git/git/security/advisories  Velg ditt operativsystem og følg instruksene for å installere den nyeste versjonen av Git.  Oppdater Git for LinuxOppdater Git for macOSOppdater Git for Windows  Du kan sjekke hvilken versjon du har med denne kommandoen:  git --version   ","version":"Next","tagName":"h2"},{"title":"Generer SSH nøkkel​","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/kom-i-gang/praktisk-intro/github/autentisering-til-github-i-terminalen#generer-ssh-nøkkel","content":" Du kan velge mellom ed25519 og RSA-4096.  (det finnes flere alternativer, men disse er vurdert som akseptable)  Bruk ssh-keygen for å generere en ny nøkkel lokalt på din maskin. Husk å bytt ut “DINEPOST” med Kartverket eposten din (f.eks. &quot;jell.fjell@kartverket.no&quot; ).  ssh-keygen -a 50 -t ed25519 -f ~/.ssh/github -C “DINEPOST”   Alternativt kan du bruke RSA-4096 ssh-keygen -t rsa -b 4096 -f ~/.ssh/github -C &quot;DINEPOST&quot;   warning NB! Husk å sette passord når du blir spurt. Ikke la passordfeltet stå tomt.  ","version":"Next","tagName":"h2"},{"title":"Sett lokale rettigheter på SSH nøkkelen​","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/kom-i-gang/praktisk-intro/github/autentisering-til-github-i-terminalen#sett-lokale-rettigheter-på-ssh-nøkkelen","content":" SSH nøkkelen er privat for din bruker, og skal kun leses av din bruker.  chmod 600 ~/.ssh/github   ","version":"Next","tagName":"h3"},{"title":"Legg til nøkkelen (public key) i GitHub​","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/kom-i-gang/praktisk-intro/github/autentisering-til-github-i-terminalen#legg-til-nøkkelen-public-key-i-github","content":" Vis og kopier din public key fra~/.ssh/github.pubfra terminalen.  cat ~/.ssh/github.pub   Marker utskriften og kopier innholdet.  Logg inn på GitHub.com med Kartverket kontoen din.Trykk på profilbildet ditt, øverst i høyre hjørne.Velg « Settings ».Naviger deg til « SSH and GPG keys » (under kategorien «Access») i venstre kolonne.Trykk på den grønne « New SSH key » knappen.Skriv inn en passelig tittel (f.eks. “Min private SSH nøkkel”).Kopier og lim inn innholdet fra~/.ssh/github.pub(ikke private key), som vist i første steg.Trykk på « Add SSH key ».Du skal nå se oversikten over dine nøkler, med den nye nøkkelen i listen.For å bruke kartverket nøkkelen må man bekrefte nøkkelen med SSO. Dette gjøres ved å trykke configure SSO på nøkkelen.  ","version":"Next","tagName":"h2"},{"title":"Test nøkkelen​","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/kom-i-gang/praktisk-intro/github/autentisering-til-github-i-terminalen#test-nøkkelen","content":" Du kan raskt teste nøkkelen din mot GitHub ved å kjøre:  ssh -T git@github.com -i ~/.ssh/github   Du skal få tilbakemelding om vellykket autentisering:  Hi! You've successfully authenticated, but GitHub does not provide shell access.   ","version":"Next","tagName":"h2"},{"title":"Automatisk bruk av nøkkelen din​","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/kom-i-gang/praktisk-intro/github/autentisering-til-github-i-terminalen#automatisk-bruk-av-nøkkelen-din","content":" note Det finnes flere måter å ta i bruk nøkkelen din. Dette er et eksempel på hvordan, men du står fritt til å bruke andre løsninger.  Opprett filen~/.ssh/configog fyll den ut med innholdet for GitHub med nøkkelen din:  Host github HostName github.com User git IdentityFile ~/.ssh/github   ","version":"Next","tagName":"h2"},{"title":"Ta i bruk nøkkelen når du kloner et repo​","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/kom-i-gang/praktisk-intro/github/autentisering-til-github-i-terminalen#ta-i-bruk-nøkkelen-når-du-kloner-et-repo","content":" Du kan ta i bruk nøkkelen din ved å refere til github når du skal klone et repo. Husk å bytt ut “DITTREPO” med navnet på repoet du prøver å klone.  git clone github:kartverket/DITTREPO.git   Når du kloner repoet på denne måten vil Git automatisk ta i bruk remote med din konfigurasjon (tar automatisk i bruk nøkkelen din ved git pull / push ).  ","version":"Next","tagName":"h3"},{"title":"Legg til navn og epost for riktig eier av commits​","type":1,"pageTitle":"Autentisering til GitHub i terminalen","url":"/docs/kom-i-gang/praktisk-intro/github/autentisering-til-github-i-terminalen#legg-til-navn-og-epost-for-riktig-eier-av-commits","content":" For at commits du gjør på din maskin skal stemme overens med GitHub brukeren din må du sette brukernavn og epost i Git. Husk å bytt ut “DITT NAVN” med github brukernavnet ditt (f.eks. “jellfjell“) og “DINEPOST” med Kartverket eposten din (f.eks. &quot;jell.fjell@kartverket.no&quot; ).  git config --global user.name &quot;DITT NAVN&quot; git config --global user.email &quot;DINEPOST&quot;   TLDR For deg som ikke leste i gjennom og vil rett på sak uten forklaring. Oppdater Git: https://git-scm.com/downloads - IKKE HOPP OVER DETTE STEGET ssh-keygen -a 50 -t ed25519 -f ~/.ssh/github -C “DINEPOST” chmod 600 ~/.ssh/github cat ~/.ssh/github.pub Kopier og lim inn public key på GitHub https://github.com/settings/ssh/new ssh -T git@github.com -i ~/.ssh/github git config --global user.name &quot;DITT NAVN&quot; git config --global user.email &quot;DINEPOST&quot; git clone URL --config core.sshCommand=&quot;ssh -i ~/.ssh/github&quot; Nå er du klar for å begi deg ut på eventyr. ","version":"Next","tagName":"h2"},{"title":"Opprette nytt repo på Github","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/github/opprette-nytt-repo-på-github","content":"","keywords":"","version":"Next"},{"title":"Merknad for produkter som ikke er på SKIP​","type":1,"pageTitle":"Opprette nytt repo på Github","url":"/docs/kom-i-gang/praktisk-intro/github/opprette-nytt-repo-på-github#merknad-for-produkter-som-ikke-er-på-skip","content":" Merk at det meste av dette dokumentet også er gyldig for prosjekter som ikke er på SKIP-plattformen - men at det likevel er skrevet for SKIP-teams, så sikkerhetsreglene kan sees på som gode råd dersom du ikke skal bruke SKIP.  For ikke å snakke om at du dersom du følger disse sikkerhetsreglene vil få en mye enklere jobb hvis du skal flytte prosjektet over til SKIP i fremtiden   ","version":"Next","tagName":"h2"},{"title":"Hvordan opprette et nytt GitHub Repository​","type":1,"pageTitle":"Opprette nytt repo på Github","url":"/docs/kom-i-gang/praktisk-intro/github/opprette-nytt-repo-på-github#hvordan-opprette-et-nytt-github-repository","content":" Logg inn på GitHubOpprett et nytt repository ved å trykke på pluss-ikonet øverst til høyre på https://github.com og velge “New repository”. Dette gjelder uansett om du skal lage et nytt prosjekt eller importere et eksisterende prosjekt, siden du ikke vil kunne bruke “Import”-funksjonaliteten på vanlig måte.Dersom du skal importere et eksisterende git-repository, følg denne tutorialen.Fyll ut skjemaet med riktig informasjon. Huskeregler: Alle prosjekter som ikke skal være åpne skal være Internal . Det er likevel mulig å invitere eksterne utviklere. Mer informasjon: https://docs.github.com/en/repositories/creating-and-managing-repositories/about-repositories#about-repository-visibilityPass på at Owner er satt til kartverket , og ikke din private bruker.Ikke velg en lisens med mindre du faktisk skal lage et open-source prosjekt. Å velge en åpen kildekode-lisens her kan ødelegge for sikkerhetsverktøyene i Kartverket og i siste instans skape legale problemer for Kartverket. Hvis du er i tvil, ta kontakt med SKIP-teamet. Dokumenter hvilket team som er ansvarlig for repositoriet ved å opprette en Codeowners fil.Dette er dokumentert her. Som regel er det nok med en linje - slik (bytt ut skip med ditt eget team).Gi teamet ditt rettigheter til repoet. Dette er dokumentert her. Det er vanlig å sette Tech Lead som eier for repositoriet, men dette bestemmer dere selv.  ","version":"Next","tagName":"h2"},{"title":"Opprett tilganger til Google Cloud for Github Actions​","type":1,"pageTitle":"Opprette nytt repo på Github","url":"/docs/kom-i-gang/praktisk-intro/github/opprette-nytt-repo-på-github#opprett-tilganger-til-google-cloud-for-github-actions","content":" Dersom du har behov til å autentisere deg mot GCP kan du legge til at ditt repo GitHub kan autentisere seg mot Google Cloud med en bestemt bruker. Da må man sette opp Workload Identity Federation. Dette er noe SKIP ordner for produktteamene på en automatisert måte ved hjelp av Terraform.  Ønsker du å legge til et nytt repo kan du opprette en Pull Request for dette repoet: https://github.com/kartverket/gcp-service-accounts  Eksempel på liste over GitHub repoer for KomReg: https://github.com/kartverket/gcp-service-accounts/blob/main/modules.tf  module &quot;komreg&quot; { source = &quot;./project_team&quot; team_name = &quot;KomReg&quot; repositories = [ &quot;kartverket/komreg-frontend&quot;, &quot;kartverket/komreg-backend&quot;, &quot;kartverket/komreg-frontend-api&quot;, # Legg til flere repoer i denne listen ] env = var.env project_id = var.komreg_project_id kubernetes_project_id = var.kubernetes_project_id can_manage_log_alerts_and_metrics = true can_manage_sa = true extra_team_sa_roles = [ &quot;roles/resourcemanager.projectIamAdmin&quot;, &quot;roles/secretmanager.admin&quot;, &quot;roles/storage.admin&quot; ] }   Når PR’en merges inn vil det ved et nytt team bli opprettet en deploy-servicekonto, som heter &lt;teamnavn&gt;-deploy@&lt;prosjekt-id&gt;.iam.gserviceaccount.com . Denne servicekontoen tillater at github-repoene i listen har lov til å etterligne den og dens tilganger.  Mer informasjon om Github Actions: GitHub Actions som CI/CD ","version":"Next","tagName":"h2"},{"title":"⚙️ Kubernetes","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/kubernetes","content":"⚙️ Kubernetes","keywords":"","version":"Next"},{"title":"Tilgang til GitHub","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/github/tilgang-til-github","content":"","keywords":"","version":"Next"},{"title":"Bistand og diskusjon rundt GitHub​","type":1,"pageTitle":"Tilgang til GitHub","url":"/docs/kom-i-gang/praktisk-intro/github/tilgang-til-github#bistand-og-diskusjon-rundt-github","content":" Logg på slack ved å laste ned programmet fra http://slack.com og bruk kartverketgroup.slack.com som workspaceTa kontakt med SKIP på slack i #gen-github for å få en invitasjon til GitHub-organisasjonen til Kartverket (send github-brukernavnet ditt). Dersom du vet på forhånd at du jobber som del av et team så fortell oss hvilke team dette er så får vi lagt deg i tilsvarende team i GitHub   ","version":"Next","tagName":"h2"},{"title":"Valg av ACME og cert-manager for sertifikathåndtering","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/bruk-av-acme-og-certmanager","content":"","keywords":"","version":"Next"},{"title":"Innledning​","type":1,"pageTitle":"Valg av ACME og cert-manager for sertifikathåndtering","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/bruk-av-acme-og-certmanager#innledning","content":" For å effektivisere og sikre prosessen med sertifikathåndtering har vi valgt å bruke ACME (Automatic Certificate Management Environment) og cert-manager. Her er hovedgrunnene til vårt valg:  ","version":"Next","tagName":"h2"},{"title":"Automatisering og effektivitet​","type":1,"pageTitle":"Valg av ACME og cert-manager for sertifikathåndtering","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/bruk-av-acme-og-certmanager#automatisering-og-effektivitet","content":" Redusert risiko for utløpte sertifikater: ACME automatiserer fornyelsen av sertifikater, noe som sikrer at de alltid er gyldige og reduserer risikoen for tjenesteavbrudd på grunn av utløpte sertifikater. Minimert menneskelig feil: Ved å automatisere sertifikathåndteringen med cert-manager, reduserer vi sannsynligheten for menneskelige feil som kan oppstå ved manuell håndtering, som feilkonfigurasjoner eller glemte fornyelser.  ","version":"Next","tagName":"h2"},{"title":"Forbedret sikkerhet​","type":1,"pageTitle":"Valg av ACME og cert-manager for sertifikathåndtering","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/bruk-av-acme-og-certmanager#forbedret-sikkerhet","content":" Hyppige fornyelser: ACME muliggjør hyppigere fornyelser av sertifikater, noe som reduserer vinduet for potensielle angrep dersom et sertifikat skulle bli kompromittert. Automatisert oppdagelse og revokasjon: Cert-manager overvåker kontinuerlig sertifikatenes status og kan automatisk oppdage og revokere kompromitterte sertifikater.  ","version":"Next","tagName":"h2"},{"title":"Skalerbarhet og fleksibilitet​","type":1,"pageTitle":"Valg av ACME og cert-manager for sertifikathåndtering","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/bruk-av-acme-og-certmanager#skalerbarhet-og-fleksibilitet","content":" Håndtering av store volumer: Med ACME og cert-manager kan vi effektivt håndtere et stort antall sertifikater uten å øke arbeidsbelastningen på teamene. Antallet tjenester i Kartverket gjør det vanskelig å håndtere uten automasjon, eller fellessertifikater som benyttes på mange tjenester, noe som gjør de mer sårbare. Integrasjon med Kubernetes: Cert-manager integreres sømløst med Kubernetes, noe som gjør det enkelt å administrere sertifikater i våre containeriserte applikasjoner, og sikre at de alltid er oppdaterte.  ","version":"Next","tagName":"h2"},{"title":"Overholdelse og beste-praksis​","type":1,"pageTitle":"Valg av ACME og cert-manager for sertifikathåndtering","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/bruk-av-acme-og-certmanager#overholdelse-og-beste-praksis","content":" Samsvar med industristandarder: Ved å bruke ACME og cert-manager, sikrer vi at vi overholder bransjestandarder og forskrifter som krever hyppige fornyelser og sikker håndtering av digitale sertifikater. Sentralisert kontroll og synlighet: Cert-manager gir oss en sentralisert plattform for å administrere alle våre sertifikater, noe som gir bedre kontroll og synlighet over hele sertifikatbeholdningen.  ","version":"Next","tagName":"h2"},{"title":"Konklusjon​","type":1,"pageTitle":"Valg av ACME og cert-manager for sertifikathåndtering","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/bruk-av-acme-og-certmanager#konklusjon","content":" Valget av ACME og cert-manager for sertifikathåndtering gir oss en sikker, effektiv og skalerbar løsning som reduserer risikoen for menneskelige feil, forbedrer sikkerheten og sikrer samsvar med bransjestandarder. Ved å automatisere sertifikathåndteringen kan vi fokusere på strategiske initiativer og opprettholde en sterk sikkerhetsstilling. ","version":"Next","tagName":"h2"},{"title":"Autentisering mot GCP fra applikasjon","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/autentisering-mot-gcp-fra-applikasjon","content":"","keywords":"","version":"Next"},{"title":"1. Opprett Servicekonto​","type":1,"pageTitle":"Autentisering mot GCP fra applikasjon","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/autentisering-mot-gcp-fra-applikasjon#1-opprett-servicekonto","content":" Dersom man ønsker å få tilgang til GCP-tjenester fra Kubernetes gjøres dette med å først opprette en servicekonto i GCP og å gi den IAM-rettigheter til det man ønsker at den skal gjøre.  Servicekontoer bør enten opprettes med terraform eller via gcp-service-accounts repoet til SKIP.  ","version":"Next","tagName":"h2"},{"title":"2. Gi WIF IAM Policy til Servicekonto​","type":1,"pageTitle":"Autentisering mot GCP fra applikasjon","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/autentisering-mot-gcp-fra-applikasjon#2-gi-wif-iam-policy-til-servicekonto","content":" For å autentisere som denne GCP-servicekontoen fra Kubernetes må Kubernetes-servicekontoen gis tilganger til det. Dette gjøres ved å gi Kubernetes-servicekontoen rollen iam.workloadIdentityUser.  Gitt variablene:  GCP_SA_NAME - Navnet på GCP servicekontoen GCP_SA_PROJECT_ID - GCP Project ID til prosjektet GCP SA ligger i KUBERNETES_PROJECT_ID - GCP Project ID for Kubernetes-cluster (for eksempel `kubernetes-dev-94b9` for dev-clusteret) KUBERNETES_NAMESPACE - Kubernetes namespace hvor servicekontoen er opprettet KUBERNETES_SA_NAME - Navnet på Kubernetes service accounten som brukes av en Pod (Vanligvis samme som applikasjonsnavnet, men med et suffix -skipjob for SKIPJob'er)   Kjør følgende kommando med gcloud CLI:  gcloud iam service-accounts add-iam-policy-binding \\ GCP_SA_NAME@GCP_SA_PROJECT_ID.iam.gserviceaccount.com \\ --role=roles/iam.workloadIdentityUser \\ --member=&quot;serviceAccount:KUBERNETES_PROJECT_ID.svc.id.goog[KUBERNETES_NAMESPACE/KUBERNETES_SA_NAME]&quot;   ","version":"Next","tagName":"h2"},{"title":"3. Legg inn config i Skiperatormanifest​","type":1,"pageTitle":"Autentisering mot GCP fra applikasjon","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/autentisering-mot-gcp-fra-applikasjon#3-legg-inn-config-i-skiperatormanifest","content":" Til slutt legger man til gcp config i sin skiperator Application for å lage kubernetes-config slik at podden kan autentisere mot GCP.  //yaml format spec: gcp: auth: serviceAccount: GCP_SA_NAME@GCP_SA_PROJECT_ID.iam.gserviceaccount.com   Nå kan man følge “Authenticate from your code” under https://cloud.google.com/anthos/fleet-management/docs/use-workload-identity#-python for å autentisere mot GCP fra koden sin.  Når dette er gjort kan applikasjonen snakke med GCP under runtime.  ","version":"Next","tagName":"h2"},{"title":"Alternativ til 1 / 2​","type":1,"pageTitle":"Autentisering mot GCP fra applikasjon","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/autentisering-mot-gcp-fra-applikasjon#alternativ-til-1--2","content":" Dersom man ikke ønsker å legge til roller manuelt har SKIP lagt til en ny måte å legge til Workload Identity User på en service account, ved hjelp av Crossplane.  apiVersion: 'skip.kartverket.no/v1alpha1' kind: 'WorkloadIdentityInstance' metadata: name: 'service-account-wi' spec: parameters: gcpKubernetesProject: 'some-kubernetes-project' #eks: 'kubernetes-dev-94b9' gcpProject: 'gcp-project-where-service-account-is' #eks: 'dsa-dev-e32c' gcpServiceAccount: 'name-of-service-account-in-gcp' #eks: 'dsa-runtime@dsa-dev-e32c.iam.gserviceaccount.com' serviceAccount: 'name-of-service-account-in-kubernetes' #eks 'dsa-backend', typically same name as your Application   Se Provisjonere infrastruktur med Crossplane om du ikke har brukt Crossplane tidligere. ","version":"Next","tagName":"h2"},{"title":"Bruk av porter i pods","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/bruk-av-porter-i-pods","content":"Bruk av porter i pods Porter under 1024 er priviligierte og krever at prosessen som kjører kjører som root. Dette er ikke tillatt på SKIP, og prosessen må derfor binde til en høyere port. Dette betyr at man ofte må gjøre tilpasninger på Docker-imaget man bygger slik at f.eks. nginx binder til en annen port. Når prosessen i containeren binder seg til en upriviligert port, kan man spesifisere denne porten i Skiperator-manifestet slik som under. I bakgrunnen vil Skiperator ta seg av å lage en Kubernetes-service for denne porten slik at trafikken kan rutes til riktig sted. apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: teamname-frontend namespace: yournamespace spec: image: &quot;kartverket/eksempel-image&quot; port: 8080 additionalPorts: - name: metrics-port port: 8181 protocol: TCP - name: another-port port: 8090 protocol: TCP En annen ting å merke seg her er muligheten for å spesifisere ekstra porter som kan benyttes til andre formål som f.eks. helsesjekker eller prometheus-metrikker. Disse portene vil automatisk få opprettet en service, men det er fortsatt kun hovedporten som kobles opp mot en ingress-gateway. På den måten kan man skille ut endepunktet som ikke trengs eksternt.","keywords":"","version":"Next"},{"title":"End-user IP-Addresses in Containers","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/end-user ip-addresses-in-containers","content":"End-user IP-Addresses in Containers To forward end-user IP-Addresses to a kubernetes container running spring boot, you need to add the following line to your configuration: server.forward-headers-strategy=NONE After testing, we found that this setting should be “NONE”. Running spring Behind a Front-end Proxy Server Spring server.forward-headers-strategy NATIVE vs FRAMEWORK","keywords":"","version":"Next"},{"title":"Certificates outside ACME","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/certificates-outside-acme","content":"","keywords":"","version":"Next"},{"title":"Create certificate secret resource in istio-gateways​","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/certificates-outside-acme#create-certificate-secret-resource-in-istio-gateways","content":" To be able to use a custom certificate we need a secret to mount to the gateway resource. This is a kubernetes.io/tls type secret and can be created via external secrets like this:  apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: star-matrikkel namespace: istio-gateways spec: dataFrom: - extract: conversionStrategy: Default decodingStrategy: Auto key: star-matrikkel-no-key refreshInterval: 1h secretStoreRef: kind: SecretStore name: gsm target: creationPolicy: Owner deletionPolicy: Retain name: star-matrikkel # Secret in Kubernetes template: engineVersion: v2 mergePolicy: Replace type: kubernetes.io/tls   This fetches the secret from Google Secret Manager. This secret should look like this:  { &quot;tls.crt&quot;:&quot;[base64 encoded cert chain]&quot;, &quot;tls.key&quot;:&quot;[base64 encoded tls.key]&quot; }   ","version":"Next","tagName":"h2"},{"title":"Edit the gateway resource​","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/certificates-outside-acme#edit-the-gateway-resource","content":" The gateway resource should then be updated with the new secret:  apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: gateway-ingress namespace: matrikkel-keycloak spec: selector: app: istio-ingress-external servers: - hosts: - auth.matrikkel.no port: name: http number: 80 protocol: HTTP - hosts: - auth.matrikkel.no port: name: https number: 443 protocol: HTTPS tls: credentialName: star-matrikkel # Secret created by externalsecret mode: SIMPLE   ","version":"Next","tagName":"h2"},{"title":"If Skiperator is the gateway creator​","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/certificates-outside-acme#if-skiperator-is-the-gateway-creator","content":" When the gateway is created via Skiperator it will have a credentialName corresponding to the secret created by the certificate from Skiperator. Skiperator will reset configurations to its resources unless the resource labeled “skiperator.kartverket.no/ignore: &quot;true&quot;“. This will make skiperator ignore this specific resource during reconciliation loops.  apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: labels: skiperator.kartverket.no/ignore: &quot;true&quot;   This is meant to be a temporary solution, and ACME is the prefered way to get certificates in SKIP.  ","version":"Next","tagName":"h3"},{"title":"Change to ACME certificate​","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/certificates-outside-acme#change-to-acme-certificate","content":" ","version":"Next","tagName":"h2"},{"title":"Non-Skiperator apps​","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/certificates-outside-acme#non-skiperator-apps","content":" Using ACME certificate on a non skiperator app requires a certificate resource, and using the resulting secret in the gateway. This resource must be created in the istio-gateways namespace and therefore in the skip-apps :  apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: certificate-name namespace: istio-gateways spec: dnsNames: - appname.kartverket.no issuerRef: kind: ClusterIssuer name: cluster-issuer secretName: desired-secret-name   After this is created and the secret is created, the gateway resource can be edited, and spec.tls.credentialName set to the secret.  ","version":"Next","tagName":"h3"},{"title":"Skiperator apps​","type":1,"pageTitle":"Certificates outside ACME","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/certificates-outside-acme#skiperator-apps","content":" Remove the “skiperator.kartverket.no/ignore: &quot;true&quot;“ label, and skiperator will handle the rest. ","version":"Next","tagName":"h3"},{"title":"API Reference","type":0,"sectionRef":"#","url":"/docs/applikasjon-utrulling/skiperator/api-docs","content":"","keywords":"","version":"Next"},{"title":"Application​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#application","content":" ↩ Parent  Application  Root object for Application resource. An application resource is a resource for easily managing a Dockerized container within the context of a Kartverket cluster. This allows product teams to avoid the need to set up networking on the cluster, as well as a lot of out of the box security features.  Name\tType\tDescription\tRequiredapiVersion\tstring\tskiperator.kartverket.no/v1alpha1\ttrue kind\tstring\tApplication\ttrue metadata\tobject\tRefer to the Kubernetes API documentation for the fields of the metadata field.\ttrue spec\tobject false status\tobject SkiperatorStatus A status field shown on a Skiperator resource which contains information regarding deployment of the resource. false  ","version":"Next","tagName":"h2"},{"title":"Application.spec​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspec","content":" ↩ Parent  Name\tType\tDescription\tRequiredimage\tstring The image the application will run. This image will be added to a Deployment resource true port\tinteger The port the deployment exposes true accessPolicy\tobject The root AccessPolicy for managing zero trust access to your Application. See AccessPolicy for more information. false additionalPorts\t[]object An optional list of extra port to expose on a pod level basis, for example so Instana or other APM tools can reach it false appProtocol\tenum Protocol that the application speaks. Enum: http, tcp, udp Default: http false authorizationSettings\tobject Used for allow listing certain default blocked endpoints, such as /actuator/ end points false command\t[]string Override the command set in the Dockerfile. Usually only used when debugging or running third-party containers where you don't have control over the Dockerfile false enablePDB\tboolean Whether to enable automatic Pod Disruption Budget creation for this application. Default: true false env\t[]object Environment variables that will be set inside the Deployment's Pod. See https://pkg.go.dev/k8s.io/api/core/v1#EnvVar for examples. false envFrom\t[]object Environment variables mounted from files. When specified all the keys of the resource will be assigned as environment variables. Supports both configmaps and secrets. For mounting as files see FilesFrom. false filesFrom\t[]object Mounting volumes into the Deployment are done using the FilesFrom argument FilesFrom supports ConfigMaps, Secrets and PVCs. The Application resource assumes these have already been created by you, and will fail if this is not the case. For mounting environment variables see EnvFrom. false gcp\tobject GCP is used to configure Google Cloud Platform specific settings for the application. false idporten\tobject Settings for IDPorten integration with Digitaliseringsdirektoratet false ingresses\t[]string Any external hostnames that route to this application. Using a skip.statkart.no-address will make the application reachable for kartverket-clients (internal), other addresses make the app reachable on the internet. Note that other addresses than skip.statkart.no (also known as pretty hostnames) requires additional DNS setup. The below hostnames will also have TLS certificates issued and be reachable on both HTTP and HTTPS. Ingresses must be lowercase, contain no spaces, be a non-empty string, and have a hostname/domain separated by a period They can optionally be suffixed with a plus and name of a custom TLS secret located in the istio-gateways namespace. E.g. &quot;foo.atkv3-dev.kartverket-intern.cloud+env-wildcard-cert&quot; false istioSettings\tobject IstioSettings are used to configure istio specific resources such as telemetry. Currently, adjusting sampling interval for tracing is the only supported option. By default, tracing is enabled with a random sampling percentage of 10%. Default: map[telemetry tracing:[map[randomSamplingPercentage:10]] ] false labels\tmap[string]string Labels can be used if you want every resource created by your application to have the same labels, including your application. This could for example be useful for metrics, where a certain label and the corresponding resources liveliness can be combined. Any amount of labels can be added as wanted, and they will all cascade down to all resources. false liveness\tobject Liveness probes define a resource that returns 200 OK when the app is running as intended. Returning a non-200 code will make kubernetes restart the app. Liveness is optional, but when provided, path and port are required See Probe for structure definition. false maskinporten\tobject Settings for Maskinporten integration with Digitaliseringsdirektoratet false podSettings\tobject PodSettings are used to apply specific settings to the Pod Template used by Skiperator to create Deployments. This allows you to set things like annotations on the Pod to change the behaviour of sidecars, and set relevant Pod options such as TerminationGracePeriodSeconds. false priority\tenum An optional priority. Supported values are 'low', 'medium' and 'high'. The default value is 'medium'. Most workloads should not have to specify this field. If you think you do, please consult with SKIP beforehand. Enum: low, medium, high Default: medium false prometheus\tobject Optional settings for how Prometheus compatible metrics should be scraped. false readiness\tobject Readiness probes define a resource that returns 200 OK when the app is running as intended. Kubernetes will wait until the resource returns 200 OK before marking the pod as Running and progressing with the deployment strategy. Readiness is optional, but when provided, path and port are required false redirectToHTTPS\tboolean Controls whether the application will automatically redirect all HTTP calls to HTTPS via the istio VirtualService. This redirect does not happen on the route /.well-known/acme-challenge/, as the ACME challenge can only be done on port 80. Default: true false replicas\tJSON The number of replicas can either be specified as a static number as follows: replicas: 2 Or by specifying a range between min and max to enable HorizontalPodAutoscaling. The default value for replicas is: replicas: min: 2 max: 5 targetCpuUtilization: 80 Using autoscaling is the recommended configuration for replicas. false resourceLabels\tmap[string]map[string]string ResourceLabels can be used if you want to add a label to a specific resources created by the application. One such label could for example be set on a Deployment, such that the deployment avoids certain rules from Gatekeeper, or similar. Any amount of labels may be added per ResourceLabels item. false resources\tobject ResourceRequirements to apply to the deployment. It's common to set some of these to prevent the app from swelling in resource usage and consuming all the resources of other apps on the cluster. false startup\tobject Kubernetes uses startup probes to know when a container application has started. If such a probe is configured, it disables liveness and readiness checks until it succeeds, making sure those probes don't interfere with the application startup. This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by Kubernetes before they are up and running. Startup is optional, but when provided, path and port are required false strategy\tobject Defines an alternative strategy for the Kubernetes deployment. This is useful when the default strategy, RollingUpdate, is not usable. Setting type to Recreate will take down all the pods before starting new pods, whereas the default of RollingUpdate will try to start the new pods before taking down the old ones. Valid values are: RollingUpdate, Recreate. Default is RollingUpdate false team\tstring Team specifies the team who owns this particular app. Usually sourced from the namespace label. false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.accessPolicy​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecaccesspolicy","content":" ↩ Parent  The root AccessPolicy for managing zero trust access to your Application. See AccessPolicy for more information.  Name\tType\tDescription\tRequiredinbound\tobject Inbound specifies the ingress rules. Which apps on the cluster can talk to this app? false outbound\tobject Outbound specifies egress rules. Which apps on the cluster and the internet is the Application allowed to send requests to? false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.accessPolicy.inbound​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecaccesspolicyinbound","content":" ↩ Parent  Inbound specifies the ingress rules. Which apps on the cluster can talk to this app?  Name\tType\tDescription\tRequiredrules\t[]object The rules list specifies a list of applications. When no namespace is specified it refers to an app in the current namespace. For apps in other namespaces namespace is required true  ","version":"Next","tagName":"h3"},{"title":"Application.spec.accessPolicy.inbound.rules[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecaccesspolicyinboundrulesindex","content":" ↩ Parent  InternalRule  The rules list specifies a list of applications. When no namespace is specified it refers to an app in the current namespace. For apps in other namespaces, namespace is required.  Name\tType\tDescription\tRequiredapplication\tstring The name of the Application you are allowing traffic to/from. If you wish to allow traffic from a SKIPJob, this field should be suffixed with -skipjob true namespace\tstring The namespace in which the Application you are allowing traffic to/from resides. If unset, uses namespace of Application. false namespacesByLabel\tmap[string]string Namespace label value-pair in which the Application you are allowing traffic to/from resides. If both namespace and namespacesByLabel are set, namespace takes precedence and namespacesByLabel is omitted. false ports\t[]object The ports to allow for the above application. false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.accessPolicy.inbound.rules[index].ports[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecaccesspolicyinboundrulesindexportsindex","content":" ↩ Parent  NetworkPolicyPort describes a port to allow traffic on  Name\tType\tDescription\tRequiredendPort\tinteger endPort indicates that the range of ports from port to endPort if set, inclusive, should be allowed by the policy. This field cannot be defined if the port field is not defined or if the port field is defined as a named (string) port. The endPort must be equal or greater than port. Format: int32 false port\tint or string port represents the port on the given protocol. This can either be a numerical or named port on a pod. If this field is not provided, this matches all port names and numbers. If present, only traffic on the specified protocol AND port will be matched. false protocol\tstring protocol represents the protocol (TCP, UDP, or SCTP) which traffic must match. If not specified, this field defaults to TCP. false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.accessPolicy.outbound​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecaccesspolicyoutbound","content":" ↩ Parent  Outbound specifies egress rules. Which apps on the cluster and the internet is the Application allowed to send requests to?  Name\tType\tDescription\tRequiredexternal\t[]object External specifies which applications on the internet the application can reach. Only host is required unless it is on another port than HTTPS port 443. If other ports or protocols are required then ports must be specified as well false rules\t[]object Rules apply the same in-cluster rules as InboundPolicy false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.accessPolicy.outbound.external[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecaccesspolicyoutboundexternalindex","content":" ↩ Parent  ExternalRule  Describes a rule for allowing your Application to route traffic to external applications and hosts.  Name\tType\tDescription\tRequiredhost\tstring The allowed hostname. Note that this does not include subdomains. true ip\tstring Non-HTTP requests (i.e. using the TCP protocol) need to use IP in addition to hostname Only required for TCP requests. Note: Hostname must always be defined even if IP is set statically false ports\t[]object The ports to allow for the above hostname. When not specified HTTP and HTTPS on port 80 and 443 respectively are put into the allowlist false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.accessPolicy.outbound.external[index].ports[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecaccesspolicyoutboundexternalindexportsindex","content":" ↩ Parent  ExternalPort  A custom port describing an external host  Name\tType\tDescription\tRequiredname\tstring Name is required and is an arbitrary name. Must be unique within all ExternalRule ports. true port\tinteger The port number of the external host true protocol\tenum The protocol to use for communication with the host. Supported protocols are: HTTP, HTTPS, TCP and TLS. Enum: HTTP, HTTPS, TCP, TLS true  ","version":"Next","tagName":"h3"},{"title":"Application.spec.accessPolicy.outbound.rules[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecaccesspolicyoutboundrulesindex","content":" ↩ Parent  InternalRule  The rules list specifies a list of applications. When no namespace is specified it refers to an app in the current namespace. For apps in other namespaces, namespace is required.  Name\tType\tDescription\tRequiredapplication\tstring The name of the Application you are allowing traffic to/from. If you wish to allow traffic from a SKIPJob, this field should be suffixed with -skipjob true namespace\tstring The namespace in which the Application you are allowing traffic to/from resides. If unset, uses namespace of Application. false namespacesByLabel\tmap[string]string Namespace label value-pair in which the Application you are allowing traffic to/from resides. If both namespace and namespacesByLabel are set, namespace takes precedence and namespacesByLabel is omitted. false ports\t[]object The ports to allow for the above application. false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.accessPolicy.outbound.rules[index].ports[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecaccesspolicyoutboundrulesindexportsindex","content":" ↩ Parent  NetworkPolicyPort describes a port to allow traffic on  Name\tType\tDescription\tRequiredendPort\tinteger endPort indicates that the range of ports from port to endPort if set, inclusive, should be allowed by the policy. This field cannot be defined if the port field is not defined or if the port field is defined as a named (string) port. The endPort must be equal or greater than port. Format: int32 false port\tint or string port represents the port on the given protocol. This can either be a numerical or named port on a pod. If this field is not provided, this matches all port names and numbers. If present, only traffic on the specified protocol AND port will be matched. false protocol\tstring protocol represents the protocol (TCP, UDP, or SCTP) which traffic must match. If not specified, this field defaults to TCP. false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.additionalPorts[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecadditionalportsindex","content":" ↩ Parent  Name\tType\tDescription\tRequiredname\tstring true port\tinteger Format: int32 true protocol\tenum Protocol defines network protocols supported for things like container ports. Enum: TCP, UDP, SCTP true  ","version":"Next","tagName":"h3"},{"title":"Application.spec.authorizationSettings​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecauthorizationsettings","content":" ↩ Parent  Used for allow listing certain default blocked endpoints, such as /actuator/ end points  Name\tType\tDescription\tRequiredallowAll\tboolean Allows all endpoints by not creating an AuthorizationPolicy, and ignores the content of AllowList. If field is false, the contents of AllowList will be used instead if AllowList is set. Default: false false allowList\t[]string Allows specific endpoints. Common endpoints one might want to allow include /actuator/health, /actuator/startup, /actuator/info. Note that endpoints are matched specifically on the input, so if you allow /actuator/health, you will not allow /actuator/health/ false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.env[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecenvindex","content":" ↩ Parent  EnvVar represents an environment variable present in a Container.  Name\tType\tDescription\tRequiredname\tstring Name of the environment variable. Must be a C_IDENTIFIER. true value\tstring Variable references $(VAR_NAME) are expanded using the previously defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. &quot;$$(VAR_NAME)&quot; will produce the string literal &quot;$(VAR_NAME)&quot;. Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to &quot;&quot;. false valueFrom\tobject Source for the environment variable's value. Cannot be used if value is not empty. false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.env[index].valueFrom​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecenvindexvaluefrom","content":" ↩ Parent  Source for the environment variable's value. Cannot be used if value is not empty.  Name\tType\tDescription\tRequiredconfigMapKeyRef\tobject Selects a key of a ConfigMap. false fieldRef\tobject Selects a field of the pod: supports metadata.name, metadata.namespace, metadata.labels['&lt;KEY&gt;'], metadata.annotations['&lt;KEY&gt;'], spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs. false resourceFieldRef\tobject Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported. false secretKeyRef\tobject Selects a key of a secret in the pod's namespace false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.env[index].valueFrom.configMapKeyRef​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecenvindexvaluefromconfigmapkeyref","content":" ↩ Parent  Selects a key of a ConfigMap.  Name\tType\tDescription\tRequiredkey\tstring The key to select. true name\tstring Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names Default: false optional\tboolean Specify whether the ConfigMap or its key must be defined false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.env[index].valueFrom.fieldRef​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecenvindexvaluefromfieldref","content":" ↩ Parent  Selects a field of the pod: supports metadata.name, metadata.namespace, metadata.labels['&lt;KEY&gt;'], metadata.annotations['&lt;KEY&gt;'], spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.  Name\tType\tDescription\tRequiredfieldPath\tstring Path of the field to select in the specified API version. true apiVersion\tstring Version of the schema the FieldPath is written in terms of, defaults to &quot;v1&quot;. false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.env[index].valueFrom.resourceFieldRef​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecenvindexvaluefromresourcefieldref","content":" ↩ Parent  Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.  Name\tType\tDescription\tRequiredresource\tstring Required: resource to select true containerName\tstring Container name: required for volumes, optional for env vars false divisor\tint or string Specifies the output format of the exposed resources, defaults to &quot;1&quot; false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.env[index].valueFrom.secretKeyRef​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecenvindexvaluefromsecretkeyref","content":" ↩ Parent  Selects a key of a secret in the pod's namespace  Name\tType\tDescription\tRequiredkey\tstring The key of the secret to select from. Must be a valid secret key. true name\tstring Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names Default: false optional\tboolean Specify whether the Secret or its key must be defined false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.envFrom[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecenvfromindex","content":" ↩ Parent  Name\tType\tDescription\tRequiredconfigMap\tstring Name of Kubernetes ConfigMap in which the deployment should mount environment variables from. Must be in the same namespace as the Application false secret\tstring Name of Kubernetes Secret in which the deployment should mount environment variables from. Must be in the same namespace as the Application false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.filesFrom[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecfilesfromindex","content":" ↩ Parent  FilesFrom  Struct representing information needed to mount a Kubernetes resource as a file to a Pod's directory. One of ConfigMap, Secret, EmptyDir or PersistentVolumeClaim must be present, and just represent the name of the resource in question NB. Out-of-the-box, skiperator provides a writable 'emptyDir'-volume at '/tmp'  Name\tType\tDescription\tRequiredmountPath\tstring The path to mount the file in the Pods directory. Required. true configMap\tstring false defaultMode\tinteger defaultMode is optional: mode bits used to set permissions on created files by default. Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal values for mode bits. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. false emptyDir\tstring false persistentVolumeClaim\tstring false secret\tstring false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.gcp​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecgcp","content":" ↩ Parent  GCP is used to configure Google Cloud Platform specific settings for the application.  Name\tType\tDescription\tRequiredauth\tobject Configuration for authenticating a Pod with Google Cloud Platform For authentication with GCP, to use services like Secret Manager and/or Pub/Sub we need to set the GCP Service Account Pods should identify as. To allow this, we need the IAM role iam.workloadIdentityUser set on a GCP service account and bind this to the Pod's Kubernetes SA. Documentation on how this is done can be found here (Closed Wiki):https://kartverket.atlassian.net/wiki/spaces/SKIPDOK/pages/422346824/Autentisering+mot+GCP+som+Kubernetes+SA false cloudSqlProxy\tobject CloudSQL is used to deploy a CloudSQL proxy sidecar in the pod. This is useful for connecting to CloudSQL databases that require Cloud SQL Auth Proxy. false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.gcp.auth​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecgcpauth","content":" ↩ Parent  Configuration for authenticating a Pod with Google Cloud Platform For authentication with GCP, to use services like Secret Manager and/or Pub/Sub we need to set the GCP Service Account Pods should identify as. To allow this, we need the IAM role iam.workloadIdentityUser set on a GCP service account and bind this to the Pod's Kubernetes SA. Documentation on how this is done can be found here (Closed Wiki):https://kartverket.atlassian.net/wiki/spaces/SKIPDOK/pages/422346824/Autentisering+mot+GCP+som+Kubernetes+SA  Name\tType\tDescription\tRequiredserviceAccount\tstring Name of the service account in which you are trying to authenticate your pod with Generally takes the form of some-name@some-project-id.iam.gserviceaccount.com true  ","version":"Next","tagName":"h3"},{"title":"Application.spec.gcp.cloudSqlProxy​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecgcpcloudsqlproxy","content":" ↩ Parent  CloudSQL is used to deploy a CloudSQL proxy sidecar in the pod. This is useful for connecting to CloudSQL databases that require Cloud SQL Auth Proxy.  Name\tType\tDescription\tRequiredconnectionName\tstring Connection name for the CloudSQL instance. Found in the Google Cloud Console under your CloudSQL resource. The format is &quot;projectName:region:instanceName&quot; E.g. &quot;skip-prod-bda1:europe-north1:my-db&quot;. true ip\tstring The IP address of the CloudSQL instance. This is used to create a serviceentry for the CloudSQL proxy. true serviceAccount\tstring Service account used by cloudsql auth proxy. This service account must have the roles/cloudsql.client role. true publicIP\tboolean Default: false false version\tstring Image version for the CloudSQL proxy sidecar. false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.idporten​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecidporten","content":" ↩ Parent  Settings for IDPorten integration with Digitaliseringsdirektoratet  Name\tType\tDescription\tRequiredenabled\tboolean Whether to enable provisioning of an ID-porten client. If enabled, an ID-porten client will be provisioned. true accessTokenLifetime\tinteger AccessTokenLifetime is the lifetime in seconds for any issued access token from ID-porten. If unspecified, defaults to 3600 seconds (1 hour). Minimum: 1 Maximum: 3600 false clientName\tstring The name of the Client as shown in Digitaliseringsdirektoratet's Samarbeidsportal Meant to be a human-readable name for separating clients in the portal. false clientURI\tstring ClientURI is the URL shown to the user at ID-porten when displaying a 'back' button or on errors. false frontchannelLogoutPath\tstring FrontchannelLogoutPath is a valid path for your application where ID-porten sends a request to whenever the user has initiated a logout elsewhere as part of a single logout (front channel logout) process. false integrationType\tenum IntegrationType is used to make sensible choices for your client. Which type of integration you choose will provide guidance on which scopes you can use with the client. A client can only have one integration type. NB! It is not possible to change the integration type after creation. Enum: krr, idporten, api_klient false postLogoutRedirectPath\tstring PostLogoutRedirectPath is a simpler verison of PostLogoutRedirectURIs that will be appended to the ingress false postLogoutRedirectURIs\t[]string PostLogoutRedirectURIs are valid URIs that ID-porten will allow redirecting the end-user to after a single logout has been initiated and performed by the application. false redirectPath\tstring RedirectPath is a valid path that ID-porten redirects back to after a successful authorization request. false requestAuthentication\tobject RequestAuthentication specifies how incoming JWTs should be validated. false scopes\t[]string Register different oauth2 Scopes on your client. You will not be able to add a scope to your client that conflicts with the client's IntegrationType. For example, you can not add a scope that is limited to the IntegrationType krr of IntegrationType idporten, and vice versa. Default for IntegrationType krr = (&quot;krr:global/kontaktinformasjon.read&quot;, &quot;krr:global/digitalpost.read&quot;) Default for IntegrationType idporten = (&quot;openid&quot;, &quot;profile&quot;) IntegrationType api_klient have no Default, checkout Digdir documentation. false sessionLifetime\tinteger SessionLifetime is the maximum lifetime in seconds for any given user's session in your application. The timeout starts whenever the user is redirected from the authorization_endpoint at ID-porten. If unspecified, defaults to 7200 seconds (2 hours). Note: Attempting to refresh the user's access_token beyond this timeout will yield an error. Minimum: 3600 Maximum: 7200 false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.idporten.requestAuthentication​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecidportenrequestauthentication","content":" ↩ Parent  RequestAuthentication specifies how incoming JWTs should be validated.  Name\tType\tDescription\tRequiredenabled\tboolean Whether to enable JWT validation. If enabled, incoming JWTs will be validated against the issuer specified in the app registration and the generated audience. true forwardJwt\tboolean If set to true, the original token will be kept for the upstream request. Defaults to true. Default: true false ignorePaths\t[]string IgnorePaths specifies paths that do not require an authenticated JWT. The specified paths must be a valid URI path. It has to start with '/' and cannot end with '/'. The paths can also contain the wildcard operator '*', but only at the end. false outputClaimToHeaders\t[]object This field specifies a list of operations to copy the claim to HTTP headers on a successfully verified token. The header specified in each operation in the list must be unique. Nested claims of type string/int/bool is supported as well. false paths\t[]string Paths specifies paths that require an authenticated JWT. The specified paths must be a valid URI path. It has to start with '/' and cannot end with '/'. The paths can also contain the wildcard operator '*', but only at the end. false secretName\tstring The name of the Kubernetes Secret containing OAuth2 credentials. If omitted, the associated client registration in the application manifest is used for JWT validation. false tokenLocation\tenum Where to find the JWT in the incoming request An enum value of header means that the JWT is present in the Authorization header as a Bearer token. An enum value of cookie means that the JWT is present as a cookie called BearerToken. If omitted, its default value depends on the provider type: Defaults to &quot;cookie&quot; for providers supporting user login (e.g. IDPorten). Defaults to &quot;header&quot; for providers not supporting user login (e.g. Maskinporten). Enum: header, cookie false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.idporten.requestAuthentication.outputClaimToHeaders[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecidportenrequestauthenticationoutputclaimtoheadersindex","content":" ↩ Parent  Name\tType\tDescription\tRequiredclaim\tstring The claim to be copied. true header\tstring The name of the HTTP header for which the specified claim will be copied to. true  ","version":"Next","tagName":"h3"},{"title":"Application.spec.istioSettings​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecistiosettings","content":" ↩ Parent  IstioSettings are used to configure istio specific resources such as telemetry. Currently, adjusting sampling interval for tracing is the only supported option. By default, tracing is enabled with a random sampling percentage of 10%.  Name\tType\tDescription\tRequiredretries\tobject Retries is configurable automatic retries for requests towards the application. By default requests falling under: &quot;connect-failure,refused-stream,unavailable,cancelled,5xx&quot; will be retried. false telemetry\tobject Telemetry is a placeholder for all relevant telemetry types, and may be extended in the future to configure additional telemetry settings. Default: map[tracing:[map[randomSamplingPercentage:10]]] false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.istioSettings.retries​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecistiosettingsretries","content":" ↩ Parent  Retries is configurable automatic retries for requests towards the application. By default requests falling under: &quot;connect-failure,refused-stream,unavailable,cancelled,5xx&quot; will be retried.  Name\tType\tDescription\tRequiredattempts\tinteger Attempts is the number of retries to be allowed for a given request before giving up. The interval between retries will be determined automatically (25ms+). Default is 2 Format: int32 Minimum: 1 false perTryTimeout\tstring PerTryTimeout is the timeout per attempt for a given request, including the initial call and any retries. Format: 1h/1m/1s/1ms. MUST be &gt;=1ms. Default: no timeout Format: duration false retryOnHttpResponseCodes\t[]int or string RetryOnHttpResponseCodes HTTP response codes that should trigger a retry. A typical value is [503]. You may also use 5xx and retriable-4xx (only 409). false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.istioSettings.telemetry​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecistiosettingstelemetry","content":" ↩ Parent  Telemetry is a placeholder for all relevant telemetry types, and may be extended in the future to configure additional telemetry settings.  Name\tType\tDescription\tRequiredtracing\t[]object Tracing is a list of tracing configurations for the telemetry resource. Normally only one tracing configuration is needed. Default: [map[randomSamplingPercentage:10]] false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.istioSettings.telemetry.tracing[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecistiosettingstelemetrytracingindex","content":" ↩ Parent  Tracing contains relevant settings for tracing in the telemetry configuration  Name\tType\tDescription\tRequiredrandomSamplingPercentage\tinteger RandomSamplingPercentage is the percentage of requests that should be sampled for tracing, specified by a whole number between 0-100. Setting RandomSamplingPercentage to 0 will disable tracing. Default: 10 Minimum: 0 Maximum: 100 false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.liveness​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecliveness","content":" ↩ Parent  Liveness probes define a resource that returns 200 OK when the app is running as intended. Returning a non-200 code will make kubernetes restart the app. Liveness is optional, but when provided, path and port are required  See Probe for structure definition.  Name\tType\tDescription\tRequiredpath\tstring The path to access on the HTTP server true port\tint or string Number of the port to access on the container true failureThreshold\tinteger Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1 Format: int32 Default: 3 false initialDelay\tinteger Delay sending the first probe by X seconds. Can be useful for applications that are slow to start. Format: int32 Default: 0 false period\tinteger Number of seconds Kubernetes waits between each probe. Defaults to 10 seconds. Format: int32 Default: 10 false successThreshold\tinteger Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup Probes. Minimum value is 1. Format: int32 Default: 1 false timeout\tinteger Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1 Format: int32 Default: 1 false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.maskinporten​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecmaskinporten","content":" ↩ Parent  Settings for Maskinporten integration with Digitaliseringsdirektoratet  Name\tType\tDescription\tRequiredenabled\tboolean If enabled, provisions and configures a Maskinporten client with consumed scopes and/or Exposed scopes with DigDir. true clientName\tstring The name of the Client as shown in Digitaliseringsdirektoratet's Samarbeidsportal Meant to be a human-readable name for separating clients in the portal false requestAuthentication\tobject RequestAuthentication specifies how incoming JWTs should be validated. false scopes\tobject Schema to configure Maskinporten clients with consumed scopes and/or exposed scopes. false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.maskinporten.requestAuthentication​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecmaskinportenrequestauthentication","content":" ↩ Parent  RequestAuthentication specifies how incoming JWTs should be validated.  Name\tType\tDescription\tRequiredenabled\tboolean Whether to enable JWT validation. If enabled, incoming JWTs will be validated against the issuer specified in the app registration and the generated audience. true forwardJwt\tboolean If set to true, the original token will be kept for the upstream request. Defaults to true. Default: true false ignorePaths\t[]string IgnorePaths specifies paths that do not require an authenticated JWT. The specified paths must be a valid URI path. It has to start with '/' and cannot end with '/'. The paths can also contain the wildcard operator '*', but only at the end. false outputClaimToHeaders\t[]object This field specifies a list of operations to copy the claim to HTTP headers on a successfully verified token. The header specified in each operation in the list must be unique. Nested claims of type string/int/bool is supported as well. false paths\t[]string Paths specifies paths that require an authenticated JWT. The specified paths must be a valid URI path. It has to start with '/' and cannot end with '/'. The paths can also contain the wildcard operator '*', but only at the end. false secretName\tstring The name of the Kubernetes Secret containing OAuth2 credentials. If omitted, the associated client registration in the application manifest is used for JWT validation. false tokenLocation\tenum Where to find the JWT in the incoming request An enum value of header means that the JWT is present in the Authorization header as a Bearer token. An enum value of cookie means that the JWT is present as a cookie called BearerToken. If omitted, its default value depends on the provider type: Defaults to &quot;cookie&quot; for providers supporting user login (e.g. IDPorten). Defaults to &quot;header&quot; for providers not supporting user login (e.g. Maskinporten). Enum: header, cookie false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.maskinporten.requestAuthentication.outputClaimToHeaders[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecmaskinportenrequestauthenticationoutputclaimtoheadersindex","content":" ↩ Parent  Name\tType\tDescription\tRequiredclaim\tstring The claim to be copied. true header\tstring The name of the HTTP header for which the specified claim will be copied to. true  ","version":"Next","tagName":"h3"},{"title":"Application.spec.maskinporten.scopes​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecmaskinportenscopes","content":" ↩ Parent  Schema to configure Maskinporten clients with consumed scopes and/or exposed scopes.  Name\tType\tDescription\tRequiredconsumes\t[]object This is the Schema for the consumes and exposes API.consumes is a list of scopes that your client can request access to. false exposes\t[]object exposes is a list of scopes your application want to expose to other organization where access to the scope is based on organization number. false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.maskinporten.scopes.consumes[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecmaskinportenscopesconsumesindex","content":" ↩ Parent  Name\tType\tDescription\tRequiredname\tstring The scope consumed by the application to gain access to an external organization API. Ensure that the NAV organization has been granted access to the scope prior to requesting access. true  ","version":"Next","tagName":"h3"},{"title":"Application.spec.maskinporten.scopes.exposes[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecmaskinportenscopesexposesindex","content":" ↩ Parent  Name\tType\tDescription\tRequiredenabled\tboolean If Enabled the configured scope is available to be used and consumed by organizations granted access. true name\tstring The actual subscope combined with Product. Ensure that &lt;Product&gt;&lt;Name&gt; matches Pattern. true product\tstring The product-area your application belongs to e.g. arbeid, helse ... This will be included in the final scope nav:&lt;Product&gt;&lt;Name&gt;. true accessibleForAll\tboolean Allow any organization to access the scope. false allowedIntegrations\t[]string Whitelisting of integration's allowed. Default is maskinporten false atMaxAge\tinteger Max time in seconds for a issued access_token. Default is 30 sec. Minimum: 30 Maximum: 680 false consumers\t[]object External consumers granted access to this scope and able to request access_token. false delegationSource\tenum Delegation source for the scope. Default is empty, which means no delegation is allowed. Enum: altinn false separator\tstring Separator is the character that separates product and name in the final scope:scope := &lt;prefix&gt;:&lt;product&gt;&lt;separator&gt;&lt;name&gt;This overrides the default separator. The default separator is :. If name contains /, the default separator is instead /. false visibility\tenum Visibility controls the scope's visibility. Public scopes are visible for everyone. Private scopes are only visible for the organization that owns the scope as well as organizations that have been granted consumer access. Enum: private, public false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.maskinporten.scopes.exposes[index].consumers[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecmaskinportenscopesexposesindexconsumersindex","content":" ↩ Parent  Name\tType\tDescription\tRequiredorgno\tstring The external business/organization number. true name\tstring This is a describing field intended for clarity not used for any other purpose. false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.podSettings​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecpodsettings","content":" ↩ Parent  PodSettings are used to apply specific settings to the Pod Template used by Skiperator to create Deployments. This allows you to set things like annotations on the Pod to change the behaviour of sidecars, and set relevant Pod options such as TerminationGracePeriodSeconds.  Name\tType\tDescription\tRequiredannotations\tmap[string]string Annotations that are set on Pods created by Skiperator. These annotations can for example be used to change the behaviour of sidecars and similar. false disablePodSpreadTopologyConstraints\tboolean DisablePodSpreadTopologyConstraints specifies whether to disable the addition of Pod Topology Spread Constraints to a given pod. Default: false false terminationGracePeriodSeconds\tinteger TerminationGracePeriodSeconds determines how long Kubernetes waits after a SIGTERM signal sent to a Pod before terminating the pod. If your application uses longer than 30 seconds to terminate, you should increase TerminationGracePeriodSeconds. Format: int64 Default: 30 false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.prometheus​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecprometheus","content":" ↩ Parent  Optional settings for how Prometheus compatible metrics should be scraped.  Name\tType\tDescription\tRequiredport\tint or string The port number or name where metrics are exposed (at the Pod level). true allowAllMetrics\tboolean Setting AllowAllMetrics to true will ensure all exposed metrics are scraped. Otherwise, a list of predefined metrics will be dropped by default. See util/constants.go for the default list. Default: false false path\tstring The HTTP path where Prometheus compatible metrics exists Default: /metrics false scrapeInterval\tstring ScrapeInterval specifies the interval at which Prometheus should scrape the metrics. The interval must be at least 15 seconds (if using &quot;Xs&quot;) and divisible by 5. If minutes (&quot;Xm&quot;) are used, the value must be at least 1m. Default: 60s false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.readiness​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecreadiness","content":" ↩ Parent  Readiness probes define a resource that returns 200 OK when the app is running as intended. Kubernetes will wait until the resource returns 200 OK before marking the pod as Running and progressing with the deployment strategy. Readiness is optional, but when provided, path and port are required  Name\tType\tDescription\tRequiredpath\tstring The path to access on the HTTP server true port\tint or string Number of the port to access on the container true failureThreshold\tinteger Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1 Format: int32 Default: 3 false initialDelay\tinteger Delay sending the first probe by X seconds. Can be useful for applications that are slow to start. Format: int32 Default: 0 false period\tinteger Number of seconds Kubernetes waits between each probe. Defaults to 10 seconds. Format: int32 Default: 10 false successThreshold\tinteger Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup Probes. Minimum value is 1. Format: int32 Default: 1 false timeout\tinteger Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1 Format: int32 Default: 1 false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.resources​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecresources","content":" ↩ Parent  ResourceRequirements to apply to the deployment. It's common to set some of these to prevent the app from swelling in resource usage and consuming all the resources of other apps on the cluster.  Name\tType\tDescription\tRequiredlimits\tmap[string]int or string Limits set the maximum the app is allowed to use. Exceeding this limit will make kubernetes kill the app and restart it. Limits can be set on the CPU and memory, but it is not recommended to put a limit on CPU, see: https://home.robusta.dev/blog/stop-using-cpu-limits false requests\tmap[string]int or string Requests set the initial allocation that is done for the app and will thus be available to the app on startup. More is allocated on demand until the limit is reached. Requests can be set on the CPU and memory. false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.startup​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecstartup","content":" ↩ Parent  Kubernetes uses startup probes to know when a container application has started. If such a probe is configured, it disables liveness and readiness checks until it succeeds, making sure those probes don't interfere with the application startup. This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by Kubernetes before they are up and running. Startup is optional, but when provided, path and port are required  Name\tType\tDescription\tRequiredpath\tstring The path to access on the HTTP server true port\tint or string Number of the port to access on the container true failureThreshold\tinteger Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1 Format: int32 Default: 3 false initialDelay\tinteger Delay sending the first probe by X seconds. Can be useful for applications that are slow to start. Format: int32 Default: 0 false period\tinteger Number of seconds Kubernetes waits between each probe. Defaults to 10 seconds. Format: int32 Default: 10 false successThreshold\tinteger Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup Probes. Minimum value is 1. Format: int32 Default: 1 false timeout\tinteger Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1 Format: int32 Default: 1 false  ","version":"Next","tagName":"h3"},{"title":"Application.spec.strategy​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationspecstrategy","content":" ↩ Parent  Defines an alternative strategy for the Kubernetes deployment. This is useful when the default strategy, RollingUpdate, is not usable. Setting type to Recreate will take down all the pods before starting new pods, whereas the default of RollingUpdate will try to start the new pods before taking down the old ones.  Valid values are: RollingUpdate, Recreate. Default is RollingUpdate  Name\tType\tDescription\tRequiredtype\tenum Valid values are: RollingUpdate, Recreate. Default is RollingUpdate Enum: RollingUpdate, Recreate Default: RollingUpdate false  ","version":"Next","tagName":"h3"},{"title":"Application.status​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationstatus","content":" ↩ Parent  SkiperatorStatus  A status field shown on a Skiperator resource which contains information regarding deployment of the resource.  Name\tType\tDescription\tRequiredaccessPolicies\tstring Indicates if access policies are valid true conditions\t[]object true subresources\tmap[string]object true summary\tobject Status true  ","version":"Next","tagName":"h3"},{"title":"Application.status.conditions[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationstatusconditionsindex","content":" ↩ Parent  Condition contains details for one aspect of the current state of this API Resource.  Name\tType\tDescription\tRequiredlastTransitionTime\tstring lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message\tstring message is a human readable message indicating details about the transition. This may be an empty string. true reason\tstring reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status\tenum status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type\tstring type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration\tinteger observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false  ","version":"Next","tagName":"h3"},{"title":"Application.status.subresources[key]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationstatussubresourceskey","content":" ↩ Parent  Status  Name\tType\tDescription\tRequiredmessage\tstring Default: hello true status\tstring Default: Synced true timestamp\tstring Default: hello true  ","version":"Next","tagName":"h3"},{"title":"Application.status.summary​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#applicationstatussummary","content":" ↩ Parent  Status  Name\tType\tDescription\tRequiredmessage\tstring Default: hello true status\tstring Default: Synced true timestamp\tstring Default: hello true  ","version":"Next","tagName":"h3"},{"title":"Routing​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#routing","content":" ↩ Parent  Name\tType\tDescription\tRequiredapiVersion\tstring\tskiperator.kartverket.no/v1alpha1\ttrue kind\tstring\tRouting\ttrue metadata\tobject\tRefer to the Kubernetes API documentation for the fields of the metadata field.\ttrue spec\tobject true status\tobject SkiperatorStatus A status field shown on a Skiperator resource which contains information regarding deployment of the resource. false  ","version":"Next","tagName":"h2"},{"title":"Routing.spec​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#routingspec","content":" ↩ Parent  Name\tType\tDescription\tRequiredhostname\tstring true routes\t[]object true redirectToHTTPS\tboolean Default: true false  ","version":"Next","tagName":"h3"},{"title":"Routing.spec.routes[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#routingspecroutesindex","content":" ↩ Parent  Name\tType\tDescription\tRequiredpathPrefix\tstring true targetApp\tstring true port\tinteger Format: int32 false rewriteUri\tboolean Default: false false  ","version":"Next","tagName":"h3"},{"title":"Routing.status​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#routingstatus","content":" ↩ Parent  SkiperatorStatus  A status field shown on a Skiperator resource which contains information regarding deployment of the resource.  Name\tType\tDescription\tRequiredaccessPolicies\tstring Indicates if access policies are valid true conditions\t[]object true subresources\tmap[string]object true summary\tobject Status true  ","version":"Next","tagName":"h3"},{"title":"Routing.status.conditions[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#routingstatusconditionsindex","content":" ↩ Parent  Condition contains details for one aspect of the current state of this API Resource.  Name\tType\tDescription\tRequiredlastTransitionTime\tstring lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message\tstring message is a human readable message indicating details about the transition. This may be an empty string. true reason\tstring reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status\tenum status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type\tstring type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration\tinteger observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false  ","version":"Next","tagName":"h3"},{"title":"Routing.status.subresources[key]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#routingstatussubresourceskey","content":" ↩ Parent  Status  Name\tType\tDescription\tRequiredmessage\tstring Default: hello true status\tstring Default: Synced true timestamp\tstring Default: hello true  ","version":"Next","tagName":"h3"},{"title":"Routing.status.summary​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#routingstatussummary","content":" ↩ Parent  Status  Name\tType\tDescription\tRequiredmessage\tstring Default: hello true status\tstring Default: Synced true timestamp\tstring Default: hello true  ","version":"Next","tagName":"h3"},{"title":"SKIPJob​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjob","content":" ↩ Parent  SKIPJob is the Schema for the skipjobs API  Name\tType\tDescription\tRequiredapiVersion\tstring\tskiperator.kartverket.no/v1alpha1\ttrue kind\tstring\tSKIPJob\ttrue metadata\tobject\tRefer to the Kubernetes API documentation for the fields of the metadata field.\ttrue spec\tobject SKIPJobSpec defines the desired state of SKIPJob A SKIPJob is either defined as a one-off or a scheduled job. If the Cron field is set for SKIPJob, it may not be removed. If the Cron field is unset, it may not be added. The Container field of a SKIPJob is only mutable if the Cron field is set. If unset, you must delete your SKIPJob to change container settings. true status\tobject SkiperatorStatus A status field shown on a Skiperator resource which contains information regarding deployment of the resource. false  ","version":"Next","tagName":"h2"},{"title":"SKIPJob.spec​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspec","content":" ↩ Parent  SKIPJobSpec defines the desired state of SKIPJob  A SKIPJob is either defined as a one-off or a scheduled job. If the Cron field is set for SKIPJob, it may not be removed. If the Cron field is unset, it may not be added. The Container field of a SKIPJob is only mutable if the Cron field is set. If unset, you must delete your SKIPJob to change container settings.  Name\tType\tDescription\tRequiredcontainer\tobject Settings for the Pods running in the job. Fields are mostly the same as an Application, and are (probably) better documented there. Some fields are omitted, but none added. Once set, you may not change Container without deleting your current SKIPJob true cron\tobject Settings for the Job if you are running a scheduled job. Optional as Jobs may be one-off. false istioSettings\tobject IstioSettings are used to configure istio specific resources such as telemetry. Currently, adjusting sampling interval for tracing is the only supported option. By default, tracing is enabled with a random sampling percentage of 10%. Default: map[telemetry tracing:[map[randomSamplingPercentage:10]] ] false job\tobject Settings for the actual Job. If you use a scheduled job, the settings in here will also specify the template of the job. false prometheus\tobject Prometheus settings for pod running in job. Fields are identical to Application and if set, a podmonitoring object is created. false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainer","content":" ↩ Parent  Settings for the Pods running in the job. Fields are mostly the same as an Application, and are (probably) better documented there. Some fields are omitted, but none added. Once set, you may not change Container without deleting your current SKIPJob  Name\tType\tDescription\tRequiredimage\tstring true accessPolicy\tobject AccessPolicy Zero trust dictates that only applications with a reason for being able to access another resource should be able to reach it. This is set up by default by denying all ingress and egress traffic from the Pods in the Deployment. The AccessPolicy field is an allowlist of other applications and hostnames that are allowed to talk with this Application and which resources this app can talk to false additionalPorts\t[]object false command\t[]string false env\t[]object false envFrom\t[]object false filesFrom\t[]object false gcp\tobject GCP Configuration for interacting with Google Cloud Platform false liveness\tobject Probe Type configuration for all types of Kubernetes probes. false podSettings\tobject PodSettings false priority\tenum Enum: low, medium, high Default: medium false readiness\tobject Probe Type configuration for all types of Kubernetes probes. false resources\tobject ResourceRequirements A simplified version of the Kubernetes native ResourceRequirement field, in which only Limits and Requests are present. For the units used for resources, see https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes false restartPolicy\tenum RestartPolicy describes how the container should be restarted. Only one of the following restart policies may be specified. If none of the following policies is specified, the default one is RestartPolicyAlways. Enum: OnFailure, Never Default: Never false startup\tobject Probe Type configuration for all types of Kubernetes probes. false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.accessPolicy​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontaineraccesspolicy","content":" ↩ Parent  AccessPolicy  Zero trust dictates that only applications with a reason for being able to access another resource should be able to reach it. This is set up by default by denying all ingress and egress traffic from the Pods in the Deployment. The AccessPolicy field is an allowlist of other applications and hostnames that are allowed to talk with this Application and which resources this app can talk to  Name\tType\tDescription\tRequiredinbound\tobject Inbound specifies the ingress rules. Which apps on the cluster can talk to this app? false outbound\tobject Outbound specifies egress rules. Which apps on the cluster and the internet is the Application allowed to send requests to? false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.accessPolicy.inbound​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontaineraccesspolicyinbound","content":" ↩ Parent  Inbound specifies the ingress rules. Which apps on the cluster can talk to this app?  Name\tType\tDescription\tRequiredrules\t[]object The rules list specifies a list of applications. When no namespace is specified it refers to an app in the current namespace. For apps in other namespaces namespace is required true  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.accessPolicy.inbound.rules[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontaineraccesspolicyinboundrulesindex","content":" ↩ Parent  InternalRule  The rules list specifies a list of applications. When no namespace is specified it refers to an app in the current namespace. For apps in other namespaces, namespace is required.  Name\tType\tDescription\tRequiredapplication\tstring The name of the Application you are allowing traffic to/from. If you wish to allow traffic from a SKIPJob, this field should be suffixed with -skipjob true namespace\tstring The namespace in which the Application you are allowing traffic to/from resides. If unset, uses namespace of Application. false namespacesByLabel\tmap[string]string Namespace label value-pair in which the Application you are allowing traffic to/from resides. If both namespace and namespacesByLabel are set, namespace takes precedence and namespacesByLabel is omitted. false ports\t[]object The ports to allow for the above application. false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.accessPolicy.inbound.rules[index].ports[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontaineraccesspolicyinboundrulesindexportsindex","content":" ↩ Parent  NetworkPolicyPort describes a port to allow traffic on  Name\tType\tDescription\tRequiredendPort\tinteger endPort indicates that the range of ports from port to endPort if set, inclusive, should be allowed by the policy. This field cannot be defined if the port field is not defined or if the port field is defined as a named (string) port. The endPort must be equal or greater than port. Format: int32 false port\tint or string port represents the port on the given protocol. This can either be a numerical or named port on a pod. If this field is not provided, this matches all port names and numbers. If present, only traffic on the specified protocol AND port will be matched. false protocol\tstring protocol represents the protocol (TCP, UDP, or SCTP) which traffic must match. If not specified, this field defaults to TCP. false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.accessPolicy.outbound​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontaineraccesspolicyoutbound","content":" ↩ Parent  Outbound specifies egress rules. Which apps on the cluster and the internet is the Application allowed to send requests to?  Name\tType\tDescription\tRequiredexternal\t[]object External specifies which applications on the internet the application can reach. Only host is required unless it is on another port than HTTPS port 443. If other ports or protocols are required then ports must be specified as well false rules\t[]object Rules apply the same in-cluster rules as InboundPolicy false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.accessPolicy.outbound.external[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontaineraccesspolicyoutboundexternalindex","content":" ↩ Parent  ExternalRule  Describes a rule for allowing your Application to route traffic to external applications and hosts.  Name\tType\tDescription\tRequiredhost\tstring The allowed hostname. Note that this does not include subdomains. true ip\tstring Non-HTTP requests (i.e. using the TCP protocol) need to use IP in addition to hostname Only required for TCP requests. Note: Hostname must always be defined even if IP is set statically false ports\t[]object The ports to allow for the above hostname. When not specified HTTP and HTTPS on port 80 and 443 respectively are put into the allowlist false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.accessPolicy.outbound.external[index].ports[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontaineraccesspolicyoutboundexternalindexportsindex","content":" ↩ Parent  ExternalPort  A custom port describing an external host  Name\tType\tDescription\tRequiredname\tstring Name is required and is an arbitrary name. Must be unique within all ExternalRule ports. true port\tinteger The port number of the external host true protocol\tenum The protocol to use for communication with the host. Supported protocols are: HTTP, HTTPS, TCP and TLS. Enum: HTTP, HTTPS, TCP, TLS true  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.accessPolicy.outbound.rules[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontaineraccesspolicyoutboundrulesindex","content":" ↩ Parent  InternalRule  The rules list specifies a list of applications. When no namespace is specified it refers to an app in the current namespace. For apps in other namespaces, namespace is required.  Name\tType\tDescription\tRequiredapplication\tstring The name of the Application you are allowing traffic to/from. If you wish to allow traffic from a SKIPJob, this field should be suffixed with -skipjob true namespace\tstring The namespace in which the Application you are allowing traffic to/from resides. If unset, uses namespace of Application. false namespacesByLabel\tmap[string]string Namespace label value-pair in which the Application you are allowing traffic to/from resides. If both namespace and namespacesByLabel are set, namespace takes precedence and namespacesByLabel is omitted. false ports\t[]object The ports to allow for the above application. false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.accessPolicy.outbound.rules[index].ports[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontaineraccesspolicyoutboundrulesindexportsindex","content":" ↩ Parent  NetworkPolicyPort describes a port to allow traffic on  Name\tType\tDescription\tRequiredendPort\tinteger endPort indicates that the range of ports from port to endPort if set, inclusive, should be allowed by the policy. This field cannot be defined if the port field is not defined or if the port field is defined as a named (string) port. The endPort must be equal or greater than port. Format: int32 false port\tint or string port represents the port on the given protocol. This can either be a numerical or named port on a pod. If this field is not provided, this matches all port names and numbers. If present, only traffic on the specified protocol AND port will be matched. false protocol\tstring protocol represents the protocol (TCP, UDP, or SCTP) which traffic must match. If not specified, this field defaults to TCP. false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.additionalPorts[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontaineradditionalportsindex","content":" ↩ Parent  Name\tType\tDescription\tRequiredname\tstring true port\tinteger Format: int32 true protocol\tenum Protocol defines network protocols supported for things like container ports. Enum: TCP, UDP, SCTP true  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.env[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainerenvindex","content":" ↩ Parent  EnvVar represents an environment variable present in a Container.  Name\tType\tDescription\tRequiredname\tstring Name of the environment variable. Must be a C_IDENTIFIER. true value\tstring Variable references $(VAR_NAME) are expanded using the previously defined environment variables in the container and any service environment variables. If a variable cannot be resolved, the reference in the input string will be unchanged. Double $$ are reduced to a single $, which allows for escaping the $(VAR_NAME) syntax: i.e. &quot;$$(VAR_NAME)&quot; will produce the string literal &quot;$(VAR_NAME)&quot;. Escaped references will never be expanded, regardless of whether the variable exists or not. Defaults to &quot;&quot;. false valueFrom\tobject Source for the environment variable's value. Cannot be used if value is not empty. false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.env[index].valueFrom​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainerenvindexvaluefrom","content":" ↩ Parent  Source for the environment variable's value. Cannot be used if value is not empty.  Name\tType\tDescription\tRequiredconfigMapKeyRef\tobject Selects a key of a ConfigMap. false fieldRef\tobject Selects a field of the pod: supports metadata.name, metadata.namespace, metadata.labels['&lt;KEY&gt;'], metadata.annotations['&lt;KEY&gt;'], spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs. false resourceFieldRef\tobject Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported. false secretKeyRef\tobject Selects a key of a secret in the pod's namespace false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.env[index].valueFrom.configMapKeyRef​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainerenvindexvaluefromconfigmapkeyref","content":" ↩ Parent  Selects a key of a ConfigMap.  Name\tType\tDescription\tRequiredkey\tstring The key to select. true name\tstring Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names Default: false optional\tboolean Specify whether the ConfigMap or its key must be defined false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.env[index].valueFrom.fieldRef​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainerenvindexvaluefromfieldref","content":" ↩ Parent  Selects a field of the pod: supports metadata.name, metadata.namespace, metadata.labels['&lt;KEY&gt;'], metadata.annotations['&lt;KEY&gt;'], spec.nodeName, spec.serviceAccountName, status.hostIP, status.podIP, status.podIPs.  Name\tType\tDescription\tRequiredfieldPath\tstring Path of the field to select in the specified API version. true apiVersion\tstring Version of the schema the FieldPath is written in terms of, defaults to &quot;v1&quot;. false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.env[index].valueFrom.resourceFieldRef​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainerenvindexvaluefromresourcefieldref","content":" ↩ Parent  Selects a resource of the container: only resources limits and requests (limits.cpu, limits.memory, limits.ephemeral-storage, requests.cpu, requests.memory and requests.ephemeral-storage) are currently supported.  Name\tType\tDescription\tRequiredresource\tstring Required: resource to select true containerName\tstring Container name: required for volumes, optional for env vars false divisor\tint or string Specifies the output format of the exposed resources, defaults to &quot;1&quot; false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.env[index].valueFrom.secretKeyRef​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainerenvindexvaluefromsecretkeyref","content":" ↩ Parent  Selects a key of a secret in the pod's namespace  Name\tType\tDescription\tRequiredkey\tstring The key of the secret to select from. Must be a valid secret key. true name\tstring Name of the referent. This field is effectively required, but due to backwards compatibility is allowed to be empty. Instances of this type with an empty value here are almost certainly wrong. More info: https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#names Default: false optional\tboolean Specify whether the Secret or its key must be defined false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.envFrom[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainerenvfromindex","content":" ↩ Parent  Name\tType\tDescription\tRequiredconfigMap\tstring Name of Kubernetes ConfigMap in which the deployment should mount environment variables from. Must be in the same namespace as the Application false secret\tstring Name of Kubernetes Secret in which the deployment should mount environment variables from. Must be in the same namespace as the Application false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.filesFrom[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainerfilesfromindex","content":" ↩ Parent  FilesFrom  Struct representing information needed to mount a Kubernetes resource as a file to a Pod's directory. One of ConfigMap, Secret, EmptyDir or PersistentVolumeClaim must be present, and just represent the name of the resource in question NB. Out-of-the-box, skiperator provides a writable 'emptyDir'-volume at '/tmp'  Name\tType\tDescription\tRequiredmountPath\tstring The path to mount the file in the Pods directory. Required. true configMap\tstring false defaultMode\tinteger defaultMode is optional: mode bits used to set permissions on created files by default. Must be an octal value between 0000 and 0777 or a decimal value between 0 and 511. YAML accepts both octal and decimal values, JSON requires decimal values for mode bits. Defaults to 0644. Directories within the path are not affected by this setting. This might be in conflict with other options that affect the file mode, like fsGroup, and the result can be other mode bits set. false emptyDir\tstring false persistentVolumeClaim\tstring false secret\tstring false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.gcp​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainergcp","content":" ↩ Parent  GCP  Configuration for interacting with Google Cloud Platform  Name\tType\tDescription\tRequiredauth\tobject Configuration for authenticating a Pod with Google Cloud Platform For authentication with GCP, to use services like Secret Manager and/or Pub/Sub we need to set the GCP Service Account Pods should identify as. To allow this, we need the IAM role iam.workloadIdentityUser set on a GCP service account and bind this to the Pod's Kubernetes SA. Documentation on how this is done can be found here (Closed Wiki):https://kartverket.atlassian.net/wiki/spaces/SKIPDOK/pages/422346824/Autentisering+mot+GCP+som+Kubernetes+SA false cloudSqlProxy\tobject CloudSQL is used to deploy a CloudSQL proxy sidecar in the pod. This is useful for connecting to CloudSQL databases that require Cloud SQL Auth Proxy. false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.gcp.auth​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainergcpauth","content":" ↩ Parent  Configuration for authenticating a Pod with Google Cloud Platform For authentication with GCP, to use services like Secret Manager and/or Pub/Sub we need to set the GCP Service Account Pods should identify as. To allow this, we need the IAM role iam.workloadIdentityUser set on a GCP service account and bind this to the Pod's Kubernetes SA. Documentation on how this is done can be found here (Closed Wiki):https://kartverket.atlassian.net/wiki/spaces/SKIPDOK/pages/422346824/Autentisering+mot+GCP+som+Kubernetes+SA  Name\tType\tDescription\tRequiredserviceAccount\tstring Name of the service account in which you are trying to authenticate your pod with Generally takes the form of some-name@some-project-id.iam.gserviceaccount.com true  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.gcp.cloudSqlProxy​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainergcpcloudsqlproxy","content":" ↩ Parent  CloudSQL is used to deploy a CloudSQL proxy sidecar in the pod. This is useful for connecting to CloudSQL databases that require Cloud SQL Auth Proxy.  Name\tType\tDescription\tRequiredconnectionName\tstring Connection name for the CloudSQL instance. Found in the Google Cloud Console under your CloudSQL resource. The format is &quot;projectName:region:instanceName&quot; E.g. &quot;skip-prod-bda1:europe-north1:my-db&quot;. true ip\tstring The IP address of the CloudSQL instance. This is used to create a serviceentry for the CloudSQL proxy. true serviceAccount\tstring Service account used by cloudsql auth proxy. This service account must have the roles/cloudsql.client role. true publicIP\tboolean Default: false false version\tstring Image version for the CloudSQL proxy sidecar. false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.liveness​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainerliveness","content":" ↩ Parent  Probe  Type configuration for all types of Kubernetes probes.  Name\tType\tDescription\tRequiredpath\tstring The path to access on the HTTP server true port\tint or string Number of the port to access on the container true failureThreshold\tinteger Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1 Format: int32 Default: 3 false initialDelay\tinteger Delay sending the first probe by X seconds. Can be useful for applications that are slow to start. Format: int32 Default: 0 false period\tinteger Number of seconds Kubernetes waits between each probe. Defaults to 10 seconds. Format: int32 Default: 10 false successThreshold\tinteger Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup Probes. Minimum value is 1. Format: int32 Default: 1 false timeout\tinteger Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1 Format: int32 Default: 1 false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.podSettings​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainerpodsettings","content":" ↩ Parent  PodSettings  Name\tType\tDescription\tRequiredannotations\tmap[string]string Annotations that are set on Pods created by Skiperator. These annotations can for example be used to change the behaviour of sidecars and similar. false disablePodSpreadTopologyConstraints\tboolean DisablePodSpreadTopologyConstraints specifies whether to disable the addition of Pod Topology Spread Constraints to a given pod. Default: false false terminationGracePeriodSeconds\tinteger TerminationGracePeriodSeconds determines how long Kubernetes waits after a SIGTERM signal sent to a Pod before terminating the pod. If your application uses longer than 30 seconds to terminate, you should increase TerminationGracePeriodSeconds. Format: int64 Default: 30 false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.readiness​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainerreadiness","content":" ↩ Parent  Probe  Type configuration for all types of Kubernetes probes.  Name\tType\tDescription\tRequiredpath\tstring The path to access on the HTTP server true port\tint or string Number of the port to access on the container true failureThreshold\tinteger Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1 Format: int32 Default: 3 false initialDelay\tinteger Delay sending the first probe by X seconds. Can be useful for applications that are slow to start. Format: int32 Default: 0 false period\tinteger Number of seconds Kubernetes waits between each probe. Defaults to 10 seconds. Format: int32 Default: 10 false successThreshold\tinteger Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup Probes. Minimum value is 1. Format: int32 Default: 1 false timeout\tinteger Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1 Format: int32 Default: 1 false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.resources​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainerresources","content":" ↩ Parent  ResourceRequirements  A simplified version of the Kubernetes native ResourceRequirement field, in which only Limits and Requests are present. For the units used for resources, see https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes  Name\tType\tDescription\tRequiredlimits\tmap[string]int or string Limits set the maximum the app is allowed to use. Exceeding this limit will make kubernetes kill the app and restart it. Limits can be set on the CPU and memory, but it is not recommended to put a limit on CPU, see: https://home.robusta.dev/blog/stop-using-cpu-limits false requests\tmap[string]int or string Requests set the initial allocation that is done for the app and will thus be available to the app on startup. More is allocated on demand until the limit is reached. Requests can be set on the CPU and memory. false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.container.startup​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccontainerstartup","content":" ↩ Parent  Probe  Type configuration for all types of Kubernetes probes.  Name\tType\tDescription\tRequiredpath\tstring The path to access on the HTTP server true port\tint or string Number of the port to access on the container true failureThreshold\tinteger Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1 Format: int32 Default: 3 false initialDelay\tinteger Delay sending the first probe by X seconds. Can be useful for applications that are slow to start. Format: int32 Default: 0 false period\tinteger Number of seconds Kubernetes waits between each probe. Defaults to 10 seconds. Format: int32 Default: 10 false successThreshold\tinteger Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup Probes. Minimum value is 1. Format: int32 Default: 1 false timeout\tinteger Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1 Format: int32 Default: 1 false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.cron​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspeccron","content":" ↩ Parent  Settings for the Job if you are running a scheduled job. Optional as Jobs may be one-off.  Name\tType\tDescription\tRequiredschedule\tstring A CronJob string for denoting the schedule of this job. See https://crontab.guru/ for help creating CronJob strings. Kubernetes CronJobs also include the extended &quot;Vixie cron&quot; step values: https://man.freebsd.org/cgi/man.cgi?crontab%285%29. true allowConcurrency\tenum Denotes how Kubernetes should react to multiple instances of the Job being started at the same time. Allow will allow concurrent jobs. Forbid will not allow this, and instead skip the newer schedule Job. Replace will replace the current active Job with the newer scheduled Job. Enum: Allow, Forbid, Replace Default: Allow false startingDeadlineSeconds\tinteger Denotes the deadline in seconds for starting a job on its schedule, if for some reason the Job's controller was not ready upon the scheduled time. If unset, Jobs missing their deadline will be considered failed jobs and will not start. Format: int64 false suspend\tboolean If set to true, this tells Kubernetes to suspend this Job till the field is set to false. If the Job is active while this field is set to true, all running Pods will be terminated. false timeZone\tstring The time zone name for the given schedule, see https://en.wikipedia.org/wiki/List_of_tz_database_time_zones. If not specified, this will default to the time zone of the cluster. Example: &quot;Europe/Oslo&quot; false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.istioSettings​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspecistiosettings","content":" ↩ Parent  IstioSettings are used to configure istio specific resources such as telemetry. Currently, adjusting sampling interval for tracing is the only supported option. By default, tracing is enabled with a random sampling percentage of 10%.  Name\tType\tDescription\tRequiredtelemetry\tobject Telemetry is a placeholder for all relevant telemetry types, and may be extended in the future to configure additional telemetry settings. Default: map[tracing:[map[randomSamplingPercentage:10]]] false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.istioSettings.telemetry​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspecistiosettingstelemetry","content":" ↩ Parent  Telemetry is a placeholder for all relevant telemetry types, and may be extended in the future to configure additional telemetry settings.  Name\tType\tDescription\tRequiredtracing\t[]object Tracing is a list of tracing configurations for the telemetry resource. Normally only one tracing configuration is needed. Default: [map[randomSamplingPercentage:10]] false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.istioSettings.telemetry.tracing[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspecistiosettingstelemetrytracingindex","content":" ↩ Parent  Tracing contains relevant settings for tracing in the telemetry configuration  Name\tType\tDescription\tRequiredrandomSamplingPercentage\tinteger RandomSamplingPercentage is the percentage of requests that should be sampled for tracing, specified by a whole number between 0-100. Setting RandomSamplingPercentage to 0 will disable tracing. Default: 10 Minimum: 0 Maximum: 100 false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.job​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspecjob","content":" ↩ Parent  Settings for the actual Job. If you use a scheduled job, the settings in here will also specify the template of the job.  Name\tType\tDescription\tRequiredactiveDeadlineSeconds\tinteger ActiveDeadlineSeconds denotes a duration in seconds started from when the job is first active. If the deadline is reached during the job's workload the job and its Pods are terminated. If the job is suspended using the Suspend field, this timer is stopped and reset when unsuspended. Format: int64 false backoffLimit\tinteger Specifies the number of retry attempts before determining the job as failed. Defaults to 6. Format: int32 false suspend\tboolean If set to true, this tells Kubernetes to suspend this Job till the field is set to false. If the Job is active while this field is set to false, all running Pods will be terminated. false ttlSecondsAfterFinished\tinteger The number of seconds to wait before removing the Job after it has finished. If unset, Job will not be cleaned up. It is recommended to set this to avoid clutter in your resource tree. Format: int32 false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.spec.prometheus​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobspecprometheus","content":" ↩ Parent  Prometheus settings for pod running in job. Fields are identical to Application and if set, a podmonitoring object is created.  Name\tType\tDescription\tRequiredport\tint or string The port number or name where metrics are exposed (at the Pod level). true allowAllMetrics\tboolean Setting AllowAllMetrics to true will ensure all exposed metrics are scraped. Otherwise, a list of predefined metrics will be dropped by default. See util/constants.go for the default list. Default: false false path\tstring The HTTP path where Prometheus compatible metrics exists Default: /metrics false scrapeInterval\tstring ScrapeInterval specifies the interval at which Prometheus should scrape the metrics. The interval must be at least 15 seconds (if using &quot;Xs&quot;) and divisible by 5. If minutes (&quot;Xm&quot;) are used, the value must be at least 1m. Default: 60s false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.status​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobstatus","content":" ↩ Parent  SkiperatorStatus  A status field shown on a Skiperator resource which contains information regarding deployment of the resource.  Name\tType\tDescription\tRequiredaccessPolicies\tstring Indicates if access policies are valid true conditions\t[]object true subresources\tmap[string]object true summary\tobject Status true  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.status.conditions[index]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobstatusconditionsindex","content":" ↩ Parent  Condition contains details for one aspect of the current state of this API Resource.  Name\tType\tDescription\tRequiredlastTransitionTime\tstring lastTransitionTime is the last time the condition transitioned from one status to another. This should be when the underlying condition changed. If that is not known, then using the time when the API field changed is acceptable. Format: date-time true message\tstring message is a human readable message indicating details about the transition. This may be an empty string. true reason\tstring reason contains a programmatic identifier indicating the reason for the condition's last transition. Producers of specific condition types may define expected values and meanings for this field, and whether the values are considered a guaranteed API. The value should be a CamelCase string. This field may not be empty. true status\tenum status of the condition, one of True, False, Unknown. Enum: True, False, Unknown true type\tstring type of condition in CamelCase or in foo.example.com/CamelCase. true observedGeneration\tinteger observedGeneration represents the .metadata.generation that the condition was set based upon. For instance, if .metadata.generation is currently 12, but the .status.conditions[x].observedGeneration is 9, the condition is out of date with respect to the current state of the instance. Format: int64 Minimum: 0 false  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.status.subresources[key]​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobstatussubresourceskey","content":" ↩ Parent  Status  Name\tType\tDescription\tRequiredmessage\tstring Default: hello true status\tstring Default: Synced true timestamp\tstring Default: hello true  ","version":"Next","tagName":"h3"},{"title":"SKIPJob.status.summary​","type":1,"pageTitle":"API Reference","url":"/docs/applikasjon-utrulling/skiperator/api-docs#skipjobstatussummary","content":" ↩ Parent  Status  Name\tType\tDescription\tRequiredmessage\tstring Default: hello true status\tstring Default: Synced true timestamp\tstring Default: hello true ","version":"Next","tagName":"h3"},{"title":"Jobbe med Kubernetes cluster","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/jobbe-med-cluster","content":"","keywords":"","version":"Next"},{"title":"K9s​","type":1,"pageTitle":"Jobbe med Kubernetes cluster","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/jobbe-med-cluster#k9s","content":" K9s er terminalbasert men gir deg mer informasjon enn du ellers ville fått ved enkle kubectl kommandoer. Se her en oversikt over alle Podder som kjører i et namespace.    Her får vi for eksempel en stor toast på at alle poddene kjører med mer minne enn de requester. I tillegg har vi en fin oversikt over generell ressursbruk og forhold mellom request/limit og faktisk bruk.  Man kan enkelt sortere på alle felter, søke på vilkårlige ressurstyper, redigere ressurser, filtere basert på søk og masse mer.  Nedlasting: K9s - Manage Your Kubernetes Clusters In Style  ","version":"Next","tagName":"h2"},{"title":"Freelens​","type":1,"pageTitle":"Jobbe med Kubernetes cluster","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/jobbe-med-cluster#freelens","content":" Om man heller foretrekker GUI kan dette være et alternativ. En fersk fork og videreutvikling av Openlens.    Freelens nettside  Nedlasting: Freelens Github ","version":"Next","tagName":"h2"},{"title":"Logge inn på cluster","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/logge-inn-på-cluster","content":"","keywords":"","version":"Next"},{"title":"CLI (kubectl)​","type":1,"pageTitle":"Logge inn på cluster","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/logge-inn-på-cluster#cli-kubectl","content":" Først installer gcloud og kubectl .  For å logge inn med kubectl gjør følgende:  # Login med gcloud hvis du ikke har gjort det allerede $ gcloud auth login # Sørg for at du står i riktig gcp-prosjekt # Hvis du ikke vet hele navnet på prosjektet kan du finne dette vet å liste prosjekter $ gcloud projects list # GCP-prosjektet vil være et kubernetes-prosjekt med format kubernetes-&lt;env&gt;-xxxx # Jobber du f.eks. i dsa-dev-e32c velger du kubernetes-dev-94b9 $ gcloud config set project kubernetes-dev-94b9 # Finn riktig clusternavn $ gcloud container hub memberships list # Clusternavn on-prem er på formatet &lt;cluster&gt;-&lt;env&gt;-user-cluster og &lt;cluster&gt;-&lt;env&gt; # i GCP, f.eks. atkv3-dev-user-cluster for on-prem og atgcp1-dev for GCP # Logg inn, generer kubeconfig og sett som aktiv context $ gcloud container hub memberships get-credentials atkv3-dev-user-cluster # Forrige kommando oppretter en ny context, som kan autentisere deg mot clusteret # Contexten som blir opprettet her ser noe a la slik ut: # For gcp: connectgateway_kubernetes-&lt;env&gt;-xxxx_global_&lt;clusternavn&gt; # For on-prem: connectgateway_kubernetes-&lt;env&gt;-xxxx_europe-north1_&lt;clusternavn&gt; # Eksempel: connectgateway_kubernetes-dev-94b9_europe-north1_atkv3-dev-user-cluster # Har du lastet ned kubectx kan du bytte til contexten slik: $ kubectx connectgateway_kubernetes-dev-94b9_europe-north1_atkv3-dev-user-cluster # Om ikke kan du gjøre det med følgende kommando i kubectl: $ kubectl config use-context connectgateway_kubernetes-dev-94b9_europe-north1_atkv3-dev-user-cluster # Du kan også rename disse contextene til noe litt mer spiselig med følgende kommando # Her er navn 2 vilkårlig $ kubectl config rename-context connectgateway_kubernetes-dev-94b9_europe-north1_atkv3-dev-user-cluster atkv3-dev   Se også https://cloud.google.com/anthos/multicluster-management/gateway/using .  For å ha adgang til å logge på clusteret må du være medlem av en en AAD - TF - TEAM EntraID-gruppe som er synket med GCP.  Du kan sjekke om teamet ditt er lagt til i entra-id-confighvis du er usikker. ","version":"Next","tagName":"h2"},{"title":"Helsesjekker i Kubernetes","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/helsesjekker-i-kubernetes","content":"","keywords":"","version":"Next"},{"title":"Bakgrunn​","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/helsesjekker-i-kubernetes#bakgrunn","content":" Helsesjekker i kubernetes er en veldig viktig del av mikrotjeneste-arkitekturen, og det er derfor lurt å sette seg litt inn i hensikten og funksjonen til de ulike helse-endepunktene man kan konfigurere. Det finnes mange måter å konfigurere disse på, alt i fra helt egenlagde endepunkter til ferdige rammeverk som eksponerer dette automatisk.  I hovedsak finnes det tre typer prober som kubernetes opererer med:  Liveness probe - Sjekker om containeren kjører og fungerer, hvis ikke så restarter kubernetes containeren    Readiness probe - Brukes for å bestemme om en pod en klar for å ta i mot trafikk. En pod er klar når alle containere i poden er klare.    Startup probe - Hvis denne er konfigurert så avventer kubernetes med liveness og readiness til dette endepunktet svarer. Dette kan brukes for å gi trege containere noe mer tid til å starte opp.  Det finnes flere måter å sette opp helsesjekker på, som f.eks. kommandoer, HTTP-requests, TCP-requests og gRPC-requests. Den aller vanligste måten er å konfigurere et endepunkt (f.eks /health, /liveness) i applikasjonen som svarer på HTTP-requests, og spesifisere dette som en del av pod-spesifikasjonen.  Litt mer om helsesjekker generelt: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/  Det er viktig å merke seg at man ikkemåbenytte seg av alle disse helsesjekkene, men man bør ta et bevisst valg om det er hensiktsmessig eller ikke. Sjekk den lenken her for å en oversikt over hva man bør gjøre: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#when-should-you-use-a-liveness-probe  ","version":"Next","tagName":"h2"},{"title":"Skiperator​","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/helsesjekker-i-kubernetes#skiperator","content":" De aller fleste som har applikasjoner på SKIP benytter Skiperator for å forenkle oppsettet rundt applikasjonen. I Skiperator-manifestet kan man konfigurere helsesjekker på samme måte man ville gjort for en vanilla-pod i kubernetes. Detaljer rundt dette står i dokumentasjonen for Skiperator . Se under ApplicationSpec og Liveness / Readiness / Startup .  ","version":"Next","tagName":"h2"},{"title":"Sikkerhetshensyn​","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/helsesjekker-i-kubernetes#sikkerhetshensyn","content":" Det er noen fallgruver å være klar over, så denne siden skal prøve å gi en oversikt over hvordan man typisk bør konfigurere dette på en sikker og god måte.  Hvis man ikke konfigurerer applikasjonen sin riktig kan man i verste fall ende opp med å eksponere de samme endepunktene som kubernetes bruker internt ut på internett. Et enkelt /health endepunkt som svarer med HTTP 200 OK, gjør ikke så stor skade. Et feilkonfigurert management-endepunkt derimot kan eksponere interne miljøvariabler, debug-info og minnedump.  Ta en titt på følgende flytskjema før du går videre, og gå til det punktet som gjelder deg  ","version":"Next","tagName":"h2"},{"title":"Undersøke hva som eksponeres som standard​","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/helsesjekker-i-kubernetes#undersøke-hva-som-eksponeres-som-standard","content":" En veldig vanlig måte å løse helsejsekker på når man bruker Spring Boot er å benytte seg av sub-prosjektet Spring Boot Actuator .  For å ta det i bruk trenger man bare å legge til følgende for Maven-prosjekt (pom.xml)  &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;   Eller dette hvis man bruker Gradle (build.gradle)  dependencies { implementation 'org.springframework.boot:spring-boot-starter-actuator' }   Rammeverket setter automatisk opp endepunktet /actuator/health som en trygg default (gjelder versjon 2 og høyere av Spring Boot). Når man starter opp en Spring-applikasjon i kubernetes vil Spring Boot Actuator også automatisk tilgjengeliggjøre /actuator/health/liveness og /actuator/health/readiness som man benytte for helsesjekker. For å teste disse manuelt kan du legge til management.endpoint.health.probes.enabled=true i application.properties .  Disse endepunktene kan du så bruke i Skiperator-manifestet:  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: some-backend namespace: yournamespace spec: # Øvrig konfigurasjon liveness: path: /actuator/health/liveness port: 8080 failureThreshold: 3 timeout: 1 initialDelay: 3 readiness: path: /actuator/health/readiness port: 8080 failureThreshold: 3 timeout: 1 initialDelay: 5   Det er viktig å merke seg følgende notat:  warning Konfigurasjon som management.endpoints.web.exposure.include=* frarådes ettersom det eksponerer alle endepunkt som er skrudd på. I så fall må man passe på å sette management.endpoints.enabled-by-default=false og manuelt skru på de man ønsker å bruke.  Ønsker man å eksponere ytterligere endepunkt , som f.eks. å eksponere /info for å presentere informasjon om bygget eller lignende er det tryggere å eksplisitt man gjøre det på denne måten i application.properties :  management.endpoint.info.enabled=true management.endpoint.health.enabled=true management.endpoints.web.exposure.include=health,info   info Savner du ditt rammeverk? Legg det til da vel   ","version":"Next","tagName":"h3"},{"title":"Eksponer endepunktene på dedikert port uten service​","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/helsesjekker-i-kubernetes#eksponer-endepunktene-på-dedikert-port-uten-service","content":" Her vil det variere litt hvordan man går frem avhengig av om man bruker et rammeverk eller ikke, siden prosessen i containeren må lytte på ekstra port.  Først må man velge seg en port, f.eks. 8081, og så eksponere denne i Dockerfile. I dette eksempelet vil 8080 være applikasjonsporten, og 8081 management/helseporten.  EXPOSE 8080 8081   Deretter må man konfigurere management-porten i application.properties på følgende måte:  management.server.port=8081 management.endpoint.info.enabled=true management.endpoint.health.enabled=true management.endpoints.web.exposure.include=health,info   Skiperator-manifestet vil være helt likt, men man insturerer kubernetes til å gjøre helsesjekkene på en annen port.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: some-backend namespace: yournamespace spec: port: 8080 # Øvrig konfigurasjon liveness: path: /actuator/health/liveness port: 8081 failureThreshold: 3 timeout: 1 initialDelay: 3 readiness: path: /actuator/health/readiness port: 8081 failureThreshold: 3 timeout: 1 initialDelay: 5   ","version":"Next","tagName":"h3"},{"title":"Eksponer endepunktene på dedikert port med service​","type":1,"pageTitle":"Helsesjekker i Kubernetes","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/helsesjekker-i-kubernetes#eksponer-endepunktene-på-dedikert-port-med-service","content":" note For øyeblikket kan man ikke spesifisere hvilken port man skal tillate trafikk til via skiperator sin accessPolicy  Hvis man har behov for at en annen applikasjon skal kunne nå endepunktet på en annen port må man gjøre ytterligere konfigurasjon. Man bør ta en ekstra vurdering på om det er hensiktmessig å eksponere denne typen informasjon via actuator-endepunkter.  Sett opp konfigurasjonen på samme måte som punktet over, men manifestet vil nå inkludere spesifikasjon for ekstra porter og tilgangssstyring.  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: some-backend namespace: yournamespace spec: port: 8080 additionalPorts: - name: actuator port: 8081 protocol: TCP # .. øvrig konfigurasjon liveness: path: /actuator/health/liveness port: 8081 failureThreshold: 3 timeout: 1 initialDelay: 3 readiness: path: /actuator/health/readiness port: 8081 failureThreshold: 3 timeout: 1 initialDelay: 5 accessPolicy: inbound: rules: - application: some-frontend port: 8081 # Ikke mulig akkurat nå  ","version":"Next","tagName":"h3"},{"title":"Sikkerhet","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/security","content":"Sikkerhet SKIP er bygget etter prinsippet om innebygget sikkerhet, slik at det blir lett å gjøre rett. Standardoppførselen skal i utgangspunktet være sikker, med mulighet for produktteamene å overstyre der det gir mening for deres applikasjon. Dette er kort fortalt hvordan SKIP balanserer behovet for sikkerhet med autonomi. Et eksempel på dette er prinsippet om Zero Trust i nettverkslaget. All trafikk på Kubernetes er i utgangspunktet stengt, en pod kan ikke snakke med en hvilken som helst annen. Kun om begge tjenestene åpner for at de kan snakke med hverandre kan trafikken flyte mellom dem. Dette gjør produktteamene selv ved å sette accessPolicy i sitt Skiperator-manifest. All trafikk mellom podder i Kubernetes er kryptert med mTLS helt automatisk. Det eneste man trenger å gjøre er å sende spørringer til en annen pod, så krypterer Service Meshet koblingen automatisk. Dersom man trenger å eksponere applikasjonen sin til omverdenen kan man konfigurere et endepunkt som applikasjonen skal eksponeres på. Når dette er konfigurert får man utstedt et gyldig sertifikat som gjør at all trafikk krypteres med HTTPS helt automatisk. Dette sertifikatet fornyes også automatisk. Dette og mye mer fører til at det er lett å gjøre rett på SKIP.","keywords":"","version":"Next"},{"title":"Retningslinjer for Kubernetes","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/retningslinjer-for-kubernetes","content":"","keywords":"","version":"Next"},{"title":"Minstekrav for sikkerhet​","type":1,"pageTitle":"Retningslinjer for Kubernetes","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/retningslinjer-for-kubernetes#minstekrav-for-sikkerhet","content":" I Kubernetes bruker vi Pod Security Standards for å sikre at alle pods som kjører i clusteret vårt er sikre. Dette er en standard som er satt av CNCF, og som vi følger for å sikre at vi ikke har noen åpenbare sikkerhetshull i Kubernetes-clusteret vårt. Alle workloads skal følge PSS nivå &quot;restricted&quot;, som er et nivå som følger dagens best practices for sikring av containere. Applikasjoner som kjører som Skiperator Applications følger allerede denne standarden. Les mer om Pod Security Standards her.  ","version":"Next","tagName":"h2"},{"title":"Namespaces som avgrensning mellom teams​","type":1,"pageTitle":"Retningslinjer for Kubernetes","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/retningslinjer-for-kubernetes#namespaces-som-avgrensning-mellom-teams","content":" Hvert team kan lage så mange Namespaces som de har behov for. Dette er for å kunne skille på ressurser og tilganger mellom forskjellige applikasjoner og team. Dette er også for å kunne gi teamene mulighet til å eksperimentere og teste ting uten at det påvirker andre team. Les mer om dette på Argo CD.  Kommunikasjon mellom tjenester internt i ett namespace er helt lukket (“default deny”-policy), og det er opp til teamet selv å sørge for å åpne for kommunikasjon mellom tjenester. Les mer om dette på Skiperator.  Informasjon om kommunikasjon mot tjenester som ligger i andre namespaces finnes her: Anthos Service Mesh Brukerdokumentasjon  Tjenester og fellesfunksjoner som brukes av flere teams skal settes i egne namespaces. Tilgang til disse namespacene gis ved at det opprettes en ny gruppe i AD på samme måte som et produktteam.  ","version":"Next","tagName":"h2"},{"title":"Ressursbruk i Kubernetes​","type":1,"pageTitle":"Retningslinjer for Kubernetes","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/retningslinjer-for-kubernetes#ressursbruk-i-kubernetes","content":" Ressursbruk i Kubernetes dreier seg om hvor mye CPU og RAM hver pod skal bruke.  Requests er hvor mye CPU og minne hvercontainerspør om når den først settes på en node. Hvis man for eksempel ber om 500 mCPUer, men noden bare har 250 mCPU ledig, kan containeres ikke kjøres på den noden.  Merk at man kan spesifisere CPU helt ned i millicpuer (mCPU).  Minne-requests kan settes i mange forskjellige enheter, se dokumentasjonen for detaljer. Vi anbefaler dog at man holder seg til M - megabytes.  Limits er hvor mye ressurser en container maksimalt får lov til å bruke. Dette er med andre ord noe som settes for å forhindre at en container med en bug tar over alle ressursene, og gjør det umulig for andre containere å skalere.  Vi anbefaler at du ser på Limiten som en mulighet til å finne bugs og memory leaks. Sett den så lavt du er komfortabel med, og følg med på det faktiske forbruket. Hvis noe kræsjer er det da god sjanse for at det ble innført en bug.  Dersom det faktiske forbruket nærmer seg limiten på grunn av naturlige grunner - flere requests eller tyngre load+ er det på tide å øke limiten. Ikke vent til appliasjonen kræsjer - det skaper en dårlig brukeropplevelse.  tip En god tommelfingerregel for requests og limits er følgende: For minne bør man profilere applikasjonens gjennomsnittlige minnebruk og doble denne som limit. For CPU trenger man ikke limit, men heller definere en fornuftig request.  Logikken bak dette er at dersom en applikasjon bruker altfor mye minne kan det føre til at andre applikasjoner går ned. Dersom en applikasjon bruker mye CPU fører det derimot bare i verste fall til at ting går tregere.  Dette er reglene for ressursbruk i Kubernetes på SKIP  Produktteamet velger selv hva som er naturlig ressursbruk for sine containere, og skal ha et bevisst forhold til hvilke grenser som er satt. Produktteamet skal følge med på ressursbruken over tid, og oppdatere grensene slik at de til enhver tid reflekterer hva applikasjonen faktisk trenger. Resource requests og limits skal settes på alle containere slik at det blir tydelig hva som er forventet ressursbruk. Resource limits skal skal alltid settes høyere enn requests, men aldri unaturlig høyt. Husk at dette fungerer både som dokumentasjon og som en sikring mot bugs og feilkonfigurasjon. Resource limits skal aldri fjernes permanent, men kan fjernes for debugging. Da skal SKIP-teamet gjøres oppmerksom på dette. Godt blogginnlegg om korrelasjonen mellom JVMs og Kubernetes’ minnebruk  Kubernetes’ dokumentasjon om ressursbruk  Google Clouds dokumentasjon om “cost effective apps” (Merk at Google anbefaler å sette limit til det samme som requests - vi setter driftsstabilitet over kostnad, og er derfor uenig i dette.) ","version":"Next","tagName":"h2"},{"title":"URLer og sertifikat for tjenester på SKIP","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/urler-og-sertifikat-for-tjenester-på-skip","content":"","keywords":"","version":"Next"},{"title":"Interne tjenester​","type":1,"pageTitle":"URLer og sertifikat for tjenester på SKIP","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/urler-og-sertifikat-for-tjenester-på-skip#interne-tjenester","content":" ","version":"Next","tagName":"h2"},{"title":"kartverket-intern.cloud​","type":1,"pageTitle":"URLer og sertifikat for tjenester på SKIP","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/urler-og-sertifikat-for-tjenester-på-skip#kartverket-interncloud","content":" Dersom du har en tjeneste som kun skal være tilgjengelig for folk på kartverkets nettverk og VPN og ikke på internett for allmennheten har man flere forskjellige alternativer. Avhengig av bruksområde og hva slags URL man ønsker seg fungerer dette litt forskjellig, og beskrives i paragrafene under.  For tjenester som skal nås på et domene under kartverket-intern.cloud håndteres alt automatisk, inkludert utstedelse og fornying av sertfikater. Det ligger et wildcard record i DNS som håndterer innkommende trafikk, og bruker cluster-leddet i URL-en på Load Balanceren til å rute denne inn til riktig cluster. Deretter rutes denne til applikasjonen din basert på URL-konfigurasjonen din i Skiperator.  info Eksempel: minapp.atkv3-prod.kartverket-intern.cloud  ","version":"Next","tagName":"h3"},{"title":"Vanity URL-er​","type":1,"pageTitle":"URLer og sertifikat for tjenester på SKIP","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/urler-og-sertifikat-for-tjenester-på-skip#vanity-url-er","content":" note Akkurat nå støttes kun kartverket-intern.cloud URL-er pga. en begrensning i utstedelse av sertfikater ( SKIP-1459 ) og en begrensning i lastbalanserer på atkv3-dev cluster ( SKIP-1458 ). Dette skal utbedres.  Dersom du ønsker et annet hostname enn app.&lt;cluster&gt;.kartverket-intern.cloud er dette mulig, men krever noe mer setup. Den nye URL-en må registreres i DNS og skiperator-applikasjonen din må settes opp til å lytte på denne. Utstedelse og fornying av sertfikater vil fremdeles håndteres automatisk av Skiperator.  For å sette opp DNS må du gjøre følgende: Først bestem hvilke URL du vil ha, deretter sett opp et CNAME for denne URL-en til &lt;cluster&gt;.kartverket-intern.cloud . Dersom du ønsker et CNAME som ligger under kartverket-intern.cloud (for eksempel minapp.kartverket-intern.cloud) kan dette gjøres av SKIP, for alle andre domener ta kontakt med eier av domenet via bestilling i pureservice. Når dette er gjort vil alle spørringer som går mot URL-en du har bestemt ende opp host lastbalansereren foran clusteret, og sendes videre inn til Kubernetes.  Neste steg er at Kubernetes sender spørringen videre til din applikasjon. Da må du registere URL-en i Skiperator som vanlig under ingresses .  ","version":"Next","tagName":"h3"},{"title":"Cluster-intern​","type":1,"pageTitle":"URLer og sertifikat for tjenester på SKIP","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/urler-og-sertifikat-for-tjenester-på-skip#cluster-intern","content":" Alle applikasjoner som kjører på SKIP har en kubernetes Service tilknyttet seg. Med denne servicen kan man sende spørringer direkte til applikasjonen uten å sende trafikken ut av clusteret.  Merk at man her bruker http og ikke https. Trafikken vil allikevel krypteres av service meshet så trafikken vil gå over https mellom tjenestene, men fra ditt perspektiv skal du bruke http og trenger ikke tenke på sertfikater.  For å sende en spørring på denne måten bruker du en URL i følgende format:  http://&lt;appnavn&gt;.&lt;namespacenavn&gt;:port   Merk at å snakke med en annen tjeneste på denne måten krever at du har åpnet opp for at trafikk kan flyte mellom disse tjenestene. I utgangspunktet blir all trafikk blokkert av sikkerhetshensyn. Å åpne opp gjøres ved å spesifisere spec.accessPolicy.outbound.rules i applikasjonen som skal sende spørringen og spec.accessPolicy.inbound.rules i applikasjonen som skal motta spørringene.  Dersom du har samme tjeneste i sky og ønsker å presisere at du skal gå mot samme cluster må man legge på dette i URL. Hvis ikke blir den “round robined” mellom remote og lokal. Eksempel:  http://&lt;appnavn&gt;.&lt;namespacenavn&gt;.svc.cluster.local:port   ","version":"Next","tagName":"h3"},{"title":"Mesh-intern​","type":1,"pageTitle":"URLer og sertifikat for tjenester på SKIP","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/urler-og-sertifikat-for-tjenester-på-skip#mesh-intern","content":" Dersom du har behov for å sende en spørring til en annen applikasjon som ikke ligger på samme cluster, men er en del av samme service mesh (for eksempel fra atkv3-prod til atgcp1-prod) så kan dette rutes på nesten samme måte som cluster-intern trafikk.  Merk at man her bruker http og ikke https. Trafikken vil allikevel krypteres av service meshet så trafikken vil gå over https mellom tjenestene, men fra ditt perspektiv skal du bruke http og trenger ikke tenke på sertfikater.  For å sende trafikk til et annet cluster over service meshet sender du en spørring i følgende format:  http://&lt;appnavn&gt;.&lt;namespacenavn&gt;.svc.cluster.&lt;cluster&gt;:port   TODO: Hvordan blir networkpolicies for Skiperator apper på mesh?  ","version":"Next","tagName":"h3"},{"title":"Tjenester eksponert på internett​","type":1,"pageTitle":"URLer og sertifikat for tjenester på SKIP","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/urler-og-sertifikat-for-tjenester-på-skip#tjenester-eksponert-på-internett","content":" Det er to alternativer for å eksponere ting på internett. Bruk kartverket.cloud eller en penere “vanity URL”.  Merk at skiperator-tjenester som eksponeres på andre domenenavn enn subdomener av kartverket-intern.cloud vil automatisk bli åpnet for trafikk fra internett, men vil ikke være tilgjengelig før DNS konfigureres.  ","version":"Next","tagName":"h2"},{"title":"kartverket.cloud​","type":1,"pageTitle":"URLer og sertifikat for tjenester på SKIP","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/urler-og-sertifikat-for-tjenester-på-skip#kartverketcloud","content":" For tjenester som skal nås på et domene under kartverket.cloud håndteres alt automatisk, inkludert utstedelse og fornying av sertfikater. Det ligger et wildcard record i DNS som håndterer innkommende trafikk, og bruker cluster-leddet i URL-en på Load Balanceren til å rute denne inn til riktig cluster. Deretter rutes denne til applikasjonen din basert på URL-konfigurasjonen din i Skiperator.  info Eksempel: minapp.atkv3-prod.kartverket.cloud  ","version":"Next","tagName":"h3"},{"title":"Vanity URL-er​","type":1,"pageTitle":"URLer og sertifikat for tjenester på SKIP","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/urler-og-sertifikat-for-tjenester-på-skip#vanity-url-er-1","content":" Dersom du ønsker et annet hostname enn app.&lt;cluster&gt;.kartverket.cloud er dette mulig, men krever noe mer setup. Den nye URL-en må registreres i DNS og skiperator-applikasjonen din må settes opp til å lytte på denne. Utstedelse og fornying av sertfikater vil fremdeles håndteres automatisk av Skiperator.  For å sette opp DNS må du gjøre følgende: Først bestem hvilke URL du vil ha, deretter sett opp et CNAME for denne URL-en til &lt;cluster&gt;.kartverket.cloud . Dersom du ønsker et CNAME som ligger under kartverket.cloud (for eksempel minapp.kartverket.cloud) kan dette gjøres av SKIP, for alle andre domener ta kontakt med eier av domenet via bestilling i pureservice. Når dette er gjort vil alle spørringer som går mot URL-en du har bestemt ende opp host lastbalansereren foran clusteret, og sendes videre inn til Kubernetes.  Neste steg er at Kubernetes sender spørringen videre til din applikasjon. Da må du registere URL-en i Skiperator som vanlig under ingresses .  Dersom du ønsker å ha en URL på toppnivå (annentjeneste.no) er ikke CNAME støttet i DNS. Her må man bruke an A record, og her kan man i så fall få IP-adresser med å gjøre et DNS-oppslag på &lt;cluster&gt;.kartverket.cloud .  ","version":"Next","tagName":"h3"},{"title":"HTTPS by default​","type":1,"pageTitle":"URLer og sertifikat for tjenester på SKIP","url":"/docs/kom-i-gang/praktisk-intro/kubernetes/urler-og-sertifikat-for-tjenester-på-skip#https-by-default","content":" Når man eksponerer en applikasjon får man også HTTPS automatisk satt opp og eksponert. I dette tilfellet kan man fort spørre seg om man burde redirecte HTTP til HTTPS for at alle brukerene skal nyte godt av dette, og svaret på det er i nesten alle tilfeller ja.  For å sette opp en slik redirect er det enkleste å få applikasjonen som serverer ressurser til klienten (nettleseren) å sende en https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Strict-Transport-Security header (HSTS). Når en nettleser laster en nettside og oppdager en HSTS header vil den legge denne nettsiden i sin interne cache med et flagg som sier at denne nettsiden alltid skal lastes med HTTPS. Lengden på denne cachen kan settes i flagget, og i de fleste tilfeller vil denne settes ganske høyt.  Den eneste tiden hvor dette kan bli problematisk er om det plutselig skjer en endring som gjør at nettsiden ikke lenger serveres på HTTPS. For å forhindre downgrade attacks vil nettleseren serveres en feilmelding om at nettsiden kun kan åpnes på HTTPS og det vil ikke være mulig å gå forbi denne for å nå HTTP-siden. Men i de aller fleste tilfeller er ikke dette noe å bekymre seg over. ","version":"Next","tagName":"h2"},{"title":"Tilgang til GCP","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/team/access-gcp","content":"Tilgang til GCP SKIP benytter Google Cloud Platform som økosystem rundt Kubernetes. Det gjør at man kan benytte seg av andre Google-produkter selv om applikasjonen kjører på et on-premise cluster. Man kan også autentisere seg mot GCP og benyttekubectl gjennom Google sin Connect Gateway for å aksessere on-premise cluster uten å være på det interne nettverket/VDI. For å kunne logge på GCP med Kartverket-brukeren må brukeren være medlem i en AAD - TF - TEAM - gruppe i EntraID. Dette er beskrevet i Legge til eller fjerne personer fra et teamSjekk onboarding-dokumentasjonen om du ikke allerede har gjort dette. Har du nettopp blitt onboardet kan det ta opptil en time før gruppen er synket inn i GCP.","keywords":"","version":"Next"},{"title":"🫂 Team på SKIP","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/team","content":"🫂 Team på SKIP Teamets ansvar på SKIPOnboarde nytt team SKIPLegge til/fjerne personer fra teametFå tilgang til GCP, Grafana, ArgoCD, GitHUBHvordan fungerer tilgangsstyring på SKIP","keywords":"","version":"Next"},{"title":"Konfigurering av Entra ID grupper","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/team/add-remove-team-member","content":"","keywords":"","version":"Next"},{"title":"Legge til eller fjerne personer fra et team​","type":1,"pageTitle":"Konfigurering av Entra ID grupper","url":"/docs/kom-i-gang/team/add-remove-team-member#legge-til-eller-fjerne-personer-fra-et-team","content":" Det vil ofte forekomme tilfeller hvor personer går inn eller ut av et produktteam. Dette kan for eksempel være om man har fått en nyansatt som skal inn eller en konsulent som har skiftet over til et annet team. I disse tilfellene har man tradisjonelt vært avhengig av at dette håndteres som en bestilling og derfor er en tidkrevende prosess å gi nye teammedlemmer tilganger, noe som fører til treghet i onboarding.  På SKIP har vi delegert tilganger til team lead på hvert team slik at disse personene kan administrere sine grupper i Entra ID. Disse gruppene er de som brukes for å gi tilgang til tjenester på SKIP slik at et medlem av disse gruppene automatisk får tilgang til SKIP-tjenester som Google Cloud og Kubernetes. Med andre ord er det produktteamene sitt ansvar å holde sine team oppdatert, og av sikkerhetsmessige hensyn forventes det at dette gjøres. Det kan bli utført kontroll av dette i ettertid, så det forventes at team på SKIP har rutiner og sjekklister for offboarding av teammedlemmer.    For å legge til eller fjerne et teammedlem må team leaden på teamet (eller personen som har fått team lead ansvar) gå tilEntra ID . Her finner man sin gruppe ved å søke etterTF - AAD - TEAM - mitt_teamnavn (bytt ut mitt_teamnavn med ditt teamnavn). Klikk deg inn på dette og du vil finne Members i sidemenyen til venstre. Etter du har klikket deg inn der bør du se et skjermbilde som ligner på det over. Her ser du alle som ligger i team-gruppen deres og i kraft av det har fått tilganger til det en person på deres team skal ha.  Du vil også se at det ligger tre grupper øverst i denne listen. Dette er grupper for personer med henholdsvis produkteier-, team lead- og tech lead-ansvar. Disse skal ligge der og det forventes også at dere fyller ut disse med de enkeltpersonene som skal ha disse ansvarsområdene.  Dersom du er logget inn som team lead vil Add members-knappen øverst være mulig å klikke på. Dersom du vil legge til et nytt teammedlem klikker du på denne og søker opp den personen du ønsker å legge til. Etter dette bekreftes vil denne personen legges i gruppen og etter litt tid få de tilgangene som forventes. Team lead må også legge til seg selv som medlem for å tilgang til SKIPs tjenester.  For å fjerne (offboarde) et teammedlem krysser du av i firkanten ved siden av bildet på brukeren og klikker Remove. Dette er også noe som kun team lead kan gjøre.  ","version":"Next","tagName":"h2"},{"title":"Endre team lead​","type":1,"pageTitle":"Konfigurering av Entra ID grupper","url":"/docs/kom-i-gang/team/add-remove-team-member#endre-team-lead","content":" Endring av team lead kan kun gjøres i entra-id-config repoet, dersom du prøver å endre på team lead eller owners i Entra ID portalen, så vil dette bli overskrevet. For å endre team lead så må du finne teamet ditt i org.yaml, endre på team_lead variabelen, og lage en pull request. ","version":"Next","tagName":"h2"},{"title":"Dynamisk tilgangskontroll (JIT)","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/team/jit","content":"Dynamisk tilgangskontroll (JIT) Most developers will at some point experience not having the correct permissions to operate on Google Cloud resources. This is intentional and is part of the principle of least privilege . In order to operate on the resources you want to access, you need to elevate your privileges. A system exists to make this operation self-service, and it is called Just-In-Time access. It can be accessed at https://jit.skip.kartverket.no . After logging in with your Kartverket google account, it will take you to the below screen. First step is filling in the ID of the project you wish to get access to. This can be found by searching in the box or by finding the ID from console.cloud.google.com. Second step, select the roles you want. It is often possible to see which role you need from the error message you got when trying to do an operation and getting denied. A common role that is used for administering secrets in Google Secret Manager is secretmanager.admin. Select a suitable duration using the slider and click continue. Note that some sensitive roles are not compatible with longer durations. Now for the final step, enter a reason for the access request. This is mostly for auditing, as generally speaking requests are granted automatically. The reason entered will be possible to see in the logs if we need to investigate a security breach. In less common cases, for example when restricted roles are to be granted, a manual approval is required. In that case the reason will be visible to the person who approves the request. When you click request access, you will be taken to a summary screen which gives you the result of your request. In the example above, my request was granted automatically. You now have access, and that's just in time!","keywords":"","version":"Next"},{"title":"Kostnadsoversikt og alarmer","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/team/kostnadsoversikt-og-alarmer","content":"","keywords":"","version":"Next"},{"title":"Kostnadsoversikt​","type":1,"pageTitle":"Kostnadsoversikt og alarmer","url":"/docs/kom-i-gang/team/kostnadsoversikt-og-alarmer#kostnadsoversikt","content":" SKIP har laget et Grafana dashboard som kan brukes til å holde en løpende oversikt over kostnader. Dashboardet viser kostnader fordelt på prosjekter og tjenester, og gir en oversikt gruppert på divisjoner eller teams.  Dashboardet kan man finne her.  Merk: GCP Cost dashboardet vil vise små forskjeller fra GCP Budgets, dette skyldes start tiden på intervallet. Dataen i dashboardet kan også oppdatere seg litt tregere. Se på dashboardet som en pekepinn, mens GCP Budgets er fasiten.  ","version":"Next","tagName":"h2"},{"title":"Alarmer​","type":1,"pageTitle":"Kostnadsoversikt og alarmer","url":"/docs/kom-i-gang/team/kostnadsoversikt-og-alarmer#alarmer","content":" For å unngå overraskelser i form av høye kostnader, er det viktig å sette opp alarmer. Alarmer kan settes opp for å varsle om kostnader som overstiger en viss grense, eller for å varsle om kostnader som øker raskt.  Vi anbefaler på det sterkeste at alarmer blir satt når dere tar i bruk tjenester i Google Cloud. Dette kan gjøres i cost-alerts repoet på Github.  Kostnadsalarmer i GCP heter 'budgets', så herfra referes kostnadsalarmer som budsjett.  Standard intervall på budsjetter er månedlig. Det vil si fra den første til den siste dagen i måneden.  ","version":"Next","tagName":"h2"},{"title":"Hvordan sette opp et budsjett​","type":1,"pageTitle":"Kostnadsoversikt og alarmer","url":"/docs/kom-i-gang/team/kostnadsoversikt-og-alarmer#hvordan-sette-opp-et-budsjett","content":" cost-alerts fungerer på mange måter likt som grafana-alerts repoet. Dersom dere skal opprette deres første budsjett, så opprett en PR mot cost-alerts repoet hvor dere gjør følgende:  Opprett en fil med navnet på teamet i teams mappen, f.eks teams/mitt-team.tfLegg til en linje i CODEOWNERS-filen, med følgende format: teams/mitt-team.tf @kartverket/mitt-team  I teams/mitt-team.tf-filen så kan man opprette et budsjett slik:  module &quot;mitt_team_gcp_budget&quot; { source = &quot;./modules/gcp-budget&quot; budgets = [ { name = &quot;produkt&quot; project_ids = [&quot;project-dev-1&quot;, &quot;project-prod-2&quot;] budget_amount = 500 alert_exceeded_threshold = [0.75, 1.0] alert_forecast_threshold = [1.0] } ] slack_channel_name = &quot;#your-teams-slack-channel&quot; email_address = &quot;alerts@example.com&quot; }   Forklaringer på variabler:  name: Navnet på budsjettet, dere velger selvproject_ids: Prosjektene som budsjettet skal gjelde for. En prosjekt ID finner dere på 'forsiden' til prosjektet på GCP.budget_amount: Beløpet som budsjettet skal varsle om. Dette er i EURO.alert_exceeded_threshold: Dette er en liste med tall som sier hvor mye av budsjettet som skal overskrides før det varsles. Tallene er i desimalformat av prosent, altså 0.75 = 75%. Valgfritt, standard er 0.75 og 1.0.alert_forecast_threshold: Samme som over, men her varsles det om forventet bruk. Valgfritt, standard er 1.0.slack_channel_name: Navnet på slack-kanalen som varsler skal sendes til, husk å inkluder # foran navnet.email_address: E-postadressen som varsler skal sendes til.  README i cost-alerts repoet inneholder mer utfyllende informasjon om bruk av modulen.  ","version":"Next","tagName":"h3"},{"title":"Slack​","type":1,"pageTitle":"Kostnadsoversikt og alarmer","url":"/docs/kom-i-gang/team/kostnadsoversikt-og-alarmer#slack","content":" Dersom dere har lagt inn at det skal varsles til slack, så må dere invitere SKIP Slack Bot til kanalen det skal varsles til.  Gå til slack kanalen og trykk på medlemslisten oppe til høyre i vinduetTrykk på Integrations i menyen som kommer opp, velg deretter Add an AppSøk etter SKIP Slack Bot og trykk på Add ","version":"Next","tagName":"h3"},{"title":"Onboarding av nytt produkt-team til SKIP","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/team/onboarding-new-teams","content":"","keywords":"","version":"Next"},{"title":"SKIP-teamets oppgaver​","type":1,"pageTitle":"Onboarding av nytt produkt-team til SKIP","url":"/docs/kom-i-gang/team/onboarding-new-teams#skip-teamets-oppgaver","content":" ","version":"Next","tagName":"h2"},{"title":"Før onboarding​","type":1,"pageTitle":"Onboarding av nytt produkt-team til SKIP","url":"/docs/kom-i-gang/team/onboarding-new-teams#før-onboarding","content":" Invitere en representant fra produktteamet til plattformlaugetDedikere et SKIP-teammedlem som kontaktpunkt for migreringsprosessen (TAM) (Kun for migreringsprosessen, etter dette starter en vanlig supportflyt)*Invitere til et møte for å avklare forventninger mellom SKIP og produktteametInvitere til gjennomgang av applikasjonerBli enige om frekvensen av onboarding standups med produktteamet og invitere til disseSørge for at en prosess rundt risikovurdering (“ROS-analyse”) startes. Denne vurderingen må være klar i tide til produksjonOpprette en kanal på Slack for samarbeid under onboardingInvitere til #gen-skip, #gen-argo og andre relevante felleskanaler for bruk av SKIPInvitere til GCP- og Kubernetes-kurs hvis produktteamet ønsker detGi en introduksjon til ArgoCD og beste praksis for dette verktøyet  ","version":"Next","tagName":"h3"},{"title":"Under onboarding​","type":1,"pageTitle":"Onboarding av nytt produkt-team til SKIP","url":"/docs/kom-i-gang/team/onboarding-new-teams#under-onboarding","content":" Invitere til et kickoff-møte hvor kontaktpunkter, ansvarsfordeling, support, veikart og andre relevante saker diskuteres.GitHub, gitt at teamet ikke har brukt dette førOpprette grupper ved å legge dem til entra-id-configTeamet må merkes med security i admin.google.com.Teamet må legges til IAM-repositorietWorkflow i IAM-repositoriet må kjøres av et SKIP-medlem med tilgang til dette.Teamene synkroniseres fra AD til IAMHvis teamet krever Terraform: Service account for Terraform settes opp med gcp-service-accounts og gis tilganger til kubernetes namespace via WIF.Terraform state migreres/settes oppTeamet og app-repositoriet settes opp i henhold til Komme i gang med Argo CD Komme i gang med Argo CD  ","version":"Next","tagName":"h3"},{"title":"Produkt-team oppgaver​","type":1,"pageTitle":"Onboarding av nytt produkt-team til SKIP","url":"/docs/kom-i-gang/team/onboarding-new-teams#produkt-team-oppgaver","content":" Produktteamet har ansvaret for å fordele disse oppgavene internt.  Informere SKIP om hvem som er teamleder slik at de kan administrere AD-gruppenVurdere hvilke teammedlemmer som trenger ekstra Kubernetes/GCP-kursHvis ArgoCD skal brukes: Opprette nytt Apps-repo i GitHub basert på denne SKIP malenSørge for at applikasjonen har en IP og/eller DPIATilpasse applikasjonen for å tilfredsstille SKIPs sikkerhetskravLese, forstå og følge GitHub-sikkerhetskravene: Sikkerhet på GitHubFullføre ROS-analyseForberede informasjon til SKIP-teamet Tekniske forventningerTjenestedesign/arkitekturUtenforliggende avhengiheter Ta ansvar for egne krav og kommunisere disse tydelig og konsist til SKIPSørge for at alle teammedlemmer inviteres til møter og Slack-grupper under onboarding-prosessenLese og forstå SKIP-dokumentasjonenGjøre forventet/påkrevd go-live-dato kjent for SKIP ","version":"Next","tagName":"h2"},{"title":"Teknologien bak SKIP","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/tech","content":"Teknologien bak SKIP SKIP er basert på en moderne teknologiplattform bygget rundt Google Cloud, GitOps-prinsipper og innebygget sikkerhet. Med Kubernetes i hjertet av plattformen automatiserer SKIP orkestreringen av containeriserte applikasjoner og sikrer sømløs distribusjon, oppdatering og skalerbarhet. Google Clouds rike økosystem gir tilgang til en rekke tjenester for å administrere og beskytte data og applikasjoner på en god måte. Argo CD gir gjennomgående innsyn og oppdateringshåndtering for Kubernetes-applikasjoner, mens Skiperator gir finjustert kontroll over applikasjonsstyring og konfigurasjon. Med Grafana kan du overvåke ytelsen og helsen til dine tjenester i sanntid, slik at du kan identifisere og håndtere eventuelle problemer raskt og effektivt. Utstrakt bruk av GitHub gjør det lett å dele kode med andre, både internt og eksternt. Byggeløyper i form av GitHub Actions er fleksible og raske å komme i gang med. I tillegg har man tilgang til GitHub Advanced Security for tilgang til blant annet sikkerhetsscanning og sårbarhetsrapporter. Med SKIP er det lett for utviklere å utforske mulighetene innen moderne skyinfrastruktur. Vi på SKIP-teamet hjelper deg med å raskt implementere, administrere og optimalisere skybaserte tjenester, og holde tritt med det stadig skiftende teknologiske landskapet. Velkommen ombord til SKIP!","keywords":"","version":"Next"},{"title":"🥔 Feilsøking","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/troubleshoot","content":"🥔 Feilsøking","keywords":"","version":"Next"},{"title":"Feilsøking på SKIP","type":0,"sectionRef":"#","url":"/docs/kom-i-gang/troubleshoot/troubleshooting-on-skip","content":"","keywords":"","version":"Next"},{"title":"Relevante lenker​","type":1,"pageTitle":"Feilsøking på SKIP","url":"/docs/kom-i-gang/troubleshoot/troubleshooting-on-skip#relevante-lenker","content":" Skiperator kode og dokumentasjon  CLI Jukselapp for SKIP (kan kreve ekstra privilegier)  ","version":"Next","tagName":"h2"},{"title":"Generell sjekkliste ved feilsøking​","type":1,"pageTitle":"Feilsøking på SKIP","url":"/docs/kom-i-gang/troubleshoot/troubleshooting-on-skip#generell-sjekkliste-ved-feilsøking","content":" Nettverk/Istio-relaterte problemer:  Network policies - default-deny and others (if applicable).AccessPolicies both outbound and inbound.ServiceEntries+++ ","version":"Next","tagName":"h2"},{"title":"💾 Lagring","type":0,"sectionRef":"#","url":"/docs/lagring","content":"💾 Lagring","keywords":"","version":"Next"},{"title":"Databaser","type":0,"sectionRef":"#","url":"/docs/lagring/databaser","content":"","keywords":"","version":"Next"},{"title":"Lokal Postgres​","type":1,"pageTitle":"Databaser","url":"/docs/lagring/databaser#lokal-postgres","content":" Dersom man ønsker en lokal postgres tar man kontakt med DBA-ene for å bestille opp en server. Da vil man få en Postgres-database og en administratorbruker som man kan bruke til å opprette tabeller.  For å bestille dette sender man ticket gjennom service desken med hvor mye lagring man trenger og circa hvor mye CPU-kraft man trenger.  Når man har fått en database så er det to ting man må gjøre før man kan ta den i bruk fra en applikasjon på SKIP:  Bestill brannmursåpning for databasen ved å opprette en sak i ServiceNow. F.eks. Jeg ønsker å bestille en brannmursåpning for en database som skal aksesseres fra SKIP. Det er clusteret “atkv1-dev” som trenger å nå “XXXX.statkart.no” på TCP port XXXX.Sett opp tilgang til databasen i Kubernetes. I Skiperator gjøres dette ved hjelp av external accessPolicies. Her må applikasjonen definere at den skal kunne snakke med den eksterne serveren som databasen lever på.  accessPolicy: outbound: external: - host: XXXX.statkart.no ip: &quot;XXX.XXX.XXX.XXX&quot; ports: name: db port: 5432 protocol: TCP   ","version":"Next","tagName":"h2"},{"title":"Database i sky​","type":1,"pageTitle":"Databaser","url":"/docs/lagring/databaser#database-i-sky","content":" Se Cloud SQL for PostgreSQL for mer informasjon om hvordan sette opp og ta bruk Cloud SQL for PostgreSQL. ","version":"Next","tagName":"h2"},{"title":"Objektlagring med Scality S3","type":0,"sectionRef":"#","url":"/docs/lagring/objektlagring-scality-s3","content":"Objektlagring med Scality S3 I Kartverket har vi en lokalt S3-kompatibel lagringsløsning ved navn Scality. Denne er mulig å få tilgang til, og er godt egnet i tilfellet at dere trengre å lagre filer fra en container. Å få tilgang til denne krever følgende: SKIP oppretter bruker og lagringsbøtter for dere i scality-løsningen Admin interface Dere får access key og secret","keywords":"","version":"Next"},{"title":"🔭 Observabilitet","type":0,"sectionRef":"#","url":"/docs/observability","content":"","keywords":"","version":"Next"},{"title":"Hva er observabilitet?​","type":1,"pageTitle":"🔭 Observabilitet","url":"/docs/observability#hva-er-observabilitet","content":" Observabilitet handler om evnen til å forstå den indre tilstanden og oppførselen til et system basert på de eksterne dataene det produserer, som logger, metrikker og sporingsdata (traces). Det gir team muligheten til å få innsikt i hvordan systemet fungerer, identifisere problemer, og diagnostisere årsakene til disse problemene uten å måtte endre systemets kode eller overvåkingsmekanismer. Observabilitet går utover tradisjonell overvåkning ved å tilby en mer fleksibel og dypere analyse av systemets ytelse og helsetilstand, noe som gjør det mulig å stille og besvare nye spørsmål om systemet etter hvert som de oppstår.  ","version":"Next","tagName":"h2"},{"title":"Hva tilbyr SKIP?​","type":1,"pageTitle":"🔭 Observabilitet","url":"/docs/observability#hva-tilbyr-skip","content":" SKIP tilbyr innsamling, visualisering og alarmering basert på telemetri innsamlet fra applikasjonene på SKIP. Telemetrien vi samler inn er metrikker, logger og traces.  Grafana er verktøyet for å søke, visualisere og sammenstille innsamlet telemetri fra ulike kilder. Her kan du også se hvilke alarmer som er konfigurert, alarmhistorikk og hvilke alarmer som går akkurat nå.  ","version":"Next","tagName":"h2"},{"title":"Andre nyttige ressurser​","type":1,"pageTitle":"🔭 Observabilitet","url":"/docs/observability#andre-nyttige-ressurser","content":" Intro to o11y: What is Observability?What is Observability?Bloggen til Charity Majors ","version":"Next","tagName":"h2"},{"title":"Dashboards","type":0,"sectionRef":"#","url":"/docs/observability/dashboard","content":"","keywords":"","version":"Next"},{"title":"Rask intro til dashboards​","type":1,"pageTitle":"Dashboards","url":"/docs/observability/dashboard#rask-intro-til-dashboards","content":" Det er ofte enklere å eksperimentere raskt med data og lage gode spørringer i Explore. Herfra kan du fra &quot;Add&quot;-menyen øverst til høyre trykke &quot;Add to dashboard&quot; og deretter gjøre visualiseringen bedre derfra.    På dashboardet kan du klikke på de tre prikkene øverst til høyre på et panel, og velge &quot;Edit&quot;.    Her kan du velge visualiseringstype oppe til høyre:    De ulike visualiseringstypene har ulike muligheter for tilpasning. De viktigste visualiseringstypene er time series, heatmap, logs, stat og table.  ","version":"Next","tagName":"h2"},{"title":"Dynamiske dashboards med variabler​","type":1,"pageTitle":"Dashboards","url":"/docs/observability/dashboard#dynamiske-dashboards-med-variabler","content":" Variabler kan brukes for å dynamisk velge miljø, namespace eller applikasjon. På denne måten kan dashboards brukes på tvers av team eller miljøer.  Legg til egne variabler ved å trykke på &quot;Settings&quot; oppe til høyre, deretter &quot;Variables&quot;.  Det viktigste variabeltypene er &quot;Data source&quot; og &quot;Query&quot;. Førstnevnte lar deg velge ulike datakilder av samme type dynamisk. &quot;Query&quot; kan brukes for å f.eks. velge namespace.  ","version":"Next","tagName":"h2"},{"title":"Eksempel med variabler​","type":1,"pageTitle":"Dashboards","url":"/docs/observability/dashboard#eksempel-med-variabler","content":" Her er fremgangsmåten for å lage en datasource variabel, metrics, og en query-variabel namespace.  Først data source-variabelen metrics:  Lag en ny variabel, med type &quot;Datasource&quot;. Gi den navnet metrics. Under &quot;Data source options&quot;, velg &quot;Type&quot; Prometheus. Helt nederst kommer det et preview av verdier, og her er det flere datakilder enn ønskelig. Dette kan fikses. Under &quot;Intance name filter&quot;, legg inn regex-en /Mimir-.*(dev|prod|atgcp1)/, som gir datakilder for både sky og on-prem clustre. Trykk &quot;Run Query&quot; helt nederst. Sørg for at alle &quot;Selection options&quot; ikke er valgt, og trykk &quot;Back to list&quot;.  Deretter namespace-variabelen:  Lag en ny variabel, med type &quot;Query&quot; og gi den navnet namespace. Under &quot;Query options&quot; velg datasource ${metrics}. Vi kan nå gjøre spørringer mot den datakilden som er valgt av metrics-variabelen. Velg &quot;Query type&quot; &quot;Label values&quot;. I boksene som dukker opp, velg &quot;Label&quot; namespace og &quot;Metric&quot; up. Trykk deretter på run query. Dette gir ganske mange verdier. Som produktteam vil alle namespaces ofte ha samme prefiks, eks. matrikkel-, norgeskart- eller annet. Vi kan igjen filtere ved hjelp av regex. I &quot;Regex&quot;-feltet, bruk regex /mittprefiks-.*/. Har du flere prefiks, kan du bruke /(prefiks1|prefiks2|prefiks3)-.*/. Deretter trykk på &quot;Run Query&quot; og verifiser at du får opp namespace-navnene du forventer. &quot;Selection options&quot; er her mer relevant, tenk over følgende: Dersom du ønsker å vise ting på tvers av namespaces, velg &quot;Multi-value&quot;. Dette er nyttig i noen tilfeller, men krever at du håndterer det i spørringene i dashboard-panelene.Dersom du ønsker å la bruker skrive inn egne verdier, velg &quot;custom values&quot; (dette er ofte ikke relevant for namespace).Dersom du vil la bruker velge alle namespaces, eller har en spesiell verdi for alle kan du velge denne. Dette krever også at spørringene håndterer det, tilsvarende &quot;multi-value&quot;. Når du er ferdig, trykk &quot;Back to list&quot; nederst for å lage flere variable, eller &quot;Save dashboard&quot; oppe til høyre for å lagre endringene.  Nå kan variablene brukes i dashboard-panelene:  Velg &quot;Data source&quot; ${metrics}, og bruk $namespace-variabelen i spørringene.    📚 Se også Grafanas dokumentasjon om variabler  ","version":"Next","tagName":"h3"},{"title":"Dashboards som kode​","type":1,"pageTitle":"Dashboards","url":"/docs/observability/dashboard#dashboards-som-kode","content":" Det er mulig å lage dashboards som kode, enten egenlaget eller hentet fra eksterne kilder, som f.eks. Grafanas dashboardside. Du kan se eksempler på dette i skip-dashboards.  Dashboards om lages som kode bør ikke endres i GUI-et. For å eksperimentere med slike dashboards anbefaler vi å lage en kopi av dashboardet, eksperimentere og deretter gjøre tilsvarende endringer i kode-repoet. Dette kan gjøres f.eks. ved å trykke &quot;Export &gt; Export as JSON&quot; oppe til høyre i dashboard UI-et.  ","version":"Next","tagName":"h2"},{"title":"Andre ressurser​","type":1,"pageTitle":"Dashboards","url":"/docs/observability/dashboard#andre-ressurser","content":" Det er mange måter å lage dashboards på, og det finnes gode artikler på det:  📚 Enkel introduksjon fra Grafana for å lage dashboards📚 Grafanas egne &quot;best practices&quot; for å lage dashboards📚 The Four Golden Signals fra Googles SRE Book diskuterer de viktigste metrikkene å monitorere ","version":"Next","tagName":"h2"},{"title":"Grafana","type":0,"sectionRef":"#","url":"/docs/observability/grafana","content":"","keywords":"","version":"Next"},{"title":"Rask rundtur i Grafana​","type":1,"pageTitle":"Grafana","url":"/docs/observability/grafana#rask-rundtur-i-grafana","content":" Når du kommer til vår Grafana-instans kan du velge &quot;Sign in with Azure AD&quot;. Om du ikke kommer inn kan det være du ikke er lagt til i teamets AD-gruppe, eller at teamet ditt ikke er onboardet på SKIP. Ta kontakt i #gen-skip på Slack for spørsmål.  Venstre sidemeny lar deg navigere dit du trenger, og viser de viktigste tingene du trenger i Grafana.    Som produktteam på SKIP er de viktigste egenskapene i Grafana:  Utforsk data i Explore-mode. Her kan du utforske både logger, metrikker og traces.Grafana har også en egen Metrics explore som lar deg utforske hvilke metrikker som finnes. Lag dashboards som gir oversikt over hva som skjer i applikasjonene dine. Få oversikt over og håndter alarmer. Vi anbefaler at du har alarmer som kode, men du kan også konfigurere alarmer manuelt her. Testing og synthetics brukes for syntetisk monitorering. ","version":"Next","tagName":"h2"},{"title":"Distributed tracing with Tempo","type":0,"sectionRef":"#","url":"/docs/observability/distributed-tracing-with-tempo","content":"","keywords":"","version":"Next"},{"title":"What is distributed tracing?​","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#what-is-distributed-tracing","content":" In complex (and distributed) systems there are at any time many ongoing parallel processes. Some of these are interlinked or trigger each other. In order to find out which operations that originate from the same request, it is common in many systems to have a so-called Trace ID. With modern distributed tracing this is standardized, and in addition sub-operations (spans) per Trace ID are also supported. When you use a standardized setup to trace applications you also gain access to a large and exciting toolbox.  Further reading:  OpenTelemetryZipkin (interesting from a historical perspective)A general guide to getting started with distributed tracing  ","version":"Next","tagName":"h2"},{"title":"What does SKIP offer?​","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#what-does-skip-offer","content":" As part of our implementation of the LGTM stack, SKIP has chosen to offer Grafana Tempo as as service. This is a component that is fully integrated with the rest of this modern observability stack, and shares the same user interface and authentication as Grafana, Mimir and Loki.  ","version":"Next","tagName":"h2"},{"title":"How do I get started?​","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#how-do-i-get-started","content":" ","version":"Next","tagName":"h2"},{"title":"Instrumentation​","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#instrumentation","content":" warning A known limitation in the way we have collected trace data is that we up until recently have had no way of excluding certain traces automatically. This means that all Prometheus scrapes (metrics collection) and automatic health checks will also be collected. Now that issue #4628has been implemented, this can finally be rectified. Follow SKIP-1250 for updates to when this is implemented in our setup.  In order to generate, propagate and send traces the application must be instrumented.  Instrumentation can be achieved in several ways, of which 2 are relevant to us: manual and automatic instrumentation.  Manual instrumentation requires the use of a library which knows how a given integration behaves, and which enables it to connect to hooks in that integrations in order to generate new traces and/or spans if those do not already exist.  The other (and recommended) method is to use an automated approach. In the case of Java applications (the only type that has been tested as of now), you will need to bundle a java agent in your Docker image, as well as set up some extra configuration when the application is run (for example through Skiperator).  info It’s worth mentioning that the Spring ecosystem offers a form of automatic instrumentation via Micrometer Tracing and OpenTelemetry OTLP exporters. Per october 2023 this is still under development and not considered a mature enough solution to utilize in our systems.  ","version":"Next","tagName":"h3"},{"title":"Example Dockerfile​","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#example-dockerfile","content":" FROM alpine:3.18.3@sha256:c5c5fda71656f28e49ac9c5416b3643eaa6a108a8093151d6d1afc9463be8e33 AS builder ARG OTEL_AGENT_VERSION=1.29.0 # 1. Last ned påkrevd java-agent RUN apk add --no-cache curl \\ &amp;&amp; mkdir /agents \\ &amp;&amp; curl -L https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/download/v${OTEL_AGENT_VERSION}/opentelemetry-javaagent.jar &gt; /agents/opentelemetry.jar ADD build/distributions/gbok-run*.tar /gbok FROM eclipse-temurin:11-jdk-alpine COPY cert/kartverket_root_ca.crt /usr/local/share/ca-certificates/kartverket_root_ca.crt ENV USER_ID=150 ENV USER_NAME=apprunner RUN apk add --no-cache tzdata \\ &amp;&amp; addgroup -g ${USER_ID} ${USER_NAME} \\ &amp;&amp; adduser -u ${USER_ID} -G ${USER_NAME} -D ${USER_NAME} \\ &amp;&amp; keytool -import -v -noprompt -trustcacerts -alias kartverketrootca -file /usr/local/share/ca-certificates/kartverket_root_ca.crt -keystore $JAVA_HOME/lib/security/cacerts -storepass changeit ENV TZ=Europe/Oslo COPY --from=builder --chown=${USER_ID}:${USER_ID} /gbok /gbok # 2. Kopier inn nedlastet agent COPY --from=builder --chown=${USER_ID}:${USER_ID} /agents /agents USER ${USER_NAME} EXPOSE 8081 ENTRYPOINT [&quot;sh&quot;, &quot;-c&quot;, &quot;/gbok/gbok-run*/bin/gbok-run&quot;]   ","version":"Next","tagName":"h3"},{"title":"Runtime configuration​","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#runtime-configuration","content":" In order to use the Java agent, it needs to be configured and loaded. Through testing with Grunnboken, we have arrived at the first version of configuration which can be seen here .  When this configuration is done, it is then passed to JAVA_TOOL_OPTIONS like this .  There is currently no inbuilt mechanism in ArgoKit to achieve this. We are open for PRs on this topic if anyone would like to contribute.  ","version":"Next","tagName":"h3"},{"title":"View traces​","type":1,"pageTitle":"Distributed tracing with Tempo","url":"/docs/observability/distributed-tracing-with-tempo#view-traces","content":" Traces can be viewed through our Grafana instance at monitoring.kartverket.cloud . From here, choose Explore in the menu and then the correct Tempo data source corresponding to the environment you wish to view traces for.  After that, you have the choice of using the Search (graphical build tool for queries) or TraceQL (manual query specification) tools.  Above: The “Search” tab is active, and fields have been filled through the use of dropdowns.  Above: The “TraceQL” tab lets you specify a user-defined query. Here is shown a query for “gbok2-server” traces, filtering out health checks ","version":"Next","tagName":"h3"},{"title":"Grafana and GCP","type":0,"sectionRef":"#","url":"/docs/observability/grafana-and-GCP","content":"","keywords":"","version":"Next"},{"title":"Google Cloud Monitoring​","type":1,"pageTitle":"Grafana and GCP","url":"/docs/observability/grafana-and-GCP#google-cloud-monitoring","content":" It is possible to get metrics from a Google Cloud project by the use of the Grafana data source “Google Cloud Monitoring”.    Through the use of this data source, you will be able to see all metrics that are exposed through different Google Cloud services, such as CloudSQL, BigQuery, CloudKMS, Logging etc. This can then be added to your dashboards and alarms.  ","version":"Next","tagName":"h2"},{"title":"Setting up the data source​","type":1,"pageTitle":"Grafana and GCP","url":"/docs/observability/grafana-and-GCP#setting-up-the-data-source","content":" While the data source is present, it will not scrape all projects in the Kartverket organisation in GCP by default. As of writing this (13 Oct 2023), SKIP does not facilitate this setup in any particular way, but you are free to do it the “SKIP way”.  To add your GCP project to the list of projects, simply add the GCP role monitoring.viewer to the Google Service Account grafana-scraper@kubernetes-0dca.iam.gserviceaccount.com. It should look like the below image.    Remember that if you do not have access to editing IAM for your projects by default, you can always elevate your access using JIT Access .  Note that the setup for this may change in the future as this feature is somewhat unexplored as of writing this documentation. ","version":"Next","tagName":"h3"},{"title":"Cloud SQL for PostgreSQL","type":0,"sectionRef":"#","url":"/docs/lagring/cloud-sql","content":"","keywords":"","version":"Next"},{"title":"Oppsett av instanser med terraform​","type":1,"pageTitle":"Cloud SQL for PostgreSQL","url":"/docs/lagring/cloud-sql#oppsett-av-instanser-med-terraform","content":" SKIP har laget to terraform-moduler (cloud_sql ogcloud_sql_config) for å gjøre det enkelt å sette opp nye Cloud SQL-instanser i GCP.  Dokumentasjon for hvordan modulene brukes finnes på wiki-siden til terraform-modulesSpesielt guiden for hvordan bruke terraform-modules repoet er relevant.  ","version":"Next","tagName":"h2"},{"title":"cloud_sql modulen​","type":1,"pageTitle":"Cloud SQL for PostgreSQL","url":"/docs/lagring/cloud-sql#cloud_sql-modulen","content":" For mer utfyllende dokumentasjon se cloud_sql wiki  module &quot;cloudsql_test&quot; { source = &quot;git@github.com:kartverket/terraform-modules.git/?ref=cloud_sql/v0.10.0&quot; env = &quot;sandbox&quot; instance_name = &quot;foo-db&quot; project_id = &quot;skip-sandbox-37c2&quot; }   Du kan koble deg til på denne måten:  JIT deg til cloudsql.adminLast ned cloudsql-proxygcloud auth application-default login./cloud-sql-proxy --private-ip &lt;connection-name&gt; --auto-iam-authn -- connection name finner du på sql instansen i GCPpsql -d admin -h localhost -U admin eller fra applikasjon  Du må være på Kartverkets nettverk for å få tilgang, selv med cloud sql proxy. Man kan ikke koble til fra egen klient uten proxy. Du trenger ikke å bruke SSL sertifikater når du kobler til via proxy.  ","version":"Next","tagName":"h3"},{"title":"cloud_sql_config modulen og konfigurering av brukere​","type":1,"pageTitle":"Cloud SQL for PostgreSQL","url":"/docs/lagring/cloud-sql#cloud_sql_config-modulen-og-konfigurering-av-brukere","content":" For mer utfyllende dokumentasjon se cloud_sql_config wiki  Denne modulen er laget for konfigurasjon av postgres instanser. Vi har laget denne for å gjøre konfigurering av databaser enklest mulig for dere, og for å unngå &quot;click-ops&quot;. Det er noen ting dere bør tenke over før dere tar denne i bruk:  Den burde bare brukes på en ny instans. Å importere eksisterende databaser, brukere og skjemaer er noe vi fraråderFeil bruk av denne modulen kan slette brukere, secrets og hele databasen inkludert all data. Sjekk alltid PLAN før du applyer.Vær sikker på at migreringene dine er kompatible med modulen mtp. privileges  Eksempel config:  module &quot;cloudsql_config&quot; { source = &quot;git@github.com:kartverket/terraform-modules.git/?ref=cloud_sql_config/v0.7.0&quot; gcp_instance_name = module.cloudsql_test.cloud_sql_instance_name gcp_project_id = module.cloudsql_test.gcp_project_id env = &quot;prod&quot; databases = { &quot;backstage&quot; = { name = &quot;backstage&quot; owner = &quot;backstage&quot; extensions = [&quot;pgcrypto&quot;, &quot;postgis&quot;] prevent_destroy = true # Denne variabelen må IKKE endres uten at dere er klare til å migrere state manuelt. schemas = [ { name = &quot;backstage&quot; migration_user = { name = &quot;backstage_migrater&quot; # migration_user blir eier av skjemaet som opprettes } application_user = { name = &quot;backstage_app&quot; # application user får CRUD privilegier } misc_users = [ { name = &quot;readonly&quot; privileges = [&quot;SELECT&quot;] } ] }, { name = &quot;opencost&quot; unified_user = true migration_user = { name = &quot;opencost_migrater&quot; # ignoreres, fordi vi har unified_user = true, men den må settes likevel } application_user = { name = &quot;opencost_app&quot; privileges = [&quot;SELECT&quot;, &quot;UPDATE&quot;] # ignoreres, app user blir owner av skjemaet fordi unified_user er true } misc_users = [ { name = &quot;readonly&quot; privileges = [&quot;SELECT&quot;] } ] } ] } } }   For hver bruker så vil modulen generere opp et klient sertifikat og en privatnøkkel, disse legges i GSM. Den private nøkkelen legges i to formater; PEM og PK8. Vi har erfart at JDBC ikke liker PEM, så i dette tilfellet så bør du bruke PK8 nøkkelen istedenfor.  ","version":"Next","tagName":"h3"},{"title":"Bruke instansen fra SKIP​","type":1,"pageTitle":"Cloud SQL for PostgreSQL","url":"/docs/lagring/cloud-sql#bruke-instansen-fra-skip","content":" Når du skal bruke instansen fra SKIP så må du gjøre noen få modifikasjoner til applikasjonsmanifestet ditt.  Det første du må gjøre er å hente ut secrets.  Terraform modulen vil generere opp og legge inn alle secrets du trenger for å koble til databasen i Google Secret Manager i prosjektet du har valgt.  Kjenner du ikke til GSM og ExternalSecrets anbefaler vi å lese Hente hemmeligheter fra hemmelighetshvelv først.  For å hente ut disse så må du lage to ExternalSecret, en for sertifikater og en for passord/brukernavn, her er et eksempel:  apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: minapp-hemmligheter spec: secretStoreRef: kind: SecretStore name: gsm data: - secretKey: db_password remoteRef: key: cloudsql-&lt;instansnavn&gt;-&lt;bruker&gt;-password --- apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: database-certs spec: secretStoreRef: kind: SecretStore name: gsm data: - secretKey: server.crt remoteRef: key: cloudsql-&lt;instansnavn&gt;-ca-certificate - secretKey: client.crt remoteRef: key: cloudsql-&lt;instansnavn&gt;-&lt;bruker&gt;-client-certificate ### client.key i PEM, fungerer for de fleste - secretKey: client.key remoteRef: key: cloudsql-&lt;instansnavn&gt;-&lt;bruker&gt;-client-key ### VISST DU TRENGER PK8 (JDBC kan kreve dette) - Bare ta med ÈN key, ikke begge - secretKey: client.pk8 remoteRef: decodingStrategy: Base64 # Må være med for pk8 key: cloudsql-&lt;instansnavn&gt;-&lt;bruker&gt;-client-key-pk8   Nå har du hentet alle hemmelighetene du trenger, og kan bruke disse i skiperator manifestet ditt:  apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: minapp spec: image: ghcr.io/kartverket/minapp port: 8080 replicas: 2 accessPolicy: outbound: external: - host: &lt;instansnavn&gt;-db-&lt;env&gt; # Velg selv hva du vil kalle denne, så lenge den er unik ip: 10.x.x.x # Privat IP-adresse til databasen, den finner du i GCP ports: - name: sql port: 5432 protocol: TCP env: ## DISSE env VERDIENE ER EKSEMPLER, OG MÅ JUSTERES FOR HVER APPLIKASJON - name: POSTGRES_DB value: minapp - name: POSTGRES_USER value: minappbrukernavn - name: POSTGRES_PASSWORD valueFrom: secretKeyRef: key: db_password name: minapp-hemmligheter - name: PGSSLCA value: /app/db-certs/server.crt - name: PGSSLKEY value: /app/db-certs/client.key - name: PGSSLCERT value: /app/db-certs/client.crt filesFrom: - mountPath: /app/db-certs secret: database-certs   Skiperator vil nå:  lage en NetworkPolicy som gir applikasjonen tilgang til databasen'mounte' sertifikatene inn i filsystemet til poden, slik applikasjonen kan bruke de til å koble til databasenlaste inn passord hemmeligheten som en env variabel inn i poden, slik applikasjonen kan koble til databasen  ","version":"Next","tagName":"h2"},{"title":"Bruke CloudSQL fra Java applikasjoner​","type":1,"pageTitle":"Cloud SQL for PostgreSQL","url":"/docs/lagring/cloud-sql#bruke-cloudsql-fra-java-applikasjoner","content":" Skal du bruke CloudSQL fra Java applikasjoner må du lage til ExternalSecrets og konfigurere skiperator som ovenfor, men bruk pk8 nøkkel istedenfor vanlig pem nøkkel. Det skal være nok å konfigurere en connection string som ser noe slik ut postgresql://&lt;privat-ip&gt;:5432/&lt;database-navn&gt;?sslmode=require&amp;sslrootcert=/app/db-certs/server.crt&amp;sslcert=/app/db-certs/client.crt&amp;sslkey=/app/db-certs/client.pk8  Alternativt kan man også bruke en Cloud Sql Auth Proxy connector, men da vil man få litt dårligere ytelse. Bruker man en connector så slipper man å bruke certs, men man må åpne for port 3307 mot databasen i access policies i skiperator manifestet.  ","version":"Next","tagName":"h3"},{"title":"Monitorering og alarmering​","type":1,"pageTitle":"Cloud SQL for PostgreSQL","url":"/docs/lagring/cloud-sql#monitorering-og-alarmering","content":" Fungerer bare dersom dere bruker høyere versjon enn 0.9.1 av cloud_sql modulen.  Vi har laget et dashboard, sammen med DBAene, for monitorering av CloudSQL databasene, som kan finne her. I tillegg så finnes også Googles standard dashboard og metrikker som man kan finne inne på CloudSQL ressursen i GCP consolen.  For alarmering så brukes grafana-alerts repoet som for alle andre type alerts. Her har vi utviklet en cloud_sql_alerts modul som gir dere et sett med standard alarmer. Metrikker vi baserer oss på blir hentet ut ved hjelp av sql_exporter, disse metrikkene er hentet ut på grunnlag av SQL-spørringer som DBAene har predefinert. Ønskes det andre metrikker så ta kontakt med dem.  Det er også tilgjengelig et sett med standardmetrikker fra Google gjennom grafana, for å bruke disse så gå inn i explore view i grafana. Velg Google Cloud Monitoring som datasource, og velg prosjektet ditt og Cloudsql som service. Se på cloud_sql_alerts modulen dersom du ønsker å se hvordan de kan brukes i en alert.  ","version":"Next","tagName":"h2"},{"title":"Backup og katastrofehåndtering​","type":1,"pageTitle":"Cloud SQL for PostgreSQL","url":"/docs/lagring/cloud-sql#backup-og-katastrofehåndtering","content":" ","version":"Next","tagName":"h2"},{"title":"Backup​","type":1,"pageTitle":"Cloud SQL for PostgreSQL","url":"/docs/lagring/cloud-sql#backup","content":" CloudSQL er en google managed løsning av postgres, og det betyr også at det har et innebygd backup system, og håndteres i gcp console. Dette systemet tar automatisk backup av databasen din, og lagrer disse i 7 dager som standard. Hvis du har behov for å bevare backups lengre enn dette kan det konfigureres med en variabel til terraform-modulen, ref: input_retained_backupsBackupen er inkrementel og man har Point-in-time recovery tilgjengelig. Vi anbefaler at du leser gjennom Google sin dokumentasjon for å forstå hvordan backup fungerer i CloudSQL.  ","version":"Next","tagName":"h3"},{"title":"Katastrofehåndtering​","type":1,"pageTitle":"Cloud SQL for PostgreSQL","url":"/docs/lagring/cloud-sql#katastrofehåndtering","content":" Google Cloud SQL har innebygd failover, og det betyr at dersom primærinstansen din går ned, så vil en av de tilgjengelige replicaene ta over. Dette må konfigureres i terraform, ved bruk av availability_type variabelen, default på denne er ZONAL som betyr at du ikke får en secondary instans. I produksjon er det anbefalt å ha en secondary instans, og da må availability_type settes til REGIONAL i terraform. Les mer her: Google sin dokumentasjon ","version":"Next","tagName":"h3"},{"title":"Alerting with Grafana","type":0,"sectionRef":"#","url":"/docs/observability/alerting-with-grafana","content":"","keywords":"","version":"Next"},{"title":"Creating alerts​","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#creating-alerts","content":" The first step to start adding alerts to your application is to onboard that app to SKIP. Grafana is only used for SKIP, the rest of Kartverket uses Zabbix. Once you have been onboarded and deployed your app to SKIP you can request access to the grafana-alerts repo.  The grafana-alerts repo is designed to be a repo that contains the alerts of all teams and handles deployment of alerts to Grafana. You will get a file which contains the configuration of your alerts in a Terraform format. For example, the file could look like this:  resource &quot;grafana_folder&quot; &quot;MYTEAMNAME_folder&quot; { for_each = local.envs title = &quot;Alerts MYTEAMNAME ${each.key}&quot; } module &quot;MYTEAMNAME_alerts_kubernetes&quot; { source = &quot;../modules/grafana_alert_group&quot; for_each = local.envs name = &quot;kube-state-metrics&quot; env = each.value runbook_base_url = # URL to document describing each alert folder_uid = grafana_folder.MYTEAMNAME_folder[each.key].uid team = { name = &quot;MYTEAMNAME&quot; } alerts = { KubernetesPodNotHealthy = { summary = &quot;Kubernetes Pod not healthy (instance {{ $labels.instance }})&quot; description = &quot;Pod has been in a non-ready state for longer than 15 minutes.\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}&quot; severity = &quot;critical&quot; for = &quot;15m&quot; expr = &lt;&lt;EOT sum by (namespace, pod) (kube_pod_status_phase{phase=~&quot;Pending|Unknown|Failed&quot;, namespace=~&quot;nrl-.*&quot;}) EOT }, # ... more alerts } }   In the above file we create an alert that monitors the health of a pod in all nrl namespaces. Pay attention to the expr field, which is the Prometheus query language PromQL. If you want to learn more about PromQL look at the documentation as well as some examples from the Prometheus documentation and the examples at awesome prometheus alerts .  This is a file that you will be given CODEOWNER access to. This means that you and your team will be able to update this file and review your own changes without involving SKIP. Your team is expected to keep them at a level that verifies the running state of the application.  Updating this file in the GitHub repo will automatically deploy the changes to Grafana.  ","version":"Next","tagName":"h2"},{"title":"Grafana Oncall Alerts​","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#grafana-oncall-alerts","content":" In addition to Grafana alerts, we have installed a plugin to Grafana called Oncall. This plugin gives us the possibility of adding schedules/shifts and custom alerting behaviour. It also gives your team an overview and a system to handle alerts.    ","version":"Next","tagName":"h2"},{"title":"Integration​","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#integration","content":" In order to start using Oncall you need an oncall integration to Grafana. This integration will show up as a contact point in Grafana which can be used in notification policies to route alerts to your integration.  From the integration you can add routes and escalation chains which decides how the integration will notify the team. The standard setup is to send all alerts to a slack channel, and also to a team member on schedule or shared inbox.      In the grafana-alerts repository we have created an oncall_integration module, which you can use to create your teams integration.  ","version":"Next","tagName":"h3"},{"title":"Routes​","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#routes","content":" In an integration you always have a default route, but you can also have a specified route. A route will decide which escalation chain the integration should use when it receives an alert. For example if you have a critical app that requires 24/7 alerting, you can create a route that checks for certain labels, and if found, it will route the alert to the “appdrift” escalation chain.  ","version":"Next","tagName":"h3"},{"title":"Schedules​","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#schedules","content":" An Oncall Schedule is a collection of “Shifts”. In short this means that you can assign a person to a shift, and that person will receive all alerts sent to the Oncall integration for the duration of their shift. In the grafana-alerts repository you can use the oncall_team integration to create both a schedule and escalation chain.    ","version":"Next","tagName":"h3"},{"title":"Escalation Chains​","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#escalation-chains","content":" Escalation chains are instructions to Oncall on how to notify you when the connected integration receives an alert. The standard setup here is to contact the assigned person in the set way in Oncall.  The escalation chain below will contact the person which has an assigned shift in Schedule, in the way they have set in Oncall. Usually email or slack mentions.    In Oncall → Users → edit user, you can decide how you want the escalation chain to contact you.    ","version":"Next","tagName":"h3"},{"title":"Terraform​","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#terraform","content":" A typical Grafana Oncall setup for a team will look like this:  module &quot;team_oncall&quot; { source = &quot;../modules/oncall_team&quot; team_name = &quot;team&quot; use_schedule = true } module &quot;team_integration&quot; { source = &quot;../modules/oncall_integration&quot; integration_name = &quot;team&quot; slack_channel_name = &quot;grafana-oncall&quot; //Not required, replace with your own vaktlag_enabled = false default_escalation_chain_id = module.team_oncall.team_escalation_chain_id }   note The slack channel must already exist in grafana. If you want to use predefined users instead of a schedule, then the users must already exist in Oncall.  ","version":"Next","tagName":"h3"},{"title":"Notification policies​","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#notification-policies","content":" You also have to configure notification policies to use your integration. Terraform doesn’t activate the contact point of the integration yet, so this has to be done manually before adding this to terraform(do this by navigating to your integration and activate the contact point). Add code here.  Example:  policy { contact_point = &quot;watchdog&quot; group_by = [&quot;cluster&quot;, &quot;alertname&quot;] matcher { label = &quot;team&quot; match = &quot;=&quot; value = &quot;Vaktlag&quot; } }   ","version":"Next","tagName":"h3"},{"title":"24/4 alerting​","type":1,"pageTitle":"Alerting with Grafana","url":"/docs/observability/alerting-with-grafana#244-alerting","content":" Once you have configured a set of alerts, you might want them to be monitored 24/7. Kartverket provides a solution for this in the form of “Vaktlaget”. Vaktlaget is a team consisting of various people in IT-drift that have a special agreement that allows them to be notified and follow up when an alert fires outside of normal working hours.  The first step for getting your alerts onboarded onto vaktlaget is to maintain a set of alerts that only fire when there is a serious outage. Keep in mind that an alert that fires will potentially wake people in the middle of the night, so it is paramount that this set of alerts don’t contain non-critical or “flaky” alerts. These alerts should be given a severity of “critical” to make them distinct from other alerts.  Once you have done this you need to contact vaktlaget to discuss the alerts you wish to onboard. They will comment on what is important enough to be onboarded and you will end up with a set of alerts that is a neat balance between ensuring the stability of our systems and preserving the mental health of the people on the alert schedule.  After you’ve discussed with vaktlaget you can contact SKIP in #gen-skip to have your alert integration be switched over. When this is done, all alerts labeled with env=prod and severity=critical will be sent to vaktlaget using the following schedule:  The alerts will be sent to your slack channel all dayThe alerts will be sent to appdrift as email, SMS and phone call between 7 and 22The alerts will be sent to infrastrukturdrift as email, SMS and phone call between 22 and 7  You can also create a pull request in grafana-alerts with the vaktlag escalation chain added to your integration:  module &quot;skip&quot; { source = &quot;../modules/oncall_integration&quot; integration_name = &quot;skip&quot; slack_channel_name = &quot;grafana-oncall&quot; //Not required, replace with your own vaktlag_enabled = true vaktlag_escalation_chain_id = module.vaktlag.appdrift_escalation_chain_id default_escalation_chain_id = module.skip.team_escalation_chain_id }   When you later add more alerts to the critical level you also need to discuss with vaktlaget so they can sign off on the new alerts before they are added. ","version":"Next","tagName":"h2"},{"title":"Logs with Loki","type":0,"sectionRef":"#","url":"/docs/observability/logs-with-Loki","content":"Logs with Loki SKIP’s LGTM stack is set up to automatically collect logs from all applications running in our Kubernetes clusters. There is nothing in particular you as a developer need to configure or set in order to achieve this, apart from ensuring that your application logs to stdout . These are picked up by the Grafana Agent through the PodLogs custom resource, which specifies which namespaces to collect logs for (all of them in this case) and a set of relabeling rules to ensure that we have a common set of labels for use in searching, dashboards and alerting. Logs are collected and stored in Loki, which is backed by an on-premise S3-compatible Scality storage bucket system, one for each cluster. Each Loki instance is defined as a data source in Grafana, which provides the tools for search queries, dashboards and alerting. For an overview of the Explore section as it pertains to Loki, see https://grafana.com/docs/grafana/latest/explore/logs-integration/ . This and other pages outline the features and how to use it efficiently in relatively good detail, so we shall not attempt to reproduce such a guide here, only to point out a few things as they apply to our own setup. By necessity, the default label set is rather limited compared to what some of you might wish. This is because a large selection of labels can be extremely detrimental to performance - see https://grafana.com/docs/loki/latest/get-started/labels/bp-labels/ for an explanation. Hence, it is recommended to use filter expressions instead. You can filter on log lines containing/not containing a given text, regex expression and a host of other possibilities. The search function is also equipped with a JSON parser which makes it easier to filter on the fields you want. You can choose between two modes of searching: typing a query manually, or building a query through Grafana’s graphical query builder. As long as the query you have built or typed is valid, you can seamlessly switch between the two modes. Above: Using JSON parser to extract fields and filtering on method “POST”","keywords":"","version":"Next"},{"title":"Grafana cheat sheet","type":0,"sectionRef":"#","url":"/docs/observability/grafana-cheat-sheet","content":"","keywords":"","version":"Next"},{"title":"Useful Mimir queries​","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#useful-mimir-queries","content":" ","version":"Next","tagName":"h2"},{"title":"Top 20 of metrics with high cardinality​","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#top-20-of-metrics-with-high-cardinality","content":" https://monitoring.kartverket.cloud/goto/cc_GwW1SR?orgId=1  # Set time range to &quot;Last 5 minutes&quot; topk(20, count by (__name__)({__name__=~&quot;.+&quot;}))   ","version":"Next","tagName":"h3"},{"title":"Top 10 namespaces with overallocated cpu resources​","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#top-10-namespaces-with-overallocated-cpu-resources","content":" https://monitoring.kartverket.cloud/goto/6V2jQZJIg?orgId=1  topk(10, sum by (namespace) (kube_pod_container_resource_requests{job=&quot;integrations/kubernetes/kube-state-metrics&quot;, resource=&quot;cpu&quot;}) - sum by (namespace) (rate(container_cpu_usage_seconds_total{}[$__rate_interval])))   ","version":"Next","tagName":"h3"},{"title":"Sum of overallocated cpu for containers by namespace​","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#sum-of-overallocated-cpu-for-containers-by-namespace","content":" https://monitoring.kartverket.cloud/goto/xF2DlW1SR?orgId=1  sum by (container) (kube_pod_container_resource_requests{job=&quot;integrations/kubernetes/kube-state-metrics&quot;, resource=&quot;cpu&quot;, namespace=~&quot;matrikkel.*&quot;}) - sum by (container) (rate(container_cpu_usage_seconds_total{namespace=~&quot;matrikkel.*&quot;}[$__rate_interval]))   ","version":"Next","tagName":"h3"},{"title":"Daily amount of requests by destination app and response code​","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#daily-amount-of-requests-by-destination-app-and-response-code","content":" sum by (destination_app, response_code) ( increase(istio_requests_total{namespace=&quot;&lt;namespace name&gt;&quot;, response_code=~&quot;.*&quot;, source_app=&quot;istio-ingress-external&quot;}[1d]) )   ","version":"Next","tagName":"h3"},{"title":"Useful Loki queries​","type":1,"pageTitle":"Grafana cheat sheet","url":"/docs/observability/grafana-cheat-sheet#useful-loki-queries","content":"","version":"Next","tagName":"h3"},{"title":"Metrikker","type":0,"sectionRef":"#","url":"/docs/observability/metrikker","content":"","keywords":"","version":"Next"},{"title":"Eksponer metrikker fra applikasjonen​","type":1,"pageTitle":"Metrikker","url":"/docs/observability/metrikker#eksponer-metrikker-fra-applikasjonen","content":" For å kunne eksponere metrikker fra applikasjonen må du gjøre følgende:  Eksponer metrikker på et eget endepunkt, f.eks. /metrics. Det beste her er også å eksponere metrikker på en egen port for å unngå at endepunktet uheldigvis eksponeres eksternt. Om du ikke kan gjøre dette, ta kontakt med SKIP for å sammen sørge for at data ikke eksponeres eksternt. Endre Skiperator-manifestet ved å legge til en ekstra port og tillate innhenting av metrikker; apiVersion: skiperator.kartverket.no/v1alpha1 kind: Application metadata: name: super-app namespace: team-foo-main spec: image: &quot;kartverket/example&quot; port: 8080 # Definer egen port additionalPorts: - name: management port: 8181 protocol: TCP # Skru på innsamling av metrikker fra den nye porten prometheus: port: management path: &quot;/metrics&quot;   ","version":"Next","tagName":"h2"},{"title":"Utforsk metrikker i Metrics Explore​","type":1,"pageTitle":"Metrikker","url":"/docs/observability/metrikker#utforsk-metrikker-i-metrics-explore","content":" For å finne ut hvilke metrikker du har tilgjengelig for applikasjonen, sjekk ut Metrics Explore.    Bruk labels for å filtrere på f.eks. namespace, application eller container for å finne metrikker som er interessante, og søk på metrikknavn i feltet. Du kan velge &quot;Select&quot; for å se videre på en enkeltmetrikk, og trykke på kompassikonent for å gå videre til Explore for å gjøre metrikkspørringen mer nøyaktig.  Se også:  📚 Grafanas egen dokumentasjon beskriver alle detaljene.🎬 Grafana har også en demovideo.  ","version":"Next","tagName":"h2"},{"title":"Lag spørringer i Explore​","type":1,"pageTitle":"Metrikker","url":"/docs/observability/metrikker#lag-spørringer-i-explore","content":" Explore lar deg videreforedle spørringer, eksperimentere og grave raskt.    Her kan du gjøre spørringer i split screen fra ulike datakilder, korrelere data fra både logger, metrikker og traces, samt legge ferdige spørringer som paneler i dashboards.  Se også:  📚 Grafanas dokumentasjon ","version":"Next","tagName":"h2"},{"title":"Recording and alerting rules","type":0,"sectionRef":"#","url":"/docs/observability/recording-and-alerting-rules","content":"","keywords":"","version":"Next"},{"title":"Recording rules in Mimir​","type":1,"pageTitle":"Recording and alerting rules","url":"/docs/observability/recording-and-alerting-rules#recording-rules-in-mimir","content":" SKIP supports https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/ through Mimir and Grafana Agent. Just define a PrometheusRule object in your application’s namespace, and Grafana Agent will pick it up and add it to Mimir.  For more information, see the Prometheus documentation above, or visit https://grafana.com/docs/grafana/latest/alerting/alerting-rules/create-mimir-loki-managed-recording-rule/  ","version":"Next","tagName":"h2"},{"title":"Recording rules in Loki​","type":1,"pageTitle":"Recording and alerting rules","url":"/docs/observability/recording-and-alerting-rules#recording-rules-in-loki","content":" TODO ","version":"Next","tagName":"h2"},{"title":"🚦 Trafikkstyring","type":0,"sectionRef":"#","url":"/docs/trafikkstyring","content":"🚦 Trafikkstyring Under denne siden finner du artikler som omhandler oppsett og bruk av Google Cloud Platform.","keywords":"","version":"Next"},{"title":"Real User Monitoring with Faro","type":0,"sectionRef":"#","url":"/docs/observability/real-user-monitoring-with-Faro","content":"","keywords":"","version":"Next"},{"title":"Getting started​","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#getting-started","content":" Setting up Faro requires two steps which are explained below:  Installing the SDKConfiguring the SDK  It will also be useful to start by reading the Faro quick start guide . See also the README of the Faro GitHub page for more links to relevant documentation.  ","version":"Next","tagName":"h2"},{"title":"Installing the SDK​","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#installing-the-sdk","content":" If you use React this is done by running one of the following commands:  # If you use npm npm i -S @grafana/faro-web-sdk # If you use Yarn yarn add @grafana/faro-web-sdk   ","version":"Next","tagName":"h3"},{"title":"Configuring the SDK​","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#configuring-the-sdk","content":" Import and configure the following options in your app’s entrypoint (main.js or similar).  import { initializeFaro } from &quot;@grafana/faro-react&quot;; initializeFaro({ app: { name: &quot;my_app_name&quot;, environment: getCurrentEnvironment(), }, url: &quot;https://faro.atgcp1-prod.kartverket.cloud/collect&quot;, });   ","version":"Next","tagName":"h3"},{"title":"List of valid options for app​","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#list-of-valid-options-for-app","content":" \tType\tDescription\tRequired?name\tstring\tThe name of the application as it will appear on dashboards in Grafana\tYes environment\t“localhost” | “dev” | “test” | “prod”\tThe environment the frontend is currently running in. This is used to filter data in Grafana dashboards\tYes  ","version":"Next","tagName":"h3"},{"title":"Configuring the SDK with React Router integration​","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#configuring-the-sdk-with-react-router-integration","content":" Grafana Faro supports integration with React Router. This gives you events for page navigation and re-renders. See the Faro docs for more information on this.  ","version":"Next","tagName":"h3"},{"title":"Showing the data​","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#showing-the-data","content":" When the metrics have started to be gathered, they will be visible in a dedicated Grafana Faro dashboard. This dashboard can be found here .  It is also possible to search for data in the explore view . Useful labels to search for are:  faro_app_namekindenv  ","version":"Next","tagName":"h2"},{"title":"Privacy concerns​","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#privacy-concerns","content":" note It is up to you and your team to consider the how to use Faro with personal information as outlined in your IP and DPIA  When we send data to Faro, it is mostly metrics that don’t contain any PII . It is possible to include PII like name, IP or anything that is accessible from JavaScript in the SDK, but this is not done by default and requres calling the setUser function on the SDK.  A session ID is sent in to enable de-duplicating events like navigation between pages and ranking top users. This is a randomly generated string and is stored in the user’s browser SessionStorage. Note that even though this is not a cookie, this means a “cookie banner” is required as per the EU’s ePrivacy directive .  As SessionInstrumentation is included by default in the web instrumentation of the JavaScript SDK, disabling it requires invoking the SDK with instrumentations set and omitting the SessionInstrumentation function.  Data is stored on SKIP’s atgcp1-prod cluster, which stores data in Google Cloud Storage europe-north1 region. This region is located in Finland, and is thus within EU. This means no data leaves the EU’s borders which means the storage of the data is compliant with GDPR.  ","version":"Next","tagName":"h2"},{"title":"Rate limiting​","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#rate-limiting","content":" A rate limit for requests is implemented and is currently set to 50 requests per second. This is shared between all users of Faro, so it’s possible that we eventually reach the limit. Contact SKIP if you start getting queries rejected with HTTP 429 Too Many Requests .  The rate limiting algorighm is a token bucket algorithm, where a bucket has a maximum capacity for up to burst_size requests and refills at a rate of rate per second.  Each HTTP request drains the capacity of the bucket by one. Once the bucket is empty, HTTP requests are rejected with an HTTP 429 Too Many Requests status code until the bucket has more available capacity.  ","version":"Next","tagName":"h2"},{"title":"Tracing​","type":1,"pageTitle":"Real User Monitoring with Faro","url":"/docs/observability/real-user-monitoring-with-Faro#tracing","content":" Faro supports tracing of HTTP requests, but this is not currently implemented in the collector on SKIP. Contact SKIP if you want this! ","version":"Next","tagName":"h2"}],"options":{"languages":["en","no"],"id":"default"}}