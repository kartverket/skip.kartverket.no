<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Hybrid Kubernetes in production pt. 2 | SKIP</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://skip.kartverket.no/blog/hybrid-kubernetes-in-production-part-2"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Hybrid Kubernetes in production pt. 2 | SKIP"><meta data-rh="true" name="description" content="In this second installment of the Anthos series, we&#x27;ll talk about how we run Anthos and hybrid cloud in Kartverket. 
"><meta data-rh="true" property="og:description" content="In this second installment of the Anthos series, we&#x27;ll talk about how we run Anthos and hybrid cloud in Kartverket. 
"><meta data-rh="true" property="og:image" content="https://skip.kartverket.no/img/anthos-2.png"><meta data-rh="true" name="twitter:image" content="https://skip.kartverket.no/img/anthos-2.png"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2023-12-14T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://eliine.dev,https://github.com/bardove"><meta data-rh="true" property="article:tag" content="anthos,kubernetes,hybrid"><link data-rh="true" rel="icon" href="/img/favicon/favicon.ico"><link data-rh="true" rel="canonical" href="https://skip.kartverket.no/blog/hybrid-kubernetes-in-production-part-2"><link data-rh="true" rel="alternate" href="https://skip.kartverket.no/blog/hybrid-kubernetes-in-production-part-2" hreflang="en"><link data-rh="true" rel="alternate" href="https://skip.kartverket.no/blog/hybrid-kubernetes-in-production-part-2" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://skip.kartverket.no/blog/hybrid-kubernetes-in-production-part-2","mainEntityOfPage":"https://skip.kartverket.no/blog/hybrid-kubernetes-in-production-part-2","url":"https://skip.kartverket.no/blog/hybrid-kubernetes-in-production-part-2","headline":"Hybrid Kubernetes in production pt. 2","name":"Hybrid Kubernetes in production pt. 2","description":"In this second installment of the Anthos series, we'll talk about how we run Anthos and hybrid cloud in Kartverket. \n","datePublished":"2023-12-14T00:00:00.000Z","author":[{"@type":"Person","name":"Eline Henriksen","description":"Product Owner and Platform Developer","url":"https://eliine.dev","image":"https://github.com/eliihen.png"},{"@type":"Person","name":"Bård Ove Hoel","description":"Tech Lead and Platform Developer","url":"https://github.com/bardove","image":"https://github.com/bardove.png"}],"image":{"@type":"ImageObject","@id":"https://skip.kartverket.no/img/anthos-2.png","url":"https://skip.kartverket.no/img/anthos-2.png","contentUrl":"https://skip.kartverket.no/img/anthos-2.png","caption":"title image for the blog post: Hybrid Kubernetes in production pt. 2"},"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://skip.kartverket.no/blog","name":"SKIP Tech Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="SKIP RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="SKIP Atom Feed">
<link rel="alternate" type="application/json" href="/blog/feed.json" title="SKIP JSON Feed">







<script src="https://platform.twitter.com/widgets.js"></script><link rel="stylesheet" href="/assets/css/styles.5c884a55.css">
<script src="/assets/js/runtime~main.66b084c8.js" defer="defer"></script>
<script src="/assets/js/main.290349d0.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/skip.png" alt="SKIP" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/skip.png" alt="SKIP" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">SKIP</b></a><a class="navbar__item navbar__link" href="/docs">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Tech Blog</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://kartverket.atlassian.net/wiki/spaces/SIK/overview" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Sikkerhetshåndboka<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><a href="https://github.com/kartverket" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search"><span aria-label="expand searchbar" role="button" class="search-icon" tabindex="0"></span><input id="search_input_react" type="search" placeholder="Loading..." aria-label="Search" class="navbar__search-input search-bar" disabled=""></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_rMGB">2024</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/20-teams-on-skip">20 teams on SKIP: What we&#x27;ve learned along the way</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/introducing-apps-repositories">Scaling with Argo CD: Introducing the Apps Repo Architecture</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/crisis-management-exercises">Crisis Management Exercises</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/hybrid-kubernetes-in-production-part-3">Hybrid Kubernetes in production pt. 3</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_rMGB">2023</h3><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/blog/skip-on-plattformpodden">SKIP on Plattformpodden!</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title_f1Hy">Hybrid Kubernetes in production pt. 2</h1><div class="container_mt6G margin-vert--md"><time datetime="2023-12-14T00:00:00.000Z">December 14, 2023</time> · <!-- -->19 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://eliine.dev" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/eliihen.png" alt="Eline Henriksen"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://eliine.dev" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Eline Henriksen</span></a></div><small class="authorTitle_nd0D" title="Product Owner and Platform Developer">Product Owner and Platform Developer</small><div class="authorSocials_rSDt"></div></div></div></div><div class="col col--6 authorCol_Hf19"><div class="avatar margin-bottom--sm"><a href="https://github.com/bardove" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo authorImage_XqGP" src="https://github.com/bardove.png" alt="Bård Ove Hoel"></a><div class="avatar__intro authorDetails_lV9A"><div class="avatar__name"><a href="https://github.com/bardove" target="_blank" rel="noopener noreferrer"><span class="authorName_yefp">Bård Ove Hoel</span></a></div><small class="authorTitle_nd0D" title="Tech Lead and Platform Developer">Tech Lead and Platform Developer</small><div class="authorSocials_rSDt"></div></div></div></div></div></header><div id="__blog-post-container" class="markdown"><p><img decoding="async" loading="lazy" alt="Anthos in Google Cloud" src="/assets/images/anthos-4-7d2f18bbcb2f378e0657da61ac28fa2f.jpg" width="2880" height="1200" class="img_ev3q"></p>
<p>In this second installment of the Anthos series, we will talk about how we run
Anthos and hybrid cloud at <a href="https://kartverket.no/en" target="_blank" rel="noopener noreferrer">Kartverket</a>. We&#x27;ll touch
on the hardware, the software, and the processes we use to keep it running.</p>
<p>By the end we hope that we&#x27;ll have de-mystified Anthos a bit, and maybe given
you an idea of what it takes to run Anthos in production.</p>
<p>If you haven&#x27;t read the first part, you can find it
<a href="/blog/hybrid-kubernetes-in-production-part-1">here</a>.</p>
<p>This newsletter is the second of the three part series about Anthos in
Kartverket.</p>
<ol>
<li><a href="/blog/hybrid-kubernetes-in-production-part-1">Why we chose Anthos</a></li>
<li>How we run Anthos (You are here!)</li>
<li><a href="/blog/hybrid-kubernetes-in-production-part-3">Benefits and what we would have done differently</a></li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="installation-and-upgrades">Installation and upgrades<a href="#installation-and-upgrades" class="hash-link" aria-label="Direct link to Installation and upgrades" title="Direct link to Installation and upgrades">​</a></h2>
<p><img decoding="async" loading="lazy" alt="Illustration of the cluster architecture" src="/assets/images/anthos-5-41f824768cab6b841ef2fbdc0f5a375b.png" width="1456" height="1082" class="img_ev3q"></p>
<p>We have been early adopters of Anthos, so when doing the install we did not have
options for controlplane architecture. We wanted to use existing underlying
VMware infrastructure, so the nodes in our clusters are VMs, provisioned by
scripts provided by Google. Our cluster is installed with
<a href="https://kubernetes.io/blog/2017/01/how-we-run-kubernetes-in-kubernetes-kubeception/" target="_blank" rel="noopener noreferrer">kubeception</a>
controlplane architechture, this no longer the only, or recommended way. The
recommended model is <a href="https://cloud.google.com/anthos/clusters/docs/on-prem/latest/how-to/create-user-cluster-controlplane-v2" target="_blank" rel="noopener noreferrer">Controlplane
V2</a>,
where the controlplane nodes for the user cluster are in the user cluster
itself.</p>
<p>In the kubeception model, Kubernetes clusters are nested inside other Kubernetes
clusters. Specifically, the control plane of the user clusters runs in an
admin-cluster. For each on-premise cluster created, a new set of nodes and a
namespace are created in the admin cluster.</p>
<p>To install and make changes to the admin cluster, an admin workstation is
required, which must be located in the same network as the admin cluster. All
configurations are done using a CLI tool called <code>gkectl</code>. This tool handles most
cluster administration tasks, and the cluster specific configuration is provided
in YAML files.</p>
<p>Our cluster setup is more or less static, and most cluster administration tasks
involve upgrading or scaling existing clusters. The SKIP team has a cluster
referred to as “sandbox”, which is always the first recipient of potentially
breaking changes. After testing in sandbox, we&#x27;ll deploy changes to both
development and test environments, and if nothing breaks, we roll out the
changes to our production environment. This is mostly done outside work-hours,
although we have not experienced downtime during cluster upgrades. Here is the
general workflow for upgrading:</p>
<ol>
<li>Upgrade your admin workstation to the target version of your upgrade.</li>
<li>From your admin workstation, upgrade your user clusters.</li>
<li>After all of the user clusters have been upgraded, you can upgrade your admin
cluster from the admin workstation.</li>
</ol>
<p>We have tried using <a href="https://www.terraform.io/" target="_blank" rel="noopener noreferrer">Terraform</a> where possible to
simplify the setup. This can not be done in the same way for clusters using the
kubeception model. When we migrate to Controlplane V2 however, clusters can be
managed via GCP, and we can finally start using terraform for our on-premise
cluster config in the same way as for our GKE clusters, and GCP configuration in
general.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="gcp-integration">GCP integration<a href="#gcp-integration" class="hash-link" aria-label="Direct link to GCP integration" title="Direct link to GCP integration">​</a></h2>
<p>When working with an on-premise Anthos cluster, some of the nice-to-have
features of a standard GKE cluster have been lost. However, recently Anthos on
VMware clusters have gradually received more and more features compared to GKE
clusters.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="iam-and-groups">IAM and Groups<a href="#iam-and-groups" class="hash-link" aria-label="Direct link to IAM and Groups" title="Direct link to IAM and Groups">​</a></h3>
<p>Since we were early adaptors of Anthos, we had to endure not being able to
delegate clusterroles to IAM groups, and had to add single users to
clusterrole/rolebindings in Kubernetes. This was not a huge problem for us,
since we were working with a very limited number of teams and devs, but it was
apparent that this was not going to scale well. Luckily we got support for
groups before it was a problem, and our config files went from containing way
too many names and email addresses, to only containing groups.</p>
<p>Our Google Workspace receives groups and users from our Microsoft Active
Directory. Groups are initially created either in Entra ID, or on our local
Domain Controllers, and at set intervals changes are pushed to Google Workspace.
<a href="https://en.wikipedia.org/wiki/Role-based_access_control" target="_blank" rel="noopener noreferrer">Role-based access control
(RBAC)</a> based on
membership in these groups was needed. We wanted to manage this through
Terraform, and created a repo with where we store and configure our entire IAM
configuration. Since we have had growing adoption of Kubernetes and public cloud
in our organization, more teams, projects and apps have been onboarded to SKIP,
and this IAM repo has grown. We&#x27;ve tried to simplify the structure more than
once, but since this is a problem not affecting dev teams, we have chosen to
prioritize other tasks.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="workloads">Workloads<a href="#workloads" class="hash-link" aria-label="Direct link to Workloads" title="Direct link to Workloads">​</a></h3>
<p>All clusters created in in Anthos can be viewed from the GCP console, and the
<a href="https://cloud.google.com/anthos/multicluster-management/gateway/using" target="_blank" rel="noopener noreferrer">Connect
gateway</a>
makes it possible to do management from the console (or via kubectl) as well.
The GCP console can be used to get information about, or manage the state of the
cluster, workloads and resources present. This is a web GUI, part of the GCP
console, and not as snappy as cli-tools, but still usable, and intuitive to use.</p>
<p><img decoding="async" loading="lazy" alt="Anthos in Google Cloud" src="/assets/images/workload-5cb93c6dd1fc37775e7194682731de53.png" width="1223" height="618" class="img_ev3q">
This view shows workloads running in the argocd namespace. All workloads
displayed here can be clicked, and explored further.</p>
<p>When accessing the cluster via the Connect gateway there are some limits. The
Connect gateway does not handle persistent connections, and this makes it
impossible to do <a href="https://cloud.google.com/anthos/multicluster-management/gateway/using#run_commands_against_the_cluster" target="_blank" rel="noopener noreferrer">exec, port-forward, proxy or
attach</a>.
This is not a problem for a production environment, where containers should
never be used in this way. But for a dev, or sandbox environment, this is a bit
of a pain-point.</p>
<p>This issue should be partially fixed in Kubernetes 1.29 and should be completely
resolved in Kubernetes 1.30.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="service-mesh">Service Mesh<a href="#service-mesh" class="hash-link" aria-label="Direct link to Service Mesh" title="Direct link to Service Mesh">​</a></h3>
<p>A <a href="https://istio.io/latest/about/service-mesh/" target="_blank" rel="noopener noreferrer">Service Mesh</a> in Kubernetes is
an infrastructure layer that manages communication between services. We are
using Anthos Service Mesh (ASM), which is based on <a href="https://istio.io" target="_blank" rel="noopener noreferrer">Istio</a> and
nicely integrated with the GCP console. It&#x27;s easy to get an overview of
services, the connection between them, and what services are connected to either
our internal or external gateways. This can be displayed in a Topology view, or
if you click on a service, you&#x27;ll get a more detailed drilldown.</p>
<p><img decoding="async" loading="lazy" alt="Anthos Service Mesh" src="/assets/images/services-67dda8848ee239d94d4edfed09900478.png" width="1539" height="935" class="img_ev3q">
<em>A snippet of services running in our sandbox cluster.</em></p>
<p>When we deploy services to our cluster we create almost all Kubernetes and
service-mesh resources with our custom operator;
<a href="https://github.com/kartverket/skiperator" target="_blank" rel="noopener noreferrer">Skiperator</a>. This operator configures
the resources to fit our setup, and applies &quot;best practices&quot; the easy way. This
has been one of the great success stories in SKIP, and Skiperator is in
continuous development.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="deployment">Deployment<a href="#deployment" class="hash-link" aria-label="Direct link to Deployment" title="Direct link to Deployment">​</a></h2>
<p>Deployment is a very interesting subject when it comes to Anthos. As a platform
team, it is our job to make sure that deployment is as quick and convenient as
possible for the product teams. This ambition has led us to iterate on our
processes, which has finally led us to a solution that both we and the
developers enjoy using.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="iteration-1---terraform">Iteration 1 - Terraform<a href="#iteration-1---terraform" class="hash-link" aria-label="Direct link to Iteration 1 - Terraform" title="Direct link to Iteration 1 - Terraform">​</a></h3>
<p>When we first started out with Anthos, we had a very manual process for
deploying applications. A service account was provisioned in GCP, which allowed
the developers to impersonate a service account in Kubernetes, which in turn
allowed them to deploy apps using Terraform. This approach worked, but had a
decent amount of rough edges, and also would fail in ways that was hard to
debug.</p>
<p>With this approach the developers would have to manage their own Terraform
files, which most of the time was not within their area of expertise. And while
SKIP was able to build modules and tools to make this easier, it was still a
complex system that was hard to understand. Observability and discoverability
was also an issue.</p>
<p>Because of this we would consistently get feedback that this way of deploying
was too complicated and slow, in addition handling Terraform state was a pain.
As a platform team we&#x27;re committed to our teams&#x27; well being, so we took this
seriously and looked at alternatives. This was around the time we adopted Anthos,
so thus Anthos Config Managment was a natural choice.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="iteration-2---anthos-config-managment-acm">Iteration 2 - Anthos Config Managment (ACM)<a href="#iteration-2---anthos-config-managment-acm" class="hash-link" aria-label="Direct link to Iteration 2 - Anthos Config Managment (ACM)" title="Direct link to Iteration 2 - Anthos Config Managment (ACM)">​</a></h3>
<p><img decoding="async" loading="lazy" alt="Anthos Config Management architecture showing multiple Git repos deployed to a cluster" src="/assets/images/acm-1-46a04fbc7dd63ebcae3c08935a5aa51c.png" width="732" height="726" class="img_ev3q"></p>
<p>ACM is a set of tools that allows you to declaratively manage your Kubernetes
resources. Here we&#x27;re mostly going to talk about Config Sync, which is a
<a href="https://about.gitlab.com/topics/gitops/" target="_blank" rel="noopener noreferrer">GitOps</a> system for Kubernetes.</p>
<p>In a GitOps system, a team will have a Git repository that contains all the
Kubernetes resources that they want to deploy. This repository is then synced
to the Kubernetes cluster, and the resources are applied.</p>
<p>This can be likened to a pull-based system, where the GitOps tool (Config sync)
watches the repo for changes and pulls them into the cluster. This is in
contrast to a push-based system, where a script pushes the changes to a
cluster. It is therefore a dedicated system for deployment to Kubernetes, and
following the <a href="https://en.wikipedia.org/wiki/Unix_philosophy" target="_blank" rel="noopener noreferrer">UNIX philosophy</a>
which focuses on doing that one thing well.</p>
<p>Using this type of a workflow solves a lot of the issues around the Terraform
based deployment that we had in the previous iteration. No longer do developers
need to set up a complicated integration with GCP service accounts and
impersonation, committing a file to a Git repo will trigger a deployment. The
Git repo and the manifests in them also works as a state of truth for the
cluster, instead of having to reverse engineer what was deployed based on
terraform diffs and state.</p>
<p><img decoding="async" loading="lazy" alt="ACM UI showing a sync in progress" src="/assets/images/acm-2-1a9a222556da2f846d4f64bb1978db27.gif" width="928" height="568" class="img_ev3q"></p>
<p>It started well, however we soon ran into issues. The system would often take
a long time to reconcile the sync, and during the sync we would not have any
visibility into what was happening. This was not a deal breaker, but at the
same time this was not a particularly good developer experience.</p>
<p>We also ran into issues with implementing a level of self-service that we were
satisfied with. We wanted to give the developers the ability to provision their
own namespaces, but due to the multi-tenant nature of our clusters we also had
to make sure that teams were not able to write to each others&#x27; namespaces.
This was not a feature we were able to implement, but luckily our next iteration
had this built in, and we&#x27;ll get back to that.</p>
<p>The final nail was the user interface. We simply expected more from a deployment
system than what ACM was able to provide. The only view into the deployment was
a long list of resources, which to a developer that is not an expert in
Kubernetes, was not intuitive enough.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="final-iteration---argo-cd">Final iteration - Argo CD<a href="#final-iteration---argo-cd" class="hash-link" aria-label="Direct link to Final iteration - Argo CD" title="Direct link to Final iteration - Argo CD">​</a></h3>
<p><img decoding="async" loading="lazy" src="/assets/images/argo-1-10ff6f4861d91a7f670f671e2f0ba43d.png" width="2498" height="1229" class="img_ev3q"></p>
<p>This finally brought us to our current iteration. We had heard about Argo CD
before, but initially we were hesitant to add another system to our stack.
After ACM had introduced us to GitOps and we looked deeper into Argo CD, it was
obvious to us that Argo was more mature and would give our developers a better
user experience.</p>
<p>The killer feature here is the UI. Argo CD has an intuitive and user-friendly
UI that gives the developers a good overview of what is deployed. Whenever
anything fails, it&#x27;s immediately obvious which resource is failing, and Argo
allows you to drill down into the resource to see the details of the failure,
logs for deployments, Kubernetes events, etc.</p>
<p><img decoding="async" loading="lazy" src="/assets/images/argo-2-61a9fc0299932ae38d232796c8f4f677.png" width="2499" height="1209" class="img_ev3q"></p>
<p>The above photo illustrates this well. Here you can see a project with a number
of <a href="https://github.com/kartverket/skiperator" target="_blank" rel="noopener noreferrer">Skiperator</a> applications. The
green checkmarks indicate that the application is synced and the green heart
indicates that the application is healthy. A developer can see the underlying
&quot;owned&quot; resources that Skiperator creates (such as a deployment, service, etc),
and get a look &quot;behind the curtain&quot; to see what is actually deployed. This helps
debugging and gives the developers a better insight into what is happening
during a deployment.</p>
<p>In terms of multi tenancy, Argo CD has a concept of projects. A project is a
set of namespaces that a team has access to, and a team can only use Argo to
sync to namespaces that are part of their project. The namespace allowlist can
also include wildcards, which sounds small but this solved our self-service
issue! With our apps-repo architecture, we would give a team a &quot;prefix&quot; (for
example <code>seeiendom-</code>), and that team would then be able to deploy to and create
any namespace that started with that prefix. If they tried to deploy to another
team&#x27;s namespace they would be stopped, as they would not have access to that
prefix.</p>
<p>The prefix feature allows product teams to create a new directory in their apps
repo, which will then be synced to the cluster and deployed as a new namespace.
This is a very simple and intuitive workflow for creating short-lived
deployments, for example for pull requests, and it has been very well received
by the developers.</p>
<p>The apps-repo architecture will be a blog post itself at some point, so I won&#x27;t
go too much into it.</p>
<p>And finally, if you&#x27;re wondering what disaster recovery of an entire cluster
looks like with Argo CD, I leave you with the following video at the end.</p>
<video controls="" width="100%" muted=""><source src="/img/argo-3.mov" type="video/mp4"></video>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="hybrid-mesh">Hybrid Mesh<a href="#hybrid-mesh" class="hash-link" aria-label="Direct link to Hybrid Mesh" title="Direct link to Hybrid Mesh">​</a></h2>
<p>A hybrid mesh service mesh configuration is a setup that allows for service
networking across different environments. For Kartverket this includes a hybrid
cloud environment. The setup involves several steps, including setting up
cross-cluster credentials, installing the east-west gateway, enabling endpoint
discovery, and configuring certificate authorities. All clusters in a hybrid
mesh are registered to the same fleet host project, and istiod in each cluster
must be able to communicate with the Kube-API on the opposing clusters.</p>
<p>ASM is as previously mentioned based on Istio, and after some internal
discussion we decided to experiment with running vanilla upstream Istio in our
GKE clusters running in GCP. Pairing it with ASM in our on-premise clusters
worked as expected (after a bit of config), and we are now running upstream
Istio in GKE, with ASM on-prem in a multi-cluster setup. We also looked into
using managed ASM in our GKE cluster, this was hard for us however, due to it
requiring firewall openings on-prem for sources we could not predict.</p>
<p><img decoding="async" loading="lazy" alt="Multi-Primary on different networks" src="/assets/images/multi-cluster-37ee6eb4218f5c79582ad4738a942ac6.png" width="748" height="555" class="img_ev3q"></p>
<p>We have chosen the <a href="https://istio.io/latest/docs/setup/install/multicluster/multi-primary_multi-network/" target="_blank" rel="noopener noreferrer">Multi-Primary on different
networks</a>
after reviewing our network topology and configuration. We connect our
on-premise network, with the GCP VPC through a VPN connection (using host and
service projects). To have a production ready environment, the VPN connection
must be configured with redundancy.</p>
<p>We&#x27;re working towards getting this architecture into production, as this will
enable us to seamlessly use GKE clusters in GCP together with our on-premise
clusters. The elasticity of cloud infrastructure can be utilized where needed,
and we can handle communication between services on different clusters much more
smoothly. This has been a bit of a journey to configure, but as a learning
experience it has been valuable. Being able to address services seamlessly and
communicate with mTLS enabled by default across sites, zones and clusters
without developers having to think about it feels a bit like magic.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="monitoring">Monitoring<a href="#monitoring" class="hash-link" aria-label="Direct link to Monitoring" title="Direct link to Monitoring">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="google-cloud-monitoring">Google Cloud Monitoring<a href="#google-cloud-monitoring" class="hash-link" aria-label="Direct link to Google Cloud Monitoring" title="Direct link to Google Cloud Monitoring">​</a></h3>
<p><img decoding="async" loading="lazy" alt="Google Cloud Monitoring dashboard" src="/assets/images/gcp-monitoring-1-eb5c94f71f6e0d8c45c6211002c702e5.png" width="2256" height="1108" class="img_ev3q"></p>
<p>GKE Enterprise includes an agent that collects metrics from the cluster and sends
them to Google Cloud. This is a great feature which makes it relatively easy
to get started with metrics and monitoring. However, we have decided not to use
the agent, and instead use Grafana and LGTM for metrics and monitoring.</p>
<p>This is mainly due to a couple of challenges:</p>
<p>The amount of metrics that are collected out of the box and sent to GCP
contributes a significant part of our total spend. It&#x27;s not that we have a lot
of clusters, but the amount of metrics that are collected out of the box is very
high, and Anthos&#x27; default setup didn&#x27;t give us the control we needed to be able
to manage it in a good way.</p>
<p>Note that this was before <a href="https://cloud.google.com/managed-prometheus?hl=en" target="_blank" rel="noopener noreferrer">Managed Service for
Prometheus</a> was released with
more fine grained control over what metrics are collected. It is now the
recommended default, which should make metrics collection easier to manage.</p>
<p>Second, while Google Cloud Monitoring has a few nice dashboards ready for
Anthos, it feels inconsistent which dashboards work on-premise and which only
work in cloud as they are not labeled as such. This is not a big issue, but it&#x27;s
a bit annoying. The bigger issue is that all the dashboards feel sluggish and
slow to load. Several of us have used Grafana before, so we&#x27;re used to a
snappy and responsive UI. In our opinion, Google Cloud Monitoring feels clunky
in comparison.</p>
<p>So the cost and the user experience were the main reasons we decided to look at
alternatives to Google Cloud Monitoring. We ended up using Grafana and LGTM,
which we&#x27;ll talk about next.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="grafana-with-the-lgtm-stack">Grafana with the LGTM stack<a href="#grafana-with-the-lgtm-stack" class="hash-link" aria-label="Direct link to Grafana with the LGTM stack" title="Direct link to Grafana with the LGTM stack">​</a></h3>
<p><img decoding="async" loading="lazy" alt="Grafana dashboard with a Kubernetes cluster overview" src="/assets/images/grafana-1-aa91144f9105339f1edf0c1ad1aab0b0.png" width="5088" height="3266" class="img_ev3q"></p>
<p>When we realized that our needs were not entirely met by Google Cloud Monitoring,
we started a project to develop a monitoring stack that would meet our needs.
Since Grafana is open source and has a large community, we decided to use that
as our frontend. Our backend is the LGTM stack, which is a set of open source
tools that are designed to work well together for ingesting, storing and querying
logs, traces and metrics.</p>
<p>What we noticed immediately was that the product teams were much more engaged
with this stack than they were with <a href="https://cloud.google.com/monitoring/?hl=en" target="_blank" rel="noopener noreferrer">Google Cloud
Monitoring</a>. Previously they would
not really look at the dashboards, but now they are using them and even creating
their own. This is a huge win for us, as we want the teams to be engaged with
the monitoring and observability of their services.</p>
<p>It definitely helps that most developers on the product teams are familiar with
Grafana, which makes it easier for them to get started as the learning curve is
not as steep.</p>
<p>There was a discussion about what the backend should be, if we should use
<a href="https://grafana.com/products/cloud/" target="_blank" rel="noopener noreferrer">Grafana Cloud</a> or host it ourselves. There
would be a lot of benefits of using the cloud, as we would not have to maintain
the stack or worry about performance or storage. There was, however, a concern
about cost and whether or not log files could be shipped to a cloud provider. In
the end we decided to host it ourselves, mostly because we didn&#x27;t have control
over what quantities of data we&#x27;re processing. Now that we have a better
understanding of our usage we can use that to calculate our spend, so we&#x27;re not
ruling out migrating to Grafana Cloud in the future.</p>
<p>The collection (scraping) of data is done by <a href="https://grafana.com/oss/agent/" target="_blank" rel="noopener noreferrer">Grafana
Agent</a>, which is an &quot;all-in-one&quot; agent that
collects metrics, logs and traces. This means a few less moving parts for the
stack, as we don&#x27;t have to run both <a href="https://prometheus.io/" target="_blank" rel="noopener noreferrer">Prometheus</a>,
<a href="https://fluentbit.io/" target="_blank" rel="noopener noreferrer">Fluent Bit</a> and some
<a href="https://opentelemetry.io/" target="_blank" rel="noopener noreferrer">OpenTelemetry</a> compatible agent for traces. It&#x27;s a
relatively new project, but it&#x27;s already relative stable and has a lot of
features. It uses a funky format for configuration called river, which is based
on Hashicorp&#x27;s HCL. The config enables forming pipelines to process data before
it&#x27;s forwarded to Loki, Tempo or Mimir.  It&#x27;s a bit different, but it works well
and is easy to understand and configure to our needs.</p>
<p><img decoding="async" loading="lazy" alt="Alerting with Grafana" src="/assets/images/grafana-2-849aa5c18bba686f0b10f13a446ff672.png" width="5088" height="3342" class="img_ev3q"></p>
<p>Using a system like Grafana also enables us to build an integrated experience
that also includes alerting. Using Grafana alerting and OnCall, we configure
alerts that are sent to the correct team based on the service that is failing.
This helps the teams get a better overview of what is happening in their
services, and also helps us as a platform team to not have to be involved in
every alert that is triggered.</p>
<p>Overall we&#x27;re very happy with the LGTM stack, even though it&#x27;s a fair bit of
work to maintain the stack (especially with Istio and other security measures).
We&#x27;re also happy with Grafana, and we&#x27;re looking forward to seeing what the
future holds for monitoring and observability in Kubernetes.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary">​</a></h2>
<p>To summarize: We like Anthos, and we think it&#x27;s a great platform for running
hybrid Kubernetes. As a platform team we look at each feature on a case-by-case
basis, with the goal of giving our developers the best possible experience
instead of naively trying to use as much as possible of the platform. Because of
this we&#x27;ve decided to use Anthos for Kubernetes and service mesh, but not for
config sync and monitoring. This has given us a great platform that we&#x27;re
confident will serve us well for years to come.</p>
<p>Stay tuned for the third and final part of this series, where we&#x27;ll talk about
the benefits we&#x27;ve seen from Anthos, and what we would have done differently if
we were to start over.</p>
<p><em>Disclaimer - Google, GKE and Anthos are trademarks of Google LLC and this website is not
endorsed by or affiliated with Google in any way.</em></p></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/anthos">anthos</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/kubernetes">kubernetes</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/blog/tags/hybrid">hybrid</a></li></ul></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/blog/skip-on-plattformpodden"><div class="pagination-nav__sublabel">Newer post</div><div class="pagination-nav__label">SKIP on Plattformpodden!</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/blog/hybrid-kubernetes-in-production-part-1"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Hybrid Kubernetes in production pt. 1</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#installation-and-upgrades" class="table-of-contents__link toc-highlight">Installation and upgrades</a></li><li><a href="#gcp-integration" class="table-of-contents__link toc-highlight">GCP integration</a><ul><li><a href="#iam-and-groups" class="table-of-contents__link toc-highlight">IAM and Groups</a></li><li><a href="#workloads" class="table-of-contents__link toc-highlight">Workloads</a></li><li><a href="#service-mesh" class="table-of-contents__link toc-highlight">Service Mesh</a></li></ul></li><li><a href="#deployment" class="table-of-contents__link toc-highlight">Deployment</a><ul><li><a href="#iteration-1---terraform" class="table-of-contents__link toc-highlight">Iteration 1 - Terraform</a></li><li><a href="#iteration-2---anthos-config-managment-acm" class="table-of-contents__link toc-highlight">Iteration 2 - Anthos Config Managment (ACM)</a></li><li><a href="#final-iteration---argo-cd" class="table-of-contents__link toc-highlight">Final iteration - Argo CD</a></li></ul></li><li><a href="#hybrid-mesh" class="table-of-contents__link toc-highlight">Hybrid Mesh</a></li><li><a href="#monitoring" class="table-of-contents__link toc-highlight">Monitoring</a><ul><li><a href="#google-cloud-monitoring" class="table-of-contents__link toc-highlight">Google Cloud Monitoring</a></li><li><a href="#grafana-with-the-lgtm-stack" class="table-of-contents__link toc-highlight">Grafana with the LGTM stack</a></li></ul></li><li><a href="#summary" class="table-of-contents__link toc-highlight">Summary</a></li></ul></div></div></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Lenker</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://uustatus.no/nb/erklaringer/publisert/7148395f-f0cf-4019-9c45-3b83d79fd544" target="_blank" rel="noopener noreferrer" class="footer__link-item">Tilgjengelighetserklæring<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2025 Statens Kartverk</div></div></div></footer></div>
</body>
</html>